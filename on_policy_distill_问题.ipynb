{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4057e08b-7e7c-4471-bb83-8d808c2cf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf83dd81-ea48-443b-84b8-a602068d2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置API密钥（需替换为你的实际密钥）\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-Gzo6arNf2T7NmrUYZUcxcQgRnIYclT5tqmgu9NfB34HfBkRd\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api2.aigcbest.top/v1\"  # 必填代理地址\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "# claude-opus-4-1-20250805\n",
    "# claude-opus-4-1-thinking-all\n",
    "# 初始化 GPT-4o 模型\n",
    "model = ChatOpenAI(\n",
    "    model=\"gemini-3-pro-preview-thinking\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 创建对话历史记录\n",
    "chat_history = []\n",
    "\n",
    "def chat(user_input):\n",
    "    # 添加用户消息到历史\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    # 生成回复\n",
    "    response = model.invoke(chat_history)\n",
    "    # 提取并显示 AI 回复\n",
    "    ai_reply = response.content\n",
    "    # 保存 AI 消息到历史\n",
    "    chat_history.append(AIMessage(content=ai_reply))\n",
    "    print(ai_reply)\n",
    "    return ai_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d74d5d-a045-45d8-9dc9-6e7dd68e95c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常敏锐的观察。日志中的数据确实揭示了一个潜在的严重问题，主要表现为 **Student LogProb (`S_LogP`) 异常地接近 0**（例如 `-0.00`），而 **Teacher LogProb (`T_LogP`) 却非常低**（例如 `-12.61`）。\n",
      "\n",
      "这通常意味着以下两个问题之一或同时存在：\n",
      "1.  **Temperature（温度）设置不一致**：Student 在计算 LogProb 时可能使用了极低的温度（接近 0），导致分布极度尖锐（Probability $\\approx 1.0$），而 Teacher 使用了默认温度（通常为 1.0）。\n",
      "2.  **Prompt 上下文严重偏移（Context Shift）**：Teacher 看到的 Prompt（包含 Hint 和 Instruction）使得 Student 生成的 Token（例如 \"Let\"）在 Teacher 眼里变得极度不可能。\n",
      "\n",
      "以下是详细的分析和修复方案。\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 问题分析\n",
      "\n",
      "#### A. 为什么 `S_LogP` 是 `-0.00`？\n",
      "*   **现象**：`S_LogP` 为 `-0.00` 意味着模型对该 Token 的预测概率接近 100%。对于一个未经多轮强化学习训练的基座模型（Qwen-Instruct），在 Step 1 就能达到这种确定性是不正常的，除非使用了 **Greedy Decoding** 或者 **极低的 Temperature**。\n",
      "*   **原因**：\n",
      "    *   在 `TeacherStudentReflectiveTrainer.fit` 中，Student 的 `batch` 是从 `gen_batch` 继承而来的。\n",
      "    *   `gen_batch` 的 `meta_info` 包含了生成时的参数。如果在配置文件中 `rollout` 的 `temperature` 设置得很低（或者 `do_sample=False`），那么这个低温度会被带入 `compute_log_prob`。\n",
      "    *   在 `DP_Actor.compute_log_prob` 中，代码执行了 `logits.div_(temperature)`。如果 `temperature` 例如是 `0.01`，Logits 会被放大 100 倍，Softmax 后概率分布会变成 One-Hot，导致 LogProb 变成 0。\n",
      "\n",
      "#### B. 为什么 `T_LogP` 是 `-12.61`？\n",
      "*   **现象**：`-12.61` 意味着 $P(token) \\approx e^{-12.61} \\approx 3 \\times 10^{-6}$。这是一个极低的概率，甚至低于随机猜测（词表大小 150k 时，均匀分布 LogProb 约为 -11.9）。\n",
      "*   **原因**：\n",
      "    1.  **Temperature**：Teacher 的 `teacher_batch` 是在 `_prepare_teacher_forward_batch` 中新创建的，**代码中没有显式设置 `meta_info['temperature']`**。如果 `DataProto` 或 `DP_Actor` 默认使用 `1.0`，那么这个 LogProb 是在正常温度下计算的。\n",
      "    2.  **Prompt 差异**：\n",
      "        *   Student 看到的 Prompt 结尾是：`Question... <|im_end|>\\n<|im_start|>assistant\\n`。它生成了 `Let`。\n",
      "        *   Teacher 看到的 Prompt 结尾是：`Analysis... solution.<|im_end|>\\n<|im_start|>assistant\\n`。\n",
      "        *   虽然结尾都是 `assistant\\n`，但 Teacher 上下文中有 \"Analysis/Hint\"。Qwen 模型在看到 \"Analysis\" 后，可能倾向于输出 \"The correct solution is...\" 或者 \"Based on...\"，而不是直接开始 \"Let's solve\"。\n",
      "        *   **但是**，`-12` 实在太低了，这说明 Teacher 几乎认为 \"Let\" 是个错误的 Token。这通常是 **Temperature 不匹配** 加上 **上下文偏移** 共同作用的结果。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 修复方案\n",
      "\n",
      "我们需要做两件事：\n",
      "1.  **强制统一 Temperature**：在计算 LogProb（无论是 Student 还是 Teacher）时，必须使用 `temperature=1.0`，以获得真实的 Logits 分布用于 KL 散度计算。\n",
      "2.  **修复 Teacher Batch 的 Meta Info**：确保 `teacher_batch` 携带正确的参数。\n",
      "\n",
      "请修改 `verl/trainer/ppo/teacher_student_reflective_trainer.py` 中的 `fit` 函数和 `_prepare_teacher_forward_batch` 函数。\n",
      "\n",
      "#### 修改 1: `_prepare_teacher_forward_batch` (添加 Meta Info)\n",
      "\n",
      "在 `_prepare_teacher_forward_batch` 函数的最后，返回 `teacher_batch` 之前，添加 `meta_info`。\n",
      "\n",
      "```python\n",
      "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor) -> DataProto:\n",
      "        # ... (前面的代码保持不变) ...\n",
      "\n",
      "        teacher_batch = DataProto.from_dict({\n",
      "            \"input_ids\": input_ids,\n",
      "            \"attention_mask\": attention_mask,\n",
      "            \"position_ids\": position_ids,\n",
      "            \"response_mask\": response_mask,\n",
      "            \"prompts\": prompts_padded,   \n",
      "            \"responses\": responses_padded \n",
      "        })\n",
      "        \n",
      "        # === FIX: 显式设置 Meta Info，确保计算 LogProb 时使用 Temp=1.0 ===\n",
      "        teacher_batch.meta_info = {\n",
      "            \"temperature\": 1.0,  # 关键：用于 LogProb 计算的标准温度\n",
      "            \"micro_batch_size\": batch.meta_info.get(\"micro_batch_size\", 1), # 继承或设默认\n",
      "            \"use_dynamic_bsz\": batch.meta_info.get(\"use_dynamic_bsz\", False),\n",
      "            \"max_token_len\": batch.meta_info.get(\"max_token_len\", 2048)\n",
      "        }\n",
      "        \n",
      "        return teacher_batch\n",
      "```\n",
      "\n",
      "#### 修改 2: `fit` 函数 (强制 Student Batch Temperature)\n",
      "\n",
      "在 `fit` 函数的 Step 3.5 (Student Computes LogProb) 之前，强制设置 `batch` 的 temperature。\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.4 Teacher LogProb 代码) ...\n",
      "                    # 3.4 Teacher Computes LogProb\n",
      "                    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries)\n",
      "                    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
      "                    teacher_full_log_probs = teacher_log_prob_output.batch['old_log_probs']\n",
      "\n",
      "                    # === FIX: 强制 Student LogProb 计算使用 Temp=1.0 ===\n",
      "                    # 避免继承生成时的低温参数导致 LogProb 为 0\n",
      "                    batch.meta_info['temperature'] = 1.0 \n",
      "\n",
      "                    # 3.5 Student Computes LogProb\n",
      "                    student_log_prob_output = self.actor_rollout_wg.compute_log_prob(batch)\n",
      "                    student_full_log_probs = student_log_prob_output.batch['old_log_probs']\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 验证与预期\n",
      "\n",
      "应用上述修复后，你应该会看到：\n",
      "\n",
      "1.  **S_LogP 变得正常**：不再是 `-0.00`，而是变成类似 `-0.5` 到 `-2.0` 的值（取决于模型对该 Token 的置信度）。\n",
      "2.  **T_LogP 保持或略有变化**：它仍然会比 `S_LogP` 低（因为 Context 不同），但不应该低到 `-15` 这种离谱的程度（除非 Teacher 真的极其确信该 Token 不该出现）。\n",
      "3.  **Token-Level KL Diff**：`T_LogP - S_LogP`。\n",
      "    *   如果 `S` 是 `-0.5`，`T` 是 `-5.0`，Diff 是 `-4.5`。\n",
      "    *   这将给出一个负的 Reward（惩罚），这是合理的：Teacher 认为 Student 在这种情况下说 \"Let\" 不太好。\n",
      "\n",
      "**关于 Teacher Prompt 的额外建议：**\n",
      "如果修复 Temperature 后，`T_LogP` 依然极低（例如 `-10`），说明 Teacher 在看到 Hint 后确实非常不想输出 \"Let\"。你可以尝试微调 Teacher 的 Prompt，引导它去“补全”学生的回答，而不是“提供”一个新的解答。\n",
      "\n",
      "例如，修改 `_prepare_teacher_forward_batch` 中的 `teacher_content`：\n",
      "\n",
      "```python\n",
      "            teacher_content = (\n",
      "                f\"Problem: {p_text_clean}\\n\\n\"\n",
      "                f\"Analysis/Hint: {s_text}\\n\\n\"\n",
      "                f\"Based on the analysis, verify the following solution step-by-step:\" # 稍微改动指令，让它更倾向于跟随\n",
      "            )\n",
      "```\n",
      "或者，由于这是 On-Policy Distillation，Teacher 的 Context 差异导致的 KL 差异本身就是一种信号，可能不需要过度修正 Prompt，而是依赖 KL Loss 让 Student 学习适应 Teacher 的分布（但这可能导致 Student 学会忽略 Prompt 直接背诵 Teacher 的偏好）。\n",
      "\n",
      "**总结：最核心的 Bug 是 LogProb 计算时的 `temperature` 不一致且未显式设置为 1.0。请先应用上述代码修复。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我在用verl框架实现on policy distillation，目标如下\n",
    "\"\"\"\n",
    "1.student先根据query生成response，这里面肯定没有hack的问题，因为query里没有ground truth\n",
    "2.把query + student response + ground truth做个摘要\n",
    "3.用teacher mode重新算一遍student的response的概率，输入是 query + 上面的摘要 + student的response，这里不做生成，只前向传播计算概率\n",
    "4.最小化student的response和第3步中的概率的reverse KL loss，注意这里不是用第3步中的所有概率，而是只取第3步中response部分的前向传播概率，所以要做截断。\n",
    "\"\"\"\n",
    "我写的teacher_student_reflective_trainer.py代码如下\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from verl import DataProto\n",
    "from verl.trainer.ppo.ray_trainer import RayPPOTrainer, compute_response_mask, compute_advantage, Role\n",
    "from verl.utils.metric import (\n",
    "    reduce_metrics,\n",
    ")\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from verl.trainer.ppo.core_algos import AdvantageEstimator\n",
    "from verl.utils.debug import marked_timer\n",
    "from verl.utils.torch_functional import masked_mean\n",
    "from verl.utils.tracking import Tracking\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\n",
    "from verl.single_controller.ray.base import create_colocated_worker_cls\n",
    "\n",
    "class TeacherStudentReflectiveTrainer(RayPPOTrainer):\n",
    "    \"\"\"\n",
    "    Teacher-Student Reflective Trainer.\n",
    "    \n",
    "    Architecture:\n",
    "    - Student (Actor): Trainable. Generates Response.\n",
    "    - Teacher (RefPolicy): Frozen. Generates Summary AND Computes LogProb.\n",
    "    \n",
    "    Resource Strategy:\n",
    "    - Splits available GPUs into two pools (Student Pool & Teacher Pool) to allow \n",
    "      two simultaneous vLLM instances without conflict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert self.use_reference_policy, \"TeacherStudentReflectiveTrainer requires a Reference Policy (Teacher)!\"\n",
    "        self.use_critic = False\n",
    "        self.use_rm = False\n",
    "        \n",
    "        print(\">>> TeacherStudentReflectiveTrainer Initialized.\")\n",
    "        print(\">>> Strategy: Frozen Teacher generates Summary & Scores.\")\n",
    "\n",
    "    def init_workers(self):\n",
    "        \"\"\"\n",
    "        Manually split resources and initialize two independent vLLM-enabled workers.\n",
    "        \"\"\"\n",
    "        # 1. Calculate Resource Split\n",
    "        # We assume homogeneous nodes. We split GPUs evenly between Student and Teacher.\n",
    "        n_gpus_per_node = self.config.trainer.n_gpus_per_node\n",
    "        n_nodes = self.config.trainer.nnodes\n",
    "        \n",
    "        if n_gpus_per_node < 2:\n",
    "            raise ValueError(f\"Need at least 2 GPUs per node to split Student/Teacher. Got {n_gpus_per_node}.\")\n",
    "            \n",
    "        student_gpus = n_gpus_per_node // 2\n",
    "        teacher_gpus = n_gpus_per_node - student_gpus\n",
    "        \n",
    "        print(f\">>> Resource Splitting: Student={student_gpus} GPUs, Teacher={teacher_gpus} GPUs (per node)\")\n",
    "\n",
    "        # 2. Create Independent Resource Pools\n",
    "        # Note: We create new RayResourcePools, ignoring the one passed from main.\n",
    "        # This ensures Ray allocates non-overlapping GPUs to each pool.\n",
    "        student_pool = RayResourcePool(\n",
    "            process_on_nodes=[student_gpus] * n_nodes,\n",
    "            use_gpu=True,\n",
    "            max_colocate_count=1,\n",
    "            name_prefix=\"student_pool\"\n",
    "        )\n",
    "        \n",
    "        teacher_pool = RayResourcePool(\n",
    "            process_on_nodes=[teacher_gpus] * n_nodes,\n",
    "            use_gpu=True,\n",
    "            max_colocate_count=1,\n",
    "            name_prefix=\"teacher_pool\"\n",
    "        )\n",
    "        \n",
    "        self.resource_pool_to_cls = {student_pool: {}, teacher_pool: {}}\n",
    "\n",
    "        # 3. Initialize Student (Actor) Config\n",
    "        # Deepcopy to prevent config pollution\n",
    "        student_config = deepcopy(self.config.actor_rollout_ref)\n",
    "        # Adjust micro_batch_size if necessary (optional, but good practice since world_size changed)\n",
    "        # student_config.actor.ppo_mini_batch_size //= 2 # Logic handled by worker usually\n",
    "        \n",
    "        student_cls = RayClassWithInitArgs(\n",
    "            cls=self.role_worker_mapping[Role.ActorRollout],\n",
    "            config=student_config,\n",
    "            role=\"actor_rollout\",\n",
    "            profile_option=self.config.trainer.npu_profile.options,\n",
    "        )\n",
    "        self.resource_pool_to_cls[student_pool][\"actor_rollout\"] = student_cls\n",
    "\n",
    "        # 4. Initialize Teacher (Ref) Config\n",
    "        # We define it as 'actor_rollout' role to FORCE vLLM initialization.\n",
    "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
    "        \n",
    "        teacher_cls = RayClassWithInitArgs(\n",
    "            cls=self.role_worker_mapping[Role.RefPolicy],\n",
    "            config=teacher_config,\n",
    "            role=\"actor_rollout\", # <--- Hack: Pretend to be Actor to enable vLLM\n",
    "            profile_option=self.config.trainer.npu_profile.options,\n",
    "        )\n",
    "        self.resource_pool_to_cls[teacher_pool][\"ref\"] = teacher_cls\n",
    "\n",
    "        # 5. Spawn Workers\n",
    "        all_wg = {}\n",
    "        wg_kwargs = {\n",
    "            \"ray_wait_register_center_timeout\": self.config.trainer.ray_wait_register_center_timeout,\n",
    "            \"device_name\": self.device_name\n",
    "        }\n",
    "        \n",
    "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
    "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
    "            wg_dict = self.ray_worker_group_cls(\n",
    "                resource_pool=resource_pool,\n",
    "                ray_cls_with_init=worker_dict_cls,\n",
    "                **wg_kwargs,\n",
    "            )\n",
    "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
    "            all_wg.update(spawn_wg)\n",
    "\n",
    "        # 6. Bind to Trainer\n",
    "        self.actor_rollout_wg = all_wg[\"actor_rollout\"]\n",
    "        self.actor_rollout_wg.init_model()\n",
    "        \n",
    "        self.ref_policy_wg = all_wg[\"ref\"]\n",
    "        self.ref_policy_wg.init_model()\n",
    "            \n",
    "        # Async Rollout Manager (Only for Student)\n",
    "        self.async_rollout_mode = False\n",
    "        if self.config.actor_rollout_ref.rollout.mode == 'async':\n",
    "            from verl.experimental.agent_loop import AgentLoopManager\n",
    "            self.async_rollout_mode = True\n",
    "            self.async_rollout_manager = AgentLoopManager(\n",
    "                config=self.config,\n",
    "                worker_group=self.actor_rollout_wg\n",
    "            )\n",
    "\n",
    "    def _prepare_summary_generation_batch(self, batch: DataProto, raw_prompts: List[str], ground_truths: List[str]) -> DataProto:\n",
    "        \"\"\"\n",
    "        构造用于生成总结 (Reflection) 的 Batch。\n",
    "        包含: 强力正则清洗，去除 System Prompt 和嵌套标记。\n",
    "        \"\"\"\n",
    "        responses = batch.batch['responses']\n",
    "        \n",
    "        assert len(raw_prompts) == len(responses)\n",
    "        assert len(ground_truths) == len(responses)\n",
    "\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "\n",
    "        for i, (r_ids, gt_text) in enumerate(zip(responses, ground_truths)):\n",
    "            p_text_dirty = raw_prompts[i]\n",
    "            \n",
    "            # --- 正则清洗逻辑 (提取纯 User Query) ---\n",
    "            p_text_clean = p_text_dirty\n",
    "            \n",
    "            # 1. 尝试截取最后一个 User 标记之后的内容\n",
    "            split_patterns = [\n",
    "                r\"<\\|im_start\\|>user\\s*\",  # Qwen/ChatML\n",
    "                r\"user\\s*\\n\",              # Qwen decode\n",
    "                r\"User:\\s*\",               # Common\n",
    "                r\"\\[INST\\]\\s*\",            # Llama\n",
    "                r\"Human:\\s*\"               # Anthropic\n",
    "            ]\n",
    "            \n",
    "            found_user = False\n",
    "            for pattern in split_patterns:\n",
    "                matches = list(re.finditer(pattern, p_text_dirty, re.IGNORECASE))\n",
    "                if matches:\n",
    "                    last_match = matches[-1]\n",
    "                    p_text_clean = p_text_dirty[last_match.end():]\n",
    "                    found_user = True\n",
    "                    break\n",
    "            \n",
    "            # 2. 移除可能残留的 Assistant 标记 (Prompt 结尾)\n",
    "            stop_patterns = [\n",
    "                r\"<\\|im_start\\|>assistant\",\n",
    "                r\"assistant\\s*\\n\",\n",
    "                r\"Assistant:\",\n",
    "                r\"\\[/INST\\]\"\n",
    "            ]\n",
    "            for pattern in stop_patterns:\n",
    "                match = re.search(pattern, p_text_clean, re.IGNORECASE) # 使用 escape 防止正则错误\n",
    "                if match:\n",
    "                    p_text_clean = p_text_clean[:match.start()]\n",
    "            \n",
    "            # 3. 移除可能残留的 User 结束标记 (如 <|im_end|>)\n",
    "            if \"<|im_end|>\" in p_text_clean:\n",
    "                p_text_clean = p_text_clean.replace(\"<|im_end|>\", \"\")\n",
    "            \n",
    "            p_text_clean = p_text_clean.strip()\n",
    "            # ----------------------------------------\n",
    "\n",
    "            r_text = self.tokenizer.decode(r_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Prompt for the Teacher (Summary Generation)\n",
    "            content = (\n",
    "                f\"Question: {p_text_clean}\\n\\n\"\n",
    "                f\"Standard Answer: {gt_text}\\n\\n\"\n",
    "                f\"Student Answer: {r_text}\\n\\n\"\n",
    "                f\"Task: Verify the Student Answer step-by-step. Is it correct?\\n\"\n",
    "                f\"Answer concisely.\"\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ]\n",
    "\n",
    "            enc_ids = self.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            if enc_ids.dim() == 2:\n",
    "                enc_ids = enc_ids[0]\n",
    "\n",
    "            input_ids_list.append(enc_ids)\n",
    "            attention_mask_list.append(torch.ones_like(enc_ids))\n",
    "\n",
    "        # --- Manual Left Padding & Position IDs ---\n",
    "        max_len = max([len(t) for t in input_ids_list])\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        \n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        padded_position_ids = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids_list, attention_mask_list):\n",
    "            pad_len = max_len - len(ids)\n",
    "            \n",
    "            pad_ids = torch.full((pad_len,), pad_token_id, dtype=ids.dtype, device=ids.device)\n",
    "            pad_mask = torch.full((pad_len,), 0, dtype=mask.dtype, device=mask.device)\n",
    "            \n",
    "            final_ids = torch.cat([pad_ids, ids])\n",
    "            final_mask = torch.cat([pad_mask, mask])\n",
    "            \n",
    "            padded_input_ids.append(final_ids)\n",
    "            padded_attention_mask.append(final_mask)\n",
    "            \n",
    "            seq_len = len(ids)\n",
    "            pos_content = torch.arange(seq_len, dtype=torch.long, device=ids.device)\n",
    "            pos_pad = torch.zeros(pad_len, dtype=torch.long, device=ids.device)\n",
    "            final_pos = torch.cat([pos_pad, pos_content])\n",
    "            \n",
    "            padded_position_ids.append(final_pos)\n",
    "            \n",
    "        input_ids = torch.stack(padded_input_ids)\n",
    "        attention_mask = torch.stack(padded_attention_mask)\n",
    "        position_ids = torch.stack(padded_position_ids)\n",
    "\n",
    "        summary_batch = DataProto.from_dict({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids\n",
    "        })\n",
    "        \n",
    "        summary_batch.meta_info = {\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.1, \n",
    "            \"max_new_tokens\": 512,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id\n",
    "        }\n",
    "        \n",
    "        return summary_batch\n",
    "\n",
    "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor) -> DataProto:\n",
    "        \"\"\"\n",
    "        构造 Teacher Forward 的 Batch。\n",
    "        逻辑：[Question] + [Summary (Hint)] -> Teacher 预测 [Correct Response]\n",
    "        实际输入：[Question] + [Summary] + [Student Response]\n",
    "        \n",
    "        如果 Student Response 是错的，Teacher (试图生成正确答案) 会给出低 LogProb。\n",
    "        如果 Student Response 是对的，Teacher (试图生成正确答案) 会给出高 LogProb。\n",
    "        \"\"\"\n",
    "        prompts = batch.batch['prompts']\n",
    "        responses = batch.batch['responses']\n",
    "        \n",
    "        new_input_ids = []\n",
    "        new_attention_mask = []\n",
    "        new_response_masks = []\n",
    "        new_prompts_list = []\n",
    "        new_responses_list = []\n",
    "\n",
    "        # 分隔符\n",
    "        sep_ids = self.tokenizer.encode(\"\\n\\nSolution:\\n\", add_special_tokens=False)\n",
    "        separator = torch.tensor(sep_ids, device=prompts.device)\n",
    "\n",
    "        for i in range(len(prompts)):\n",
    "            # 1. 解码 Prompt (包含 System + User Query)\n",
    "            p_ids = prompts[i]\n",
    "            p_ids = p_ids[p_ids != self.tokenizer.pad_token_id]\n",
    "            p_text_dirty = self.tokenizer.decode(p_ids, skip_special_tokens=False)\n",
    "            \n",
    "            # --- 强力清洗逻辑 (只提取 User Query) ---\n",
    "            p_text_clean = p_text_dirty\n",
    "            # 尝试截取最后一个 User 标记之后的内容\n",
    "            split_patterns = [r\"<\\|im_start\\|>user\\s*\", r\"user\\s*\\n\", r\"User:\\s*\", r\"\\[INST\\]\\s*\", r\"Human:\\s*\"]\n",
    "            for pattern in split_patterns:\n",
    "                matches = list(re.finditer(pattern, p_text_dirty, re.IGNORECASE))\n",
    "                if matches:\n",
    "                    p_text_clean = p_text_dirty[matches[-1].end():]\n",
    "                    break\n",
    "            \n",
    "            stop_patterns = [r\"<\\|im_start\\|>assistant\", r\"assistant\\s*\\n\", r\"Assistant:\", r\"\\[/INST\\]\"]\n",
    "            for pattern in stop_patterns:\n",
    "                match = re.search(pattern, p_text_clean, re.IGNORECASE)\n",
    "                if match:\n",
    "                    p_text_clean = p_text_clean[:match.start()]\n",
    "            \n",
    "            if \"<|im_end|>\" in p_text_clean:\n",
    "                p_text_clean = p_text_clean.replace(\"<|im_end|>\", \"\")\n",
    "            \n",
    "            p_text_clean = p_text_clean.strip()\n",
    "            # ----------------------------------------\n",
    "            \n",
    "            # 2. 获取 Summary (作为 Hint/Analysis)\n",
    "            s_ids = summaries[i]\n",
    "            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
    "            s_text = self.tokenizer.decode(s_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # 3. 获取 Student Response (Target)\n",
    "            r_ids = responses[i]\n",
    "            r_ids = r_ids[r_ids != self.tokenizer.pad_token_id]\n",
    "            \n",
    "            # 4. 构造 Teacher 的 Context (Corrective Guidance)\n",
    "            # 这里的 Prompt 设计至关重要\n",
    "            teacher_content = (\n",
    "                f\"Problem: {p_text_clean}\\n\\n\"\n",
    "                f\"Analysis/Hint: {s_text}\\n\\n\"\n",
    "                f\"Based on the analysis above, please provide the correct, step-by-step solution.\"\n",
    "            )\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": teacher_content}\n",
    "            ]\n",
    "            \n",
    "            # 5. Apply Chat Template\n",
    "            prefix_ids = self.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_tensors='pt'\n",
    "            )[0].to(r_ids.device)\n",
    "            \n",
    "            # 6. 拼接: [Prefix] + [Separator] + [Student Response]\n",
    "            # Teacher 看到的是：\n",
    "            # User: Problem + Hint + \"Please provide correct solution\"\n",
    "            # Assistant: [Student Response]\n",
    "            \n",
    "            # 如果 Student Response 是错的，Teacher 本来想生成对的，结果看到错的 -> LogProb 低\n",
    "            \n",
    "            # 注意：separator 可以直接拼在 prefix_ids 后面，或者作为 response 的开头\n",
    "            # 这里我们不加额外的 separator，因为 apply_chat_template 已经加了 assistant 标签\n",
    "            # 直接拼接 r_ids 即可\n",
    "            \n",
    "            teacher_full = torch.cat([prefix_ids, r_ids])\n",
    "            \n",
    "            # 7. Masks\n",
    "            att_mask = torch.ones_like(teacher_full)\n",
    "            resp_mask = torch.zeros_like(teacher_full)\n",
    "            \n",
    "            # 只计算 Response 部分的 LogProb\n",
    "            start_idx = len(prefix_ids)\n",
    "            resp_mask[start_idx:] = 1 \n",
    "            \n",
    "            new_input_ids.append(teacher_full)\n",
    "            new_attention_mask.append(att_mask)\n",
    "            new_response_masks.append(resp_mask)\n",
    "            \n",
    "            new_prompts_list.append(prefix_ids)\n",
    "            new_responses_list.append(r_ids)\n",
    "\n",
    "        # Padding\n",
    "        input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        response_mask = pad_sequence(new_response_masks, batch_first=True, padding_value=0)\n",
    "        \n",
    "        prompts_padded = pad_sequence(new_prompts_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        responses_padded = pad_sequence(new_responses_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 0)\n",
    "\n",
    "        teacher_batch = DataProto.from_dict({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"response_mask\": response_mask,\n",
    "            \"prompts\": prompts_padded,   \n",
    "            \"responses\": responses_padded \n",
    "        })\n",
    "        \n",
    "        return teacher_batch\n",
    "\n",
    "    def fit(self):\n",
    "        logger = Tracking(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "            default_backend=self.config.trainer.logger,\n",
    "            config=OmegaConf.to_container(self.config, resolve=True),\n",
    "        )\n",
    "\n",
    "        self.global_steps = 0\n",
    "        self._load_checkpoint()\n",
    "\n",
    "        progress_bar = tqdm(total=self.total_training_steps, initial=self.global_steps, desc=\"Training Progress\")\n",
    "        self.global_steps += 1\n",
    "\n",
    "        # === Sanity Check: Is Teacher Brain-Dead? ===\n",
    "        print(\"Running Teacher Sanity Check...\")\n",
    "        test_prompts = [\"1+1=\"]\n",
    "        test_responses = [\"2\"]\n",
    "        \n",
    "        # 1. 构造单个样本\n",
    "        p_ids = self.tokenizer.encode(test_prompts[0], return_tensors='pt')[0]\n",
    "        r_ids = self.tokenizer.encode(test_responses[0], return_tensors='pt')[0]\n",
    "        \n",
    "        input_ids = torch.cat([p_ids, r_ids])\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        position_ids = torch.arange(len(input_ids))\n",
    "        response_mask = torch.zeros_like(input_ids)\n",
    "        response_mask[len(p_ids):] = 1\n",
    "        \n",
    "        # 2. 获取 Worker 数量 (World Size)\n",
    "        # 我们需要让 batch size 能够被 world size 整除\n",
    "        world_size = self.ref_policy_wg.world_size\n",
    "        \n",
    "        # 3. 复制数据以匹配 World Size\n",
    "        input_ids = input_ids.unsqueeze(0).repeat(world_size, 1)\n",
    "        attention_mask = attention_mask.unsqueeze(0).repeat(world_size, 1)\n",
    "        position_ids = position_ids.unsqueeze(0).repeat(world_size, 1)\n",
    "        response_mask = response_mask.unsqueeze(0).repeat(world_size, 1)\n",
    "        \n",
    "        # Prompts 和 Responses 在 DataProto 中主要是占位，但也需要对齐\n",
    "        # 注意：DataProto.from_dict 对 tensor 的处理比较严格，这里我们只放必要的 keys\n",
    "        # 为了避免复杂，我们只放 input_ids 等核心 tensor，prompts/responses 可以留空或者也 repeat\n",
    "        # compute_log_prob 主要依赖 input_ids, attention_mask, position_ids, responses(用于mask)\n",
    "        \n",
    "        # 重新构造 responses (tensor)\n",
    "        responses_tensor = r_ids.unsqueeze(0).repeat(world_size, 1)\n",
    "\n",
    "        sanity_batch = DataProto.from_dict({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"response_mask\": response_mask,\n",
    "            \"responses\": responses_tensor # 必须有，用于 compute_log_prob 内部切片\n",
    "        })\n",
    "        \n",
    "        # 补充 meta_info\n",
    "        sanity_batch.meta_info = {\n",
    "            \"micro_batch_size\": world_size, # 一次性跑完\n",
    "            \"temperature\": 1.0,\n",
    "            \"use_dynamic_bsz\": False\n",
    "        }\n",
    "        \n",
    "        # Teacher Forward\n",
    "        out = self.ref_policy_wg.compute_log_prob(sanity_batch)\n",
    "        \n",
    "        # 取第一个样本的结果\n",
    "        log_prob = out.batch['old_log_probs'][0, 0].item() \n",
    "        \n",
    "        print(f\"Teacher Sanity Check: LogProb('2' | '1+1=') = {log_prob:.4f}\")\n",
    "        \n",
    "        if log_prob < -5.0:\n",
    "            print(\"!!! CRITICAL WARNING: Teacher LogProb is extremely low. Model might be randomly initialized!\")\n",
    "            # raise RuntimeError(\"Teacher Model is broken!\")\n",
    "        else:\n",
    "            print(\"Teacher seems healthy.\")\n",
    "        # ============================================\n",
    "\n",
    "        for epoch in range(self.config.trainer.total_epochs):\n",
    "            for batch_dict in self.train_dataloader:\n",
    "                metrics = {}\n",
    "                timing_raw = {}\n",
    "\n",
    "                batch: DataProto = DataProto.from_single_dict(batch_dict)\n",
    "                \n",
    "                # --- Step 1: Data Alignment ---\n",
    "                B = batch.batch.batch_size[0]\n",
    "                N = self.config.actor_rollout_ref.rollout.n\n",
    "                \n",
    "                raw_prompts_list = []\n",
    "                for p_ids in batch.batch['input_ids']: \n",
    "                    raw_prompts_list.append(self.tokenizer.decode(p_ids, skip_special_tokens=True))\n",
    "                \n",
    "                ground_truths_list = []\n",
    "                possible_gt_keys = [\"ground_truth\", \"solution\", \"reward_model\"]\n",
    "                found_gt = False\n",
    "                for key in possible_gt_keys:\n",
    "                    if key in batch.non_tensor_batch:\n",
    "                        data = batch.non_tensor_batch[key]\n",
    "                        if key == \"reward_model\":\n",
    "                            ground_truths_list = [item.get('ground_truth', '') if isinstance(item, dict) else '' for item in data]\n",
    "                        else:\n",
    "                            ground_truths_list = data.tolist() if isinstance(data, np.ndarray) else data\n",
    "                        found_gt = True\n",
    "                        break\n",
    "                if not found_gt:\n",
    "                    ground_truths_list = [\"\"] * B\n",
    "\n",
    "                expanded_raw_prompts = [p for p in raw_prompts_list for _ in range(N)]\n",
    "                expanded_ground_truths = [g for g in ground_truths_list for _ in range(N)]\n",
    "                \n",
    "                batch_keys_to_pop = ['input_ids', 'attention_mask', 'position_ids']\n",
    "                possible_non_tensor_keys = ['raw_prompt_ids', 'multi_modal_data', 'raw_prompt', 'tools_kwargs', 'interaction_kwargs', 'index', 'agent_name']\n",
    "                non_tensor_batch_keys_to_pop = [k for k in possible_non_tensor_keys if k in batch.non_tensor_batch]\n",
    "                \n",
    "                gen_batch = batch.pop(batch_keys=batch_keys_to_pop, non_tensor_batch_keys=non_tensor_batch_keys_to_pop)\n",
    "                gen_batch.meta_info['global_steps'] = self.global_steps\n",
    "                gen_batch = gen_batch.repeat(repeat_times=N, interleave=True)\n",
    "\n",
    "                # --- Step 2: Student Generation ---\n",
    "                with marked_timer(\"gen_student\", timing_raw):\n",
    "                    if not self.async_rollout_mode:\n",
    "                        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
    "                    else:\n",
    "                        gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch)\n",
    "                    \n",
    "                    batch = batch.repeat(repeat_times=N, interleave=True)\n",
    "                    batch = batch.union(gen_batch_output)\n",
    "\n",
    "                batch.non_tensor_batch[\"uid\"] = np.array(\n",
    "                    [str(uuid.uuid4()) for _ in range(len(batch.batch))], dtype=object\n",
    "                )\n",
    "                if \"response_mask\" not in batch.batch.keys():\n",
    "                    batch.batch[\"response_mask\"] = compute_response_mask(batch)\n",
    "                batch.meta_info['global_token_num'] = torch.sum(batch.batch['attention_mask'], dim=-1).tolist()\n",
    "\n",
    "                # --- Step 3: Reflection & Reward ---\n",
    "                with marked_timer(\"reward_reflection\", timing_raw, color=\"yellow\"):\n",
    "                    # 3.1 Extract Prompts\n",
    "                    tensor_prompts = batch.batch['prompts']\n",
    "                    decoded_prompts = []\n",
    "                    for p_ids in tensor_prompts:\n",
    "                        decoded_prompts.append(self.tokenizer.decode(p_ids, skip_special_tokens=False))\n",
    "                    \n",
    "                    # 3.2 Extract GT\n",
    "                    current_ground_truths = []\n",
    "                    if \"reward_model\" in batch.non_tensor_batch:\n",
    "                        rm_data = batch.non_tensor_batch[\"reward_model\"]\n",
    "                        current_ground_truths = [item.get('ground_truth', '') if isinstance(item, dict) else '' for item in rm_data]\n",
    "                    elif \"solution\" in batch.non_tensor_batch:\n",
    "                        current_ground_truths = batch.non_tensor_batch[\"solution\"].tolist()\n",
    "                    elif \"ground_truth\" in batch.non_tensor_batch:\n",
    "                        current_ground_truths = batch.non_tensor_batch[\"ground_truth\"].tolist()\n",
    "                    else:\n",
    "                        current_ground_truths = [\"\"] * len(tensor_prompts)\n",
    "\n",
    "                    # 3.3 Teacher Generates Summary\n",
    "                    summary_input_batch = self._prepare_summary_generation_batch(\n",
    "                        batch, \n",
    "                        decoded_prompts, \n",
    "                        current_ground_truths\n",
    "                    )\n",
    "                    \n",
    "                    summary_output = self.actor_rollout_wg.generate_sequences(summary_input_batch)\n",
    "                    summaries = summary_output.batch['responses']\n",
    "                    \n",
    "                    # 3.4 Teacher Computes LogProb\n",
    "                    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries)\n",
    "                    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
    "                    teacher_full_log_probs = teacher_log_prob_output.batch['old_log_probs']\n",
    "\n",
    "                    # 3.5 Student Computes LogProb\n",
    "                    student_log_prob_output = self.actor_rollout_wg.compute_log_prob(batch)\n",
    "                    student_full_log_probs = student_log_prob_output.batch['old_log_probs']\n",
    "                    \n",
    "                    # === Log Part 1: Generation Content ===\n",
    "                    if True:\n",
    "                        print(f\"\\n{'='*20} Teacher-Student Reflection Debug (Step {self.global_steps}) {'='*20}\")\n",
    "                        try:\n",
    "                            idx = 0 \n",
    "                            # Summary Input\n",
    "                            sum_in_ids = summary_input_batch.batch['input_ids'][idx]\n",
    "                            sum_in_ids = sum_in_ids[sum_in_ids != self.tokenizer.pad_token_id]\n",
    "                            sum_in_text = self.tokenizer.decode(sum_in_ids, skip_special_tokens=False)\n",
    "                            \n",
    "                            # Summary Output\n",
    "                            s_ids = summaries[idx]\n",
    "                            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
    "                            s_text = self.tokenizer.decode(s_ids, skip_special_tokens=True)\n",
    "                            \n",
    "                            # Teacher Input (Prompt + Summary + Response)\n",
    "                            # 这里展示 Teacher LogProb 看到的完整输入\n",
    "                            t_full_ids = teacher_batch.batch['input_ids'][idx]\n",
    "                            t_full_ids = t_full_ids[t_full_ids != self.tokenizer.pad_token_id]\n",
    "                            t_full_text = self.tokenizer.decode(t_full_ids, skip_special_tokens=False)\n",
    "\n",
    "                            print(f\"--- [0] Summary Generation Input ---\\n{sum_in_text.strip()}\\n\")\n",
    "                            print(f\"--- [1] Summary Output ---\\n{s_text.strip()}\\n\")\n",
    "                            print(f\"--- [2] Teacher LogProb Input (Full Context) ---\\n{t_full_text.strip()}\\n\") # 只看最后500字符\n",
    "                        except Exception as e:\n",
    "                            print(f\"Log Error 1: {e}\")\n",
    "                        print(f\"{'='*60}\\n\")\n",
    "                    # ======================================\n",
    "\n",
    "                    # 3.6 Reward Calculation\n",
    "                    token_level_rewards = torch.zeros_like(student_full_log_probs)\n",
    "                    s_probs = student_full_log_probs\n",
    "                    t_probs = teacher_full_log_probs\n",
    "                    \n",
    "                    min_len = min(s_probs.shape[1], t_probs.shape[1])\n",
    "                    s_part = s_probs[:, :min_len]\n",
    "                    t_part = t_probs[:, :min_len]\n",
    "                    \n",
    "                    kl_diff = t_part - s_part\n",
    "                    \n",
    "                    # === Log Part 2: Token-Level KL Analysis ===\n",
    "                    if True:\n",
    "                        print(f\"\\n{'='*20} Token-Level KL Analysis (Step {self.global_steps}) {'='*20}\")\n",
    "                        try:\n",
    "                            idx = 0 \n",
    "                            # 1. Student Response (Remove Pad)\n",
    "                            s_ids = batch.batch['responses'][idx]\n",
    "                            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
    "                            \n",
    "                            # 2. Teacher Response (Extract using Mask)\n",
    "                            t_full_ids = teacher_batch.batch['input_ids'][idx]\n",
    "                            t_mask = teacher_batch.batch['response_mask'][idx]\n",
    "                            t_ids = t_full_ids[t_mask.bool()]\n",
    "                            t_ids = t_ids[t_ids != self.tokenizer.pad_token_id]\n",
    "                            \n",
    "                            # 3. Decode\n",
    "                            s_text_check = self.tokenizer.decode(s_ids, skip_special_tokens=False)\n",
    "                            t_text_check = self.tokenizer.decode(t_ids, skip_special_tokens=False)\n",
    "                            \n",
    "                            print(f\"--- Sequence Alignment Check ---\")\n",
    "\n",
    "                            \n",
    "                            if len(s_ids) != len(t_ids) or not torch.equal(s_ids, t_ids):\n",
    "                                print(\"[WARNING] ID Mismatch! Teacher sees different tokens than Student generated!\")\n",
    "                                print(f\"S IDs: {s_ids.tolist()}\")\n",
    "                                print(f\"T IDs: {t_ids.tolist()}\")\n",
    "                                print(f\"Student Seq (Len={len(s_ids)}): {s_text_check}...\")\n",
    "                                print(f\"Teacher Seq (Len={len(t_ids)}): {t_text_check}...\")\n",
    "                            else:\n",
    "                                print(\"[OK] Token IDs match perfectly.\")\n",
    "\n",
    "                            print(f\"\\n--- Token-wise KL Breakdown (First 50 tokens) ---\")\n",
    "                            print(f\"{'Token':<15} | {'ID':<6} | {'S_LogP':<8} | {'T_LogP':<8} | {'Diff':<8}\")\n",
    "                            print(\"-\" * 75)\n",
    "                            \n",
    "                            s_vals = s_part[idx].tolist()\n",
    "                            t_vals = t_part[idx].tolist()\n",
    "                            diff_vals = kl_diff[idx].tolist()\n",
    "                            \n",
    "                            valid_count = 0\n",
    "                            for i in range(len(s_ids)):\n",
    "                                if valid_count >= 50: break\n",
    "                                if i >= len(s_vals): break\n",
    "                                \n",
    "                                tid = s_ids[i].item()\n",
    "                                token_str = self.tokenizer.decode([tid]).replace('\\n', '\\\\n')\n",
    "                                print(f\"{token_str:<15} | {tid:<6} | {s_vals[i]:.2f}   | {t_vals[i]:.2f}   | {diff_vals[i]:.2f}\")\n",
    "                                valid_count += 1\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Log Error 2: {e}\")\n",
    "                        print(f\"{'='*60}\\n\")\n",
    "                    # ==========================================\n",
    "\n",
    "                    kl_diff = torch.clamp(kl_diff, min=-1.0, max=1.0)\n",
    "                    token_level_rewards[:, :min_len] = kl_diff\n",
    "                    \n",
    "                    batch.batch['token_level_rewards'] = token_level_rewards\n",
    "                    \n",
    "                    valid_mask = batch.batch['response_mask']\n",
    "                    with torch.no_grad():\n",
    "                        mean_kl = (token_level_rewards * valid_mask).sum() / (valid_mask.sum() + 1e-6)\n",
    "                        metrics['reward/reflection_kl'] = mean_kl.item()\n",
    "\n",
    "                # --- Step 4: PPO Flow ---\n",
    "                entropys = student_log_prob_output.batch['entropys']\n",
    "                entropy_agg = torch.mean(entropys)\n",
    "                metrics['actor/entropy'] = entropy_agg.item()\n",
    "                \n",
    "                student_log_prob_output.batch.pop('entropys', None)\n",
    "                batch = batch.union(student_log_prob_output)\n",
    "\n",
    "                if \"values\" not in batch.batch.keys():\n",
    "                    batch.batch[\"values\"] = torch.zeros_like(batch.batch[\"token_level_rewards\"])\n",
    "\n",
    "                with marked_timer(\"adv\", timing_raw):\n",
    "                    batch = compute_advantage(\n",
    "                        batch,\n",
    "                        adv_estimator=self.config.algorithm.adv_estimator,\n",
    "                        gamma=self.config.algorithm.gamma,\n",
    "                        lam=self.config.algorithm.lam,\n",
    "                        num_repeat=self.config.actor_rollout_ref.rollout.n,\n",
    "                        norm_adv_by_std_in_grpo=self.config.algorithm.get('norm_adv_by_std_in_grpo', True),\n",
    "                        config=self.config.algorithm\n",
    "                    )\n",
    "                \n",
    "                batch.batch['advantages'] = batch.batch['token_level_rewards']\n",
    "\n",
    "                with marked_timer(\"update_actor\", timing_raw):\n",
    "                    actor_output = self.actor_rollout_wg.update_actor(batch)\n",
    "                    actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n",
    "                    metrics.update(actor_output_metrics)\n",
    "\n",
    "                metrics.update({\n",
    "                    \"training/global_step\": self.global_steps,\n",
    "                    \"training/epoch\": epoch\n",
    "                })\n",
    "                \n",
    "                logger.log(data=metrics, step=self.global_steps)\n",
    "                progress_bar.update(1)\n",
    "                self.global_steps += 1\n",
    "                \n",
    "                if self.config.trainer.save_freq > 0 and self.global_steps % self.config.trainer.save_freq == 0:\n",
    "                    self._save_checkpoint()\n",
    "\n",
    "                if self.global_steps >= self.total_training_steps:\n",
    "                    progress_bar.close()\n",
    "                    return\n",
    "\"\"\"\n",
    "ray_trainer.py的代码如下\n",
    "\"\"\"\n",
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "# Copyright 2023-2024 SGLang Team\n",
    "# Copyright 2025 ModelBest Inc. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "PPO Trainer with Ray-based single controller.\n",
    "This trainer supports model-agonistic model initialization with huggingface\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from verl import DataProto\n",
    "from verl.experimental.dataset.sampler import AbstractCurriculumSampler\n",
    "from verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\n",
    "from verl.single_controller.base import Worker\n",
    "from verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\n",
    "from verl.single_controller.ray.base import create_colocated_worker_cls\n",
    "from verl.trainer.config import AlgoConfig\n",
    "from verl.trainer.ppo import core_algos\n",
    "from verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\n",
    "from verl.trainer.ppo.metric_utils import (\n",
    "    compute_data_metrics,\n",
    "    compute_throughout_metrics,\n",
    "    compute_timing_metrics,\n",
    "    process_validation_metrics,\n",
    ")\n",
    "from verl.trainer.ppo.reward import compute_reward, compute_reward_async\n",
    "from verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\n",
    "from verl.utils.debug import marked_timer\n",
    "from verl.utils.metric import (\n",
    "    reduce_metrics,\n",
    ")\n",
    "from verl.utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance\n",
    "from verl.utils.torch_functional import masked_mean\n",
    "from verl.utils.tracking import ValidationGenerationsLogger\n",
    "\n",
    "WorkerType = type[Worker]\n",
    "\n",
    "\n",
    "class Role(Enum):\n",
    "    \"\"\"\n",
    "    To create more roles dynamically, you can subclass Role and add new members\n",
    "    \"\"\"\n",
    "\n",
    "    Actor = 0\n",
    "    Rollout = 1\n",
    "    ActorRollout = 2\n",
    "    Critic = 3\n",
    "    RefPolicy = 4\n",
    "    RewardModel = 5\n",
    "    ActorRolloutRef = 6\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResourcePoolManager:\n",
    "    \"\"\"\n",
    "    Define a resource pool specification. Resource pool will be initialized first.\n",
    "    \"\"\"\n",
    "\n",
    "    resource_pool_spec: dict[str, list[int]]\n",
    "    mapping: dict[Role, str]\n",
    "    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n",
    "\n",
    "    def create_resource_pool(self):\n",
    "        \"\"\"Create Ray resource pools for distributed training.\n",
    "\n",
    "        Initializes resource pools based on the resource pool specification,\n",
    "        with each pool managing GPU resources across multiple nodes.\n",
    "        For FSDP backend, uses max_colocate_count=1 to merge WorkerGroups.\n",
    "        For Megatron backend, uses max_colocate_count>1 for different models.\n",
    "        \"\"\"\n",
    "        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():\n",
    "            # max_colocate_count means the number of WorkerGroups (i.e. processes) in each RayResourcePool\n",
    "            # For FSDP backend, we recommend using max_colocate_count=1 that merge all WorkerGroups into one.\n",
    "            # For Megatron backend, we recommend using max_colocate_count>1\n",
    "            # that can utilize different WorkerGroup for differnt models\n",
    "            resource_pool = RayResourcePool(\n",
    "                process_on_nodes=process_on_nodes, use_gpu=True, max_colocate_count=1, name_prefix=resource_pool_name\n",
    "            )\n",
    "            self.resource_pool_dict[resource_pool_name] = resource_pool\n",
    "\n",
    "        self._check_resource_available()\n",
    "\n",
    "    def get_resource_pool(self, role: Role) -> RayResourcePool:\n",
    "        \"\"\"Get the resource pool of the worker_cls\"\"\"\n",
    "        return self.resource_pool_dict[self.mapping[role]]\n",
    "\n",
    "    def get_n_gpus(self) -> int:\n",
    "        \"\"\"Get the number of gpus in this cluster.\"\"\"\n",
    "        return sum([n_gpus for process_on_nodes in self.resource_pool_spec.values() for n_gpus in process_on_nodes])\n",
    "\n",
    "    def _check_resource_available(self):\n",
    "        \"\"\"Check if the resource pool can be satisfied in this ray cluster.\"\"\"\n",
    "        node_available_resources = ray.state.available_resources_per_node()\n",
    "        node_available_gpus = {\n",
    "            node: node_info.get(\"GPU\", 0) if \"GPU\" in node_info else node_info.get(\"NPU\", 0)\n",
    "            for node, node_info in node_available_resources.items()\n",
    "        }\n",
    "\n",
    "        # check total required gpus can be satisfied\n",
    "        total_available_gpus = sum(node_available_gpus.values())\n",
    "        total_required_gpus = sum(\n",
    "            [n_gpus for process_on_nodes in self.resource_pool_spec.values() for n_gpus in process_on_nodes]\n",
    "        )\n",
    "        if total_available_gpus < total_required_gpus:\n",
    "            raise ValueError(\n",
    "                f\"Total available GPUs {total_available_gpus} is less than total desired GPUs {total_required_gpus}\"\n",
    "            )\n",
    "\n",
    "        # check each resource pool can be satisfied, O(#resource_pools * #nodes)\n",
    "        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():\n",
    "            num_gpus, num_nodes = process_on_nodes[0], len(process_on_nodes)\n",
    "            for node, available_gpus in node_available_gpus.items():\n",
    "                if available_gpus >= num_gpus:\n",
    "                    node_available_gpus[node] -= num_gpus\n",
    "                    num_nodes -= 1\n",
    "                    if num_nodes == 0:\n",
    "                        break\n",
    "            if num_nodes > 0:\n",
    "                raise ValueError(\n",
    "                    f\"Resource pool {resource_pool_name}: {num_gpus}*{num_nodes}\"\n",
    "                    + \"cannot be satisfied in this ray cluster\"\n",
    "                )\n",
    "\n",
    "\n",
    "def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty=\"kl\"):\n",
    "    \"\"\"Apply KL penalty to the token-level rewards.\n",
    "\n",
    "    This function computes the KL divergence between the reference policy and current policy,\n",
    "    then applies a penalty to the token-level rewards based on this divergence.\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): The data containing batched model outputs and inputs.\n",
    "        kl_ctrl (core_algos.AdaptiveKLController): Controller for adaptive KL penalty.\n",
    "        kl_penalty (str, optional): Type of KL penalty to apply. Defaults to \"kl\".\n",
    "        multi_turn (bool, optional): Whether the data is from a multi-turn conversation. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - The updated data with token-level rewards adjusted by KL penalty\n",
    "            - A dictionary of metrics related to the KL penalty\n",
    "    \"\"\"\n",
    "    response_mask = data.batch[\"response_mask\"]\n",
    "    token_level_scores = data.batch[\"token_level_scores\"]\n",
    "    batch_size = data.batch.batch_size[0]\n",
    "\n",
    "    # compute kl between ref_policy and current policy\n",
    "    # When apply_kl_penalty, algorithm.use_kl_in_reward=True, so the reference model has been enabled.\n",
    "    kld = core_algos.kl_penalty(\n",
    "        data.batch[\"old_log_probs\"], data.batch[\"ref_log_prob\"], kl_penalty=kl_penalty\n",
    "    )  # (batch_size, response_length)\n",
    "    kld = kld * response_mask\n",
    "    beta = kl_ctrl.value\n",
    "\n",
    "    token_level_rewards = token_level_scores - beta * kld\n",
    "\n",
    "    current_kl = masked_mean(kld, mask=response_mask, axis=-1)  # average over sequence\n",
    "    current_kl = torch.mean(current_kl, dim=0).item()\n",
    "\n",
    "    # according to https://github.com/huggingface/trl/blob/951ca1841f29114b969b57b26c7d3e80a39f75a0/trl/trainer/ppo_trainer.py#L837\n",
    "    kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)\n",
    "    data.batch[\"token_level_rewards\"] = token_level_rewards\n",
    "\n",
    "    metrics = {\"actor/reward_kl_penalty\": current_kl, \"actor/reward_kl_penalty_coeff\": beta}\n",
    "\n",
    "    return data, metrics\n",
    "\n",
    "\n",
    "def compute_response_mask(data: DataProto):\n",
    "    \"\"\"Compute the attention mask for the response part of the sequence.\n",
    "\n",
    "    This function extracts the portion of the attention mask that corresponds to the model's response,\n",
    "    which is used for masking computations that should only apply to response tokens.\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): The data containing batched model outputs and inputs.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The attention mask for the response tokens.\n",
    "    \"\"\"\n",
    "    responses = data.batch[\"responses\"]\n",
    "    response_length = responses.size(1)\n",
    "    attention_mask = data.batch[\"attention_mask\"]\n",
    "    return attention_mask[:, -response_length:]\n",
    "\n",
    "\n",
    "def compute_advantage(\n",
    "    data: DataProto,\n",
    "    adv_estimator: AdvantageEstimator,\n",
    "    gamma: float = 1.0,\n",
    "    lam: float = 1.0,\n",
    "    num_repeat: int = 1,\n",
    "    norm_adv_by_std_in_grpo: bool = True,\n",
    "    config: Optional[AlgoConfig] = None,\n",
    ") -> DataProto:\n",
    "    \"\"\"Compute advantage estimates for policy optimization.\n",
    "\n",
    "    This function computes advantage estimates using various estimators like GAE, GRPO, REINFORCE++, etc.\n",
    "    The advantage estimates are used to guide policy optimization in RL algorithms.\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): The data containing batched model outputs and inputs.\n",
    "        adv_estimator (AdvantageEstimator): The advantage estimator to use (e.g., GAE, GRPO, REINFORCE++).\n",
    "        gamma (float, optional): Discount factor for future rewards. Defaults to 1.0.\n",
    "        lam (float, optional): Lambda parameter for GAE. Defaults to 1.0.\n",
    "        num_repeat (int, optional): Number of times to repeat the computation. Defaults to 1.\n",
    "        norm_adv_by_std_in_grpo (bool, optional): Whether to normalize advantages by standard deviation in\n",
    "            GRPO. Defaults to True.\n",
    "        config (dict, optional): Configuration dictionary for algorithm settings. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataProto: The updated data with computed advantages and returns.\n",
    "    \"\"\"\n",
    "    # Back-compatible with trainers that do not compute response mask in fit\n",
    "    if \"response_mask\" not in data.batch.keys():\n",
    "        data.batch[\"response_mask\"] = compute_response_mask(data)\n",
    "    # prepare response group\n",
    "    if adv_estimator == AdvantageEstimator.GAE:\n",
    "        # Compute advantages and returns using Generalized Advantage Estimation (GAE)\n",
    "        advantages, returns = core_algos.compute_gae_advantage_return(\n",
    "            token_level_rewards=data.batch[\"token_level_rewards\"],\n",
    "            values=data.batch[\"values\"],\n",
    "            response_mask=data.batch[\"response_mask\"],\n",
    "            gamma=gamma,\n",
    "            lam=lam,\n",
    "        )\n",
    "        data.batch[\"advantages\"] = advantages\n",
    "        data.batch[\"returns\"] = returns\n",
    "        if config.get(\"use_pf_ppo\", False):\n",
    "            data = core_algos.compute_pf_ppo_reweight_data(\n",
    "                data,\n",
    "                config.pf_ppo.reweight_method,\n",
    "                config.pf_ppo.weight_pow,\n",
    "            )\n",
    "    elif adv_estimator == AdvantageEstimator.GRPO:\n",
    "        # Initialize the mask for GRPO calculation\n",
    "        grpo_calculation_mask = data.batch[\"response_mask\"]\n",
    "        # Call compute_grpo_outcome_advantage with parameters matching its definition\n",
    "        advantages, returns = core_algos.compute_grpo_outcome_advantage(\n",
    "            token_level_rewards=data.batch[\"token_level_rewards\"],\n",
    "            response_mask=grpo_calculation_mask,\n",
    "            index=data.non_tensor_batch[\"uid\"],\n",
    "            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n",
    "        )\n",
    "        data.batch[\"advantages\"] = advantages\n",
    "        data.batch[\"returns\"] = returns\n",
    "    else:\n",
    "        # handle all other adv estimator type other than GAE and GRPO\n",
    "        adv_estimator_fn = core_algos.get_adv_estimator_fn(adv_estimator)\n",
    "        adv_kwargs = {\n",
    "            \"token_level_rewards\": data.batch[\"token_level_rewards\"],\n",
    "            \"response_mask\": data.batch[\"response_mask\"],\n",
    "            \"config\": config,\n",
    "        }\n",
    "        if \"uid\" in data.non_tensor_batch:  # optional\n",
    "            adv_kwargs[\"index\"] = data.non_tensor_batch[\"uid\"]\n",
    "        if \"reward_baselines\" in data.batch:  # optional\n",
    "            adv_kwargs[\"reward_baselines\"] = data.batch[\"reward_baselines\"]\n",
    "\n",
    "        # calculate advantage estimator\n",
    "        advantages, returns = adv_estimator_fn(**adv_kwargs)\n",
    "        data.batch[\"advantages\"] = advantages\n",
    "        data.batch[\"returns\"] = returns\n",
    "    return data\n",
    "\n",
    "\n",
    "class RayPPOTrainer:\n",
    "    \"\"\"Distributed PPO trainer using Ray for scalable reinforcement learning.\n",
    "\n",
    "    This trainer orchestrates distributed PPO training across multiple nodes and GPUs,\n",
    "    managing actor rollouts, critic training, and reward computation with Ray backend.\n",
    "    Supports various model architectures including FSDP, Megatron, and vLLM integration.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: support each role have individual ray_worker_group_cls,\n",
    "    # i.e., support different backend of different role\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        tokenizer,\n",
    "        role_worker_mapping: dict[Role, WorkerType],\n",
    "        resource_pool_manager: ResourcePoolManager,\n",
    "        ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup,\n",
    "        processor=None,\n",
    "        reward_fn=None,\n",
    "        val_reward_fn=None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        val_dataset: Optional[Dataset] = None,\n",
    "        collate_fn=None,\n",
    "        train_sampler: Optional[Sampler] = None,\n",
    "        device_name=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize distributed PPO trainer with Ray backend.\n",
    "        Note that this trainer runs on the driver process on a single CPU/GPU node.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object containing training parameters.\n",
    "            tokenizer: Tokenizer used for encoding and decoding text.\n",
    "            role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes.\n",
    "            resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools.\n",
    "            ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup.\n",
    "            processor: Optional data processor, used for multimodal data\n",
    "            reward_fn: Function for computing rewards during training.\n",
    "            val_reward_fn: Function for computing rewards during validation.\n",
    "            train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None.\n",
    "            val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None.\n",
    "            collate_fn: Function to collate data samples into batches.\n",
    "            train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None.\n",
    "            device_name (str, optional): Device name for training (e.g., \"cuda\", \"cpu\"). Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the tokenizer for text processing\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.reward_fn = reward_fn\n",
    "        self.val_reward_fn = val_reward_fn\n",
    "\n",
    "        self.hybrid_engine = config.actor_rollout_ref.hybrid_engine\n",
    "        assert self.hybrid_engine, \"Currently, only support hybrid engine\"\n",
    "\n",
    "        if self.hybrid_engine:\n",
    "            assert Role.ActorRollout in role_worker_mapping, f\"{role_worker_mapping.keys()=}\"\n",
    "\n",
    "        self.role_worker_mapping = role_worker_mapping\n",
    "        self.resource_pool_manager = resource_pool_manager\n",
    "        self.use_reference_policy = Role.RefPolicy in role_worker_mapping\n",
    "        self.use_rm = Role.RewardModel in role_worker_mapping\n",
    "        self.ray_worker_group_cls = ray_worker_group_cls\n",
    "        self.device_name = device_name if device_name else self.config.trainer.device\n",
    "        self.validation_generations_logger = ValidationGenerationsLogger(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "        )\n",
    "\n",
    "        # if ref_in_actor is True, the reference policy will be actor without lora applied\n",
    "        self.ref_in_actor = config.actor_rollout_ref.model.get(\"lora_rank\", 0) > 0\n",
    "\n",
    "        # define in-reward KL control\n",
    "        # kl loss control currently not suppoorted\n",
    "        if self.config.algorithm.use_kl_in_reward:\n",
    "            self.kl_ctrl_in_reward = core_algos.get_kl_controller(self.config.algorithm.kl_ctrl)\n",
    "\n",
    "        if self.config.algorithm.adv_estimator == AdvantageEstimator.GAE:\n",
    "            self.use_critic = True\n",
    "        elif self.config.algorithm.adv_estimator in [\n",
    "            AdvantageEstimator.GRPO,\n",
    "            AdvantageEstimator.GRPO_PASSK,\n",
    "            AdvantageEstimator.REINFORCE_PLUS_PLUS,\n",
    "            AdvantageEstimator.REMAX,\n",
    "            AdvantageEstimator.RLOO,\n",
    "            AdvantageEstimator.OPO,\n",
    "            AdvantageEstimator.REINFORCE_PLUS_PLUS_BASELINE,\n",
    "            AdvantageEstimator.GPG,\n",
    "        ]:\n",
    "            self.use_critic = False\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self._validate_config()\n",
    "        self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler)\n",
    "\n",
    "    def _validate_config(self):\n",
    "        config = self.config\n",
    "        # number of GPUs total\n",
    "        n_gpus = config.trainer.n_gpus_per_node * config.trainer.nnodes\n",
    "        if config.actor_rollout_ref.actor.strategy == \"megatron\":\n",
    "            model_parallel_size = (\n",
    "                config.actor_rollout_ref.actor.megatron.tensor_model_parallel_size\n",
    "                * config.actor_rollout_ref.actor.megatron.pipeline_model_parallel_size\n",
    "            )\n",
    "            assert (\n",
    "                n_gpus % (model_parallel_size * config.actor_rollout_ref.actor.megatron.context_parallel_size) == 0\n",
    "            ), (\n",
    "                f\"n_gpus ({n_gpus}) must be divisible by model_parallel_size ({model_parallel_size}) times \"\n",
    "                f\"context_parallel_size ({config.actor_rollout_ref.actor.megatron.context_parallel_size})\"\n",
    "            )\n",
    "            megatron_dp = n_gpus // (\n",
    "                model_parallel_size * config.actor_rollout_ref.actor.megatron.context_parallel_size\n",
    "            )\n",
    "            minimal_bsz = megatron_dp * config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu\n",
    "        else:\n",
    "            minimal_bsz = n_gpus\n",
    "\n",
    "        # 1. Check total batch size for data correctness\n",
    "        real_train_batch_size = config.data.train_batch_size * config.actor_rollout_ref.rollout.n\n",
    "        assert real_train_batch_size % minimal_bsz == 0, (\n",
    "            f\"real_train_batch_size ({real_train_batch_size}) must be divisible by minimal possible batch size \"\n",
    "            f\"({minimal_bsz})\"\n",
    "        )\n",
    "\n",
    "        # A helper function to check \"micro_batch_size\" vs \"micro_batch_size_per_gpu\"\n",
    "        # We throw an error if the user sets both. The new convention is \"..._micro_batch_size_per_gpu\".\n",
    "        def check_mutually_exclusive(mbs, mbs_per_gpu, name: str):\n",
    "            \"\"\"Validate mutually exclusive micro batch size configuration options.\n",
    "\n",
    "            Ensures that users don't set both deprecated micro_batch_size and\n",
    "            the new micro_batch_size_per_gpu parameters simultaneously.\n",
    "\n",
    "            Args:\n",
    "                mbs: Deprecated micro batch size parameter value.\n",
    "                mbs_per_gpu: New micro batch size per GPU parameter value.\n",
    "                name (str): Configuration section name for error messages.\n",
    "\n",
    "            Raises:\n",
    "                ValueError: If both parameters are set or neither is set.\n",
    "            \"\"\"\n",
    "            settings = {\n",
    "                \"actor_rollout_ref.actor\": \"micro_batch_size\",\n",
    "                \"critic\": \"micro_batch_size\",\n",
    "                \"reward_model\": \"micro_batch_size\",\n",
    "                \"actor_rollout_ref.ref\": \"log_prob_micro_batch_size\",\n",
    "                \"actor_rollout_ref.rollout\": \"log_prob_micro_batch_size\",\n",
    "            }\n",
    "\n",
    "            if name in settings:\n",
    "                param = settings[name]\n",
    "                param_per_gpu = f\"{param}_per_gpu\"\n",
    "\n",
    "                if mbs is None and mbs_per_gpu is None:\n",
    "                    raise ValueError(\n",
    "                        f\"[{name}] Please set at least one of '{name}.{param}' or '{name}.{param_per_gpu}'.\"\n",
    "                    )\n",
    "\n",
    "                if mbs is not None and mbs_per_gpu is not None:\n",
    "                    raise ValueError(\n",
    "                        f\"[{name}] You have set both '{name}.{param}' AND '{name}.{param_per_gpu}'. Please remove \"\n",
    "                        f\"'{name}.{param}' because only '*_{param_per_gpu}' is supported (the former is deprecated).\"\n",
    "                    )\n",
    "\n",
    "        if not config.actor_rollout_ref.actor.use_dynamic_bsz:\n",
    "            # actor: ppo_micro_batch_size vs. ppo_micro_batch_size_per_gpu\n",
    "            check_mutually_exclusive(\n",
    "                config.actor_rollout_ref.actor.ppo_micro_batch_size,\n",
    "                config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu,\n",
    "                \"actor_rollout_ref.actor\",\n",
    "            )\n",
    "\n",
    "            if self.use_reference_policy:\n",
    "                # reference: log_prob_micro_batch_size vs. log_prob_micro_batch_size_per_gpu\n",
    "                check_mutually_exclusive(\n",
    "                    config.actor_rollout_ref.ref.log_prob_micro_batch_size,\n",
    "                    config.actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu,\n",
    "                    \"actor_rollout_ref.ref\",\n",
    "                )\n",
    "\n",
    "            #  The rollout section also has log_prob_micro_batch_size vs. log_prob_micro_batch_size_per_gpu\n",
    "            check_mutually_exclusive(\n",
    "                config.actor_rollout_ref.rollout.log_prob_micro_batch_size,\n",
    "                config.actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu,\n",
    "                \"actor_rollout_ref.rollout\",\n",
    "            )\n",
    "\n",
    "        if self.use_critic and not config.critic.use_dynamic_bsz:\n",
    "            # Check for critic micro-batch size conflicts\n",
    "            check_mutually_exclusive(\n",
    "                config.critic.ppo_micro_batch_size, config.critic.ppo_micro_batch_size_per_gpu, \"critic\"\n",
    "            )\n",
    "\n",
    "        # Check for reward model micro-batch size conflicts\n",
    "        if config.reward_model.enable and not config.reward_model.use_dynamic_bsz:\n",
    "            check_mutually_exclusive(\n",
    "                config.reward_model.micro_batch_size, config.reward_model.micro_batch_size_per_gpu, \"reward_model\"\n",
    "            )\n",
    "\n",
    "        # Actor\n",
    "        # check if train_batch_size is larger than ppo_mini_batch_size\n",
    "        # if NOT dynamic_bsz, we must ensure:\n",
    "        #    ppo_mini_batch_size is divisible by ppo_micro_batch_size\n",
    "        #    ppo_micro_batch_size * sequence_parallel_size >= n_gpus\n",
    "        if not config.actor_rollout_ref.actor.use_dynamic_bsz:\n",
    "            assert config.data.train_batch_size >= config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
    "            sp_size = config.actor_rollout_ref.actor.get(\"ulysses_sequence_parallel_size\", 1)\n",
    "            if config.actor_rollout_ref.actor.ppo_micro_batch_size is not None:\n",
    "                assert (\n",
    "                    config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
    "                    % config.actor_rollout_ref.actor.ppo_micro_batch_size\n",
    "                    == 0\n",
    "                )\n",
    "                assert config.actor_rollout_ref.actor.ppo_micro_batch_size * sp_size >= n_gpus\n",
    "\n",
    "        assert config.actor_rollout_ref.actor.loss_agg_mode in [\n",
    "            \"token-mean\",\n",
    "            \"seq-mean-token-sum\",\n",
    "            \"seq-mean-token-mean\",\n",
    "            \"seq-mean-token-sum-norm\",\n",
    "        ], f\"Invalid loss_agg_mode: {config.actor_rollout_ref.actor.loss_agg_mode}\"\n",
    "\n",
    "        if self.config.algorithm.use_kl_in_reward and config.actor_rollout_ref.actor.use_kl_loss:\n",
    "            print(\"NOTICE: You have both enabled in-reward kl and kl loss.\")\n",
    "\n",
    "        # critic\n",
    "        if self.use_critic and not config.critic.use_dynamic_bsz:\n",
    "            assert config.data.train_batch_size >= config.critic.ppo_mini_batch_size\n",
    "            sp_size = config.critic.get(\"ulysses_sequence_parallel_size\", 1)\n",
    "            if config.critic.ppo_micro_batch_size is not None:\n",
    "                assert config.critic.ppo_mini_batch_size % config.critic.ppo_micro_batch_size == 0\n",
    "                assert config.critic.ppo_micro_batch_size * sp_size >= n_gpus\n",
    "\n",
    "        # Check if use_remove_padding is enabled when using sequence parallelism for fsdp\n",
    "        if config.actor_rollout_ref.actor.strategy in {\"fsdp\", \"fsdp2\"} and (\n",
    "            config.actor_rollout_ref.actor.get(\"ulysses_sequence_parallel_size\", 1) > 1\n",
    "            or config.actor_rollout_ref.ref.get(\"ulysses_sequence_parallel_size\", 1) > 1\n",
    "        ):\n",
    "            assert config.actor_rollout_ref.model.use_remove_padding, (\n",
    "                \"When using sequence parallelism for actor/ref policy, you must enable `use_remove_padding`.\"\n",
    "            )\n",
    "\n",
    "        if self.use_critic and config.critic.strategy in {\"fsdp\", \"fsdp2\"}:\n",
    "            if config.critic.get(\"ulysses_sequence_parallel_size\", 1) > 1:\n",
    "                assert config.critic.model.use_remove_padding, (\n",
    "                    \"When using sequence parallelism for critic, you must enable `use_remove_padding`.\"\n",
    "                )\n",
    "\n",
    "        if config.data.get(\"val_batch_size\", None) is not None:\n",
    "            print(\n",
    "                \"WARNING: val_batch_size is deprecated.\"\n",
    "                + \" Validation datasets are sent to inference engines as a whole batch,\"\n",
    "                + \" which will schedule the memory themselves.\"\n",
    "            )\n",
    "\n",
    "        # check eval config\n",
    "        if config.actor_rollout_ref.rollout.val_kwargs.do_sample:\n",
    "            assert config.actor_rollout_ref.rollout.temperature > 0, (\n",
    "                \"validation gen temperature should be greater than 0 when enabling do_sample\"\n",
    "            )\n",
    "\n",
    "        print(\"[validate_config] All configuration checks passed successfully!\")\n",
    "\n",
    "    def _create_dataloader(self, train_dataset, val_dataset, collate_fn, train_sampler: Optional[Sampler]):\n",
    "        \"\"\"\n",
    "        Creates the train and validation dataloaders.\n",
    "        \"\"\"\n",
    "        # TODO: we have to make sure the batch size is divisible by the dp size\n",
    "        from verl.trainer.main_ppo import create_rl_dataset, create_rl_sampler\n",
    "\n",
    "        if train_dataset is None:\n",
    "            train_dataset = create_rl_dataset(\n",
    "                self.config.data.train_files, self.config.data, self.tokenizer, self.processor\n",
    "            )\n",
    "        if val_dataset is None:\n",
    "            val_dataset = create_rl_dataset(\n",
    "                self.config.data.val_files, self.config.data, self.tokenizer, self.processor\n",
    "            )\n",
    "        self.train_dataset, self.val_dataset = train_dataset, val_dataset\n",
    "\n",
    "        if train_sampler is None:\n",
    "            train_sampler = create_rl_sampler(self.config.data, self.train_dataset)\n",
    "        if collate_fn is None:\n",
    "            from verl.utils.dataset.rl_dataset import collate_fn as default_collate_fn\n",
    "\n",
    "            collate_fn = default_collate_fn\n",
    "\n",
    "        num_workers = self.config.data[\"dataloader_num_workers\"]\n",
    "\n",
    "        self.train_dataloader = StatefulDataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.config.data.get(\"gen_batch_size\", self.config.data.train_batch_size),\n",
    "            num_workers=num_workers,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn,\n",
    "            sampler=train_sampler,\n",
    "        )\n",
    "\n",
    "        val_batch_size = self.config.data.val_batch_size  # Prefer config value if set\n",
    "        if val_batch_size is None:\n",
    "            val_batch_size = len(self.val_dataset)\n",
    "\n",
    "        self.val_dataloader = StatefulDataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=val_batch_size,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=self.config.data.get(\"validation_shuffle\", True),\n",
    "            drop_last=False,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "        assert len(self.train_dataloader) >= 1, \"Train dataloader is empty!\"\n",
    "        assert len(self.val_dataloader) >= 1, \"Validation dataloader is empty!\"\n",
    "\n",
    "        print(\n",
    "            f\"Size of train dataloader: {len(self.train_dataloader)}, Size of val dataloader: \"\n",
    "            f\"{len(self.val_dataloader)}\"\n",
    "        )\n",
    "\n",
    "        total_training_steps = len(self.train_dataloader) * self.config.trainer.total_epochs\n",
    "\n",
    "        if self.config.trainer.total_training_steps is not None:\n",
    "            total_training_steps = self.config.trainer.total_training_steps\n",
    "\n",
    "        self.total_training_steps = total_training_steps\n",
    "        print(f\"Total training steps: {self.total_training_steps}\")\n",
    "\n",
    "        try:\n",
    "            OmegaConf.set_struct(self.config, True)\n",
    "            with open_dict(self.config):\n",
    "                if OmegaConf.select(self.config, \"actor_rollout_ref.actor.optim\"):\n",
    "                    self.config.actor_rollout_ref.actor.optim.total_training_steps = total_training_steps\n",
    "                if OmegaConf.select(self.config, \"critic.optim\"):\n",
    "                    self.config.critic.optim.total_training_steps = total_training_steps\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set total_training_steps in config. Structure missing? Error: {e}\")\n",
    "\n",
    "    def _dump_generations(self, inputs, outputs, scores, reward_extra_infos_dict, dump_path):\n",
    "        \"\"\"Dump rollout/validation samples as JSONL.\"\"\"\n",
    "        os.makedirs(dump_path, exist_ok=True)\n",
    "        filename = os.path.join(dump_path, f\"{self.global_steps}.jsonl\")\n",
    "\n",
    "        n = len(inputs)\n",
    "        base_data = {\n",
    "            \"input\": inputs,\n",
    "            \"output\": outputs,\n",
    "            \"score\": scores,\n",
    "            \"step\": [self.global_steps] * n,\n",
    "        }\n",
    "\n",
    "        for k, v in reward_extra_infos_dict.items():\n",
    "            if len(v) == n:\n",
    "                base_data[k] = v\n",
    "\n",
    "        lines = []\n",
    "        for i in range(n):\n",
    "            entry = {k: v[i] for k, v in base_data.items()}\n",
    "            lines.append(json.dumps(entry, ensure_ascii=False))\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "        print(f\"Dumped generations to {filename}\")\n",
    "\n",
    "    def _maybe_log_val_generations(self, inputs, outputs, scores):\n",
    "        \"\"\"Log a table of validation samples to the configured logger (wandb or swanlab)\"\"\"\n",
    "\n",
    "        generations_to_log = self.config.trainer.log_val_generations\n",
    "\n",
    "        if generations_to_log == 0:\n",
    "            return\n",
    "\n",
    "        import numpy as np\n",
    "\n",
    "        # Create tuples of (input, output, score) and sort by input text\n",
    "        samples = list(zip(inputs, outputs, scores, strict=True))\n",
    "        samples.sort(key=lambda x: x[0])  # Sort by input text\n",
    "\n",
    "        # Use fixed random seed for deterministic shuffling\n",
    "        rng = np.random.RandomState(42)\n",
    "        rng.shuffle(samples)\n",
    "\n",
    "        # Take first N samples after shuffling\n",
    "        samples = samples[:generations_to_log]\n",
    "\n",
    "        # Log to each configured logger\n",
    "        self.validation_generations_logger.log(self.config.trainer.logger, samples, self.global_steps)\n",
    "\n",
    "    def _validate(self):\n",
    "        data_source_lst = []\n",
    "        reward_extra_infos_dict: dict[str, list] = defaultdict(list)\n",
    "\n",
    "        # Lists to collect samples for the table\n",
    "        sample_inputs = []\n",
    "        sample_outputs = []\n",
    "        sample_scores = []\n",
    "        sample_turns = []\n",
    "\n",
    "        for test_data in self.val_dataloader:\n",
    "            test_batch = DataProto.from_single_dict(test_data)\n",
    "\n",
    "            # repeat test batch\n",
    "            test_batch = test_batch.repeat(\n",
    "                repeat_times=self.config.actor_rollout_ref.rollout.val_kwargs.n, interleave=True\n",
    "            )\n",
    "\n",
    "            # we only do validation on rule-based rm\n",
    "            if self.config.reward_model.enable and test_batch[0].non_tensor_batch[\"reward_model\"][\"style\"] == \"model\":\n",
    "                return {}\n",
    "\n",
    "            # Store original inputs\n",
    "            input_ids = test_batch.batch[\"input_ids\"]\n",
    "            # TODO: Can we keep special tokens except for padding tokens?\n",
    "            input_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "            sample_inputs.extend(input_texts)\n",
    "\n",
    "            batch_keys_to_pop = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "            non_tensor_batch_keys_to_pop = [\"raw_prompt_ids\"]\n",
    "            if \"multi_modal_data\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"multi_modal_data\")\n",
    "            if \"raw_prompt\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"raw_prompt\")\n",
    "            if \"tools_kwargs\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"tools_kwargs\")\n",
    "            if \"interaction_kwargs\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"interaction_kwargs\")\n",
    "            if \"agent_name\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"agent_name\")\n",
    "            test_gen_batch = test_batch.pop(\n",
    "                batch_keys=batch_keys_to_pop,\n",
    "                non_tensor_batch_keys=non_tensor_batch_keys_to_pop,\n",
    "            )\n",
    "\n",
    "            test_gen_batch.meta_info = {\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"recompute_log_prob\": False,\n",
    "                \"do_sample\": self.config.actor_rollout_ref.rollout.val_kwargs.do_sample,\n",
    "                \"validate\": True,\n",
    "                \"global_steps\": self.global_steps,\n",
    "            }\n",
    "            print(f\"test_gen_batch meta info: {test_gen_batch.meta_info}\")\n",
    "\n",
    "            # pad to be divisible by dp_size\n",
    "            size_divisor = (\n",
    "                self.actor_rollout_wg.world_size\n",
    "                if not self.async_rollout_mode\n",
    "                else self.config.actor_rollout_ref.rollout.agent.num_workers\n",
    "            )\n",
    "            test_gen_batch_padded, pad_size = pad_dataproto_to_divisor(test_gen_batch, size_divisor)\n",
    "            if not self.async_rollout_mode:\n",
    "                test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(test_gen_batch_padded)\n",
    "            else:\n",
    "                test_output_gen_batch_padded = self.async_rollout_manager.generate_sequences(test_gen_batch_padded)\n",
    "\n",
    "            # unpad\n",
    "            test_output_gen_batch = unpad_dataproto(test_output_gen_batch_padded, pad_size=pad_size)\n",
    "\n",
    "            print(\"validation generation end\")\n",
    "\n",
    "            # Store generated outputs\n",
    "            output_ids = test_output_gen_batch.batch[\"responses\"]\n",
    "            output_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "            sample_outputs.extend(output_texts)\n",
    "\n",
    "            test_batch = test_batch.union(test_output_gen_batch)\n",
    "            test_batch.meta_info[\"validate\"] = True\n",
    "\n",
    "            # evaluate using reward_function\n",
    "            result = self.val_reward_fn(test_batch, return_dict=True)\n",
    "            reward_tensor = result[\"reward_tensor\"]\n",
    "            scores = reward_tensor.sum(-1).cpu().tolist()\n",
    "            sample_scores.extend(scores)\n",
    "\n",
    "            reward_extra_infos_dict[\"reward\"].extend(scores)\n",
    "            print(f\"len reward_extra_infos_dict['reward']: {len(reward_extra_infos_dict['reward'])}\")\n",
    "            if \"reward_extra_info\" in result:\n",
    "                for key, lst in result[\"reward_extra_info\"].items():\n",
    "                    reward_extra_infos_dict[key].extend(lst)\n",
    "                    print(f\"len reward_extra_infos_dict['{key}']: {len(reward_extra_infos_dict[key])}\")\n",
    "\n",
    "            # collect num_turns of each prompt\n",
    "            if \"__num_turns__\" in test_batch.non_tensor_batch:\n",
    "                sample_turns.append(test_batch.non_tensor_batch[\"__num_turns__\"])\n",
    "\n",
    "            data_source_lst.append(test_batch.non_tensor_batch.get(\"data_source\", [\"unknown\"] * reward_tensor.shape[0]))\n",
    "\n",
    "        self._maybe_log_val_generations(inputs=sample_inputs, outputs=sample_outputs, scores=sample_scores)\n",
    "\n",
    "        # dump generations\n",
    "        val_data_dir = self.config.trainer.get(\"validation_data_dir\", None)\n",
    "        if val_data_dir:\n",
    "            self._dump_generations(\n",
    "                inputs=sample_inputs,\n",
    "                outputs=sample_outputs,\n",
    "                scores=sample_scores,\n",
    "                reward_extra_infos_dict=reward_extra_infos_dict,\n",
    "                dump_path=val_data_dir,\n",
    "            )\n",
    "\n",
    "        for key_info, lst in reward_extra_infos_dict.items():\n",
    "            assert len(lst) == 0 or len(lst) == len(sample_scores), f\"{key_info}: {len(lst)=}, {len(sample_scores)=}\"\n",
    "\n",
    "        data_sources = np.concatenate(data_source_lst, axis=0)\n",
    "\n",
    "        data_src2var2metric2val = process_validation_metrics(data_sources, sample_inputs, reward_extra_infos_dict)\n",
    "        metric_dict = {}\n",
    "        for data_source, var2metric2val in data_src2var2metric2val.items():\n",
    "            core_var = \"acc\" if \"acc\" in var2metric2val else \"reward\"\n",
    "            for var_name, metric2val in var2metric2val.items():\n",
    "                n_max = max([int(name.split(\"@\")[-1].split(\"/\")[0]) for name in metric2val.keys()])\n",
    "                for metric_name, metric_val in metric2val.items():\n",
    "                    if (\n",
    "                        (var_name == core_var)\n",
    "                        and any(metric_name.startswith(pfx) for pfx in [\"mean\", \"maj\", \"best\"])\n",
    "                        and (f\"@{n_max}\" in metric_name)\n",
    "                    ):\n",
    "                        metric_sec = \"val-core\"\n",
    "                    else:\n",
    "                        metric_sec = \"val-aux\"\n",
    "                    pfx = f\"{metric_sec}/{data_source}/{var_name}/{metric_name}\"\n",
    "                    metric_dict[pfx] = metric_val\n",
    "\n",
    "        if len(sample_turns) > 0:\n",
    "            sample_turns = np.concatenate(sample_turns)\n",
    "            metric_dict[\"val-aux/num_turns/min\"] = sample_turns.min()\n",
    "            metric_dict[\"val-aux/num_turns/max\"] = sample_turns.max()\n",
    "            metric_dict[\"val-aux/num_turns/mean\"] = sample_turns.mean()\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def init_workers(self):\n",
    "        \"\"\"Initialize distributed training workers using Ray backend.\n",
    "\n",
    "        Creates:\n",
    "        1. Ray resource pools from configuration\n",
    "        2. Worker groups for each role (actor, critic, etc.)\n",
    "        \"\"\"\n",
    "        self.resource_pool_manager.create_resource_pool()\n",
    "\n",
    "        self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}\n",
    "\n",
    "        # create actor and rollout\n",
    "        if self.hybrid_engine:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)\n",
    "            actor_rollout_cls = RayClassWithInitArgs(\n",
    "                cls=self.role_worker_mapping[Role.ActorRollout],\n",
    "                config=self.config.actor_rollout_ref,\n",
    "                role=\"actor_rollout\",\n",
    "                profile_option=self.config.trainer.npu_profile.options,\n",
    "            )\n",
    "            self.resource_pool_to_cls[resource_pool][\"actor_rollout\"] = actor_rollout_cls\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # create critic\n",
    "        if self.use_critic:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)\n",
    "            critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic)\n",
    "            self.resource_pool_to_cls[resource_pool][\"critic\"] = critic_cls\n",
    "\n",
    "        # create reference policy if needed\n",
    "        if self.use_reference_policy:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)\n",
    "            ref_policy_cls = RayClassWithInitArgs(\n",
    "                self.role_worker_mapping[Role.RefPolicy],\n",
    "                config=self.config.actor_rollout_ref,\n",
    "                role=\"ref\",\n",
    "                profile_option=self.config.trainer.npu_profile.options,\n",
    "            )\n",
    "            self.resource_pool_to_cls[resource_pool][\"ref\"] = ref_policy_cls\n",
    "\n",
    "        # create a reward model if reward_fn is None\n",
    "        if self.use_rm:\n",
    "            # we create a RM here\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel)\n",
    "            rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model)\n",
    "            self.resource_pool_to_cls[resource_pool][\"rm\"] = rm_cls\n",
    "\n",
    "        # initialize WorkerGroup\n",
    "        # NOTE: if you want to use a different resource pool for each role, which can support different parallel size,\n",
    "        # you should not use `create_colocated_worker_cls`.\n",
    "        # Instead, directly pass different resource pool to different worker groups.\n",
    "        # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.\n",
    "        all_wg = {}\n",
    "        wg_kwargs = {}  # Setting up kwargs for RayWorkerGroup\n",
    "        if OmegaConf.select(self.config.trainer, \"ray_wait_register_center_timeout\") is not None:\n",
    "            wg_kwargs[\"ray_wait_register_center_timeout\"] = self.config.trainer.ray_wait_register_center_timeout\n",
    "        if OmegaConf.select(self.config.trainer, \"profile_steps\") is not None:\n",
    "            wg_kwargs[\"profile_steps\"] = OmegaConf.select(self.config.trainer, \"profile_steps\")\n",
    "            assert OmegaConf.select(self.config.trainer, \"worker_nsight_options\") is not None, (\n",
    "                \"worker_nsight_options must be set when profile_steps is set\"\n",
    "            )\n",
    "            wg_kwargs[\"worker_nsight_options\"] = OmegaConf.to_container(\n",
    "                OmegaConf.select(self.config.trainer, \"worker_nsight_options\")\n",
    "            )\n",
    "        wg_kwargs[\"device_name\"] = self.device_name\n",
    "\n",
    "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
    "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
    "            wg_dict = self.ray_worker_group_cls(\n",
    "                resource_pool=resource_pool,\n",
    "                ray_cls_with_init=worker_dict_cls,\n",
    "                **wg_kwargs,\n",
    "            )\n",
    "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
    "            all_wg.update(spawn_wg)\n",
    "\n",
    "        if self.use_critic:\n",
    "            self.critic_wg = all_wg[\"critic\"]\n",
    "            self.critic_wg.init_model()\n",
    "\n",
    "        if self.use_reference_policy and not self.ref_in_actor:\n",
    "            self.ref_policy_wg = all_wg[\"ref\"]\n",
    "            self.ref_policy_wg.init_model()\n",
    "\n",
    "        if self.use_rm:\n",
    "            self.rm_wg = all_wg[\"rm\"]\n",
    "            self.rm_wg.init_model()\n",
    "\n",
    "        # we should create rollout at the end so that vllm can have a better estimation of kv cache memory\n",
    "        self.actor_rollout_wg = all_wg[\"actor_rollout\"]\n",
    "        self.actor_rollout_wg.init_model()\n",
    "\n",
    "        # create async rollout manager and request scheduler\n",
    "        self.async_rollout_mode = False\n",
    "        if self.config.actor_rollout_ref.rollout.mode == \"async\":\n",
    "            from verl.experimental.agent_loop import AgentLoopManager\n",
    "\n",
    "            self.async_rollout_mode = True\n",
    "            self.async_rollout_manager = AgentLoopManager(\n",
    "                config=self.config,\n",
    "                worker_group=self.actor_rollout_wg,\n",
    "            )\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        from verl.utils.fs import local_mkdir_safe\n",
    "\n",
    "        # path: given_path + `/global_step_{global_steps}` + `/actor`\n",
    "        local_global_step_folder = os.path.join(\n",
    "            self.config.trainer.default_local_dir, f\"global_step_{self.global_steps}\"\n",
    "        )\n",
    "\n",
    "        print(f\"local_global_step_folder: {local_global_step_folder}\")\n",
    "        actor_local_path = os.path.join(local_global_step_folder, \"actor\")\n",
    "\n",
    "        actor_remote_path = (\n",
    "            None\n",
    "            if self.config.trainer.default_hdfs_dir is None\n",
    "            else os.path.join(self.config.trainer.default_hdfs_dir, f\"global_step_{self.global_steps}\", \"actor\")\n",
    "        )\n",
    "\n",
    "        remove_previous_ckpt_in_save = self.config.trainer.get(\"remove_previous_ckpt_in_save\", False)\n",
    "        if remove_previous_ckpt_in_save:\n",
    "            print(\n",
    "                \"Warning: remove_previous_ckpt_in_save is deprecated,\"\n",
    "                + \" set max_actor_ckpt_to_keep=1 and max_critic_ckpt_to_keep=1 instead\"\n",
    "            )\n",
    "        max_actor_ckpt_to_keep = (\n",
    "            self.config.trainer.get(\"max_actor_ckpt_to_keep\", None) if not remove_previous_ckpt_in_save else 1\n",
    "        )\n",
    "        max_critic_ckpt_to_keep = (\n",
    "            self.config.trainer.get(\"max_critic_ckpt_to_keep\", None) if not remove_previous_ckpt_in_save else 1\n",
    "        )\n",
    "\n",
    "        self.actor_rollout_wg.save_checkpoint(\n",
    "            actor_local_path, actor_remote_path, self.global_steps, max_ckpt_to_keep=max_actor_ckpt_to_keep\n",
    "        )\n",
    "\n",
    "        if self.use_critic:\n",
    "            critic_local_path = os.path.join(local_global_step_folder, \"critic\")\n",
    "            critic_remote_path = (\n",
    "                None\n",
    "                if self.config.trainer.default_hdfs_dir is None\n",
    "                else os.path.join(self.config.trainer.default_hdfs_dir, f\"global_step_{self.global_steps}\", \"critic\")\n",
    "            )\n",
    "            self.critic_wg.save_checkpoint(\n",
    "                critic_local_path, critic_remote_path, self.global_steps, max_ckpt_to_keep=max_critic_ckpt_to_keep\n",
    "            )\n",
    "\n",
    "        # save dataloader\n",
    "        local_mkdir_safe(local_global_step_folder)\n",
    "        dataloader_local_path = os.path.join(local_global_step_folder, \"data.pt\")\n",
    "        dataloader_state_dict = self.train_dataloader.state_dict()\n",
    "        torch.save(dataloader_state_dict, dataloader_local_path)\n",
    "\n",
    "        # latest checkpointed iteration tracker (for atomic usage)\n",
    "        local_latest_checkpointed_iteration = os.path.join(\n",
    "            self.config.trainer.default_local_dir, \"latest_checkpointed_iteration.txt\"\n",
    "        )\n",
    "        with open(local_latest_checkpointed_iteration, \"w\") as f:\n",
    "            f.write(str(self.global_steps))\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        if self.config.trainer.resume_mode == \"disable\":\n",
    "            return 0\n",
    "\n",
    "        # load from hdfs\n",
    "        if self.config.trainer.default_hdfs_dir is not None:\n",
    "            raise NotImplementedError(\"load from hdfs is not implemented yet\")\n",
    "        else:\n",
    "            checkpoint_folder = self.config.trainer.default_local_dir  # TODO: check path\n",
    "            if not os.path.isabs(checkpoint_folder):\n",
    "                working_dir = os.getcwd()\n",
    "                checkpoint_folder = os.path.join(working_dir, checkpoint_folder)\n",
    "            global_step_folder = find_latest_ckpt_path(checkpoint_folder)  # None if no latest\n",
    "\n",
    "        # find global_step_folder\n",
    "        if self.config.trainer.resume_mode == \"auto\":\n",
    "            if global_step_folder is None:\n",
    "                print(\"Training from scratch\")\n",
    "                return 0\n",
    "        else:\n",
    "            if self.config.trainer.resume_mode == \"resume_path\":\n",
    "                assert isinstance(self.config.trainer.resume_from_path, str), \"resume ckpt must be str type\"\n",
    "                assert \"global_step_\" in self.config.trainer.resume_from_path, (\n",
    "                    \"resume ckpt must specify the global_steps\"\n",
    "                )\n",
    "                global_step_folder = self.config.trainer.resume_from_path\n",
    "                if not os.path.isabs(global_step_folder):\n",
    "                    working_dir = os.getcwd()\n",
    "                    global_step_folder = os.path.join(working_dir, global_step_folder)\n",
    "        print(f\"Load from checkpoint folder: {global_step_folder}\")\n",
    "        # set global step\n",
    "        self.global_steps = int(global_step_folder.split(\"global_step_\")[-1])\n",
    "\n",
    "        print(f\"Setting global step to {self.global_steps}\")\n",
    "        print(f\"Resuming from {global_step_folder}\")\n",
    "\n",
    "        actor_path = os.path.join(global_step_folder, \"actor\")\n",
    "        critic_path = os.path.join(global_step_folder, \"critic\")\n",
    "        # load actor\n",
    "        self.actor_rollout_wg.load_checkpoint(\n",
    "            actor_path, del_local_after_load=self.config.trainer.del_local_ckpt_after_load\n",
    "        )\n",
    "        # load critic\n",
    "        if self.use_critic:\n",
    "            self.critic_wg.load_checkpoint(\n",
    "                critic_path, del_local_after_load=self.config.trainer.del_local_ckpt_after_load\n",
    "            )\n",
    "\n",
    "        # load dataloader,\n",
    "        # TODO: from remote not implemented yet\n",
    "        dataloader_local_path = os.path.join(global_step_folder, \"data.pt\")\n",
    "        if os.path.exists(dataloader_local_path):\n",
    "            dataloader_state_dict = torch.load(dataloader_local_path, weights_only=False)\n",
    "            self.train_dataloader.load_state_dict(dataloader_state_dict)\n",
    "        else:\n",
    "            print(f\"Warning: No dataloader state found at {dataloader_local_path}, will start from scratch\")\n",
    "\n",
    "    def _start_profiling(self, do_profile: bool) -> None:\n",
    "        \"\"\"Start profiling for all worker groups if profiling is enabled.\"\"\"\n",
    "        if do_profile:\n",
    "            self.actor_rollout_wg.start_profile(role=\"e2e\", profile_step=self.global_steps)\n",
    "            if self.use_reference_policy:\n",
    "                self.ref_policy_wg.start_profile()\n",
    "            if self.use_critic:\n",
    "                self.critic_wg.start_profile()\n",
    "            if self.use_rm:\n",
    "                self.rm_wg.start_profile()\n",
    "\n",
    "    def _stop_profiling(self, do_profile: bool) -> None:\n",
    "        \"\"\"Stop profiling for all worker groups if profiling is enabled.\"\"\"\n",
    "        if do_profile:\n",
    "            self.actor_rollout_wg.stop_profile()\n",
    "            if self.use_reference_policy:\n",
    "                self.ref_policy_wg.stop_profile()\n",
    "            if self.use_critic:\n",
    "                self.critic_wg.stop_profile()\n",
    "            if self.use_rm:\n",
    "                self.rm_wg.stop_profile()\n",
    "\n",
    "    def _balance_batch(self, batch: DataProto, metrics, logging_prefix=\"global_seqlen\"):\n",
    "        \"\"\"Reorder the data on single controller such that each dp rank gets similar total tokens\"\"\"\n",
    "        attention_mask = batch.batch[\"attention_mask\"]\n",
    "        batch_size = attention_mask.shape[0]\n",
    "        global_seqlen_lst = batch.batch[\"attention_mask\"].view(batch_size, -1).sum(-1).tolist()  # (train_batch_size,)\n",
    "        world_size = self.actor_rollout_wg.world_size\n",
    "        global_partition_lst = get_seqlen_balanced_partitions(\n",
    "            global_seqlen_lst, k_partitions=world_size, equal_size=True\n",
    "        )\n",
    "        # reorder based on index. The data will be automatically equally partitioned by dispatch function\n",
    "        global_idx = torch.tensor([j for partition in global_partition_lst for j in partition])\n",
    "        batch.reorder(global_idx)\n",
    "        global_balance_stats = log_seqlen_unbalance(\n",
    "            seqlen_list=global_seqlen_lst, partitions=global_partition_lst, prefix=logging_prefix\n",
    "        )\n",
    "        metrics.update(global_balance_stats)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        The training loop of PPO.\n",
    "        The driver process only need to call the compute functions of the worker group through RPC\n",
    "        to construct the PPO dataflow.\n",
    "        The light-weight advantage computation is done on the driver process.\n",
    "        \"\"\"\n",
    "        from omegaconf import OmegaConf\n",
    "\n",
    "        from verl.utils.tracking import Tracking\n",
    "\n",
    "        logger = Tracking(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "            default_backend=self.config.trainer.logger,\n",
    "            config=OmegaConf.to_container(self.config, resolve=True),\n",
    "        )\n",
    "\n",
    "        self.global_steps = 0\n",
    "\n",
    "        # load checkpoint before doing anything\n",
    "        self._load_checkpoint()\n",
    "\n",
    "        # perform validation before training\n",
    "        # currently, we only support validation using the reward_function.\n",
    "        if self.val_reward_fn is not None and self.config.trainer.get(\"val_before_train\", True):\n",
    "            val_metrics = self._validate()\n",
    "            assert val_metrics, f\"{val_metrics=}\"\n",
    "            pprint(f\"Initial validation metrics: {val_metrics}\")\n",
    "            logger.log(data=val_metrics, step=self.global_steps)\n",
    "            if self.config.trainer.get(\"val_only\", False):\n",
    "                return\n",
    "\n",
    "        # add tqdm\n",
    "        progress_bar = tqdm(total=self.total_training_steps, initial=self.global_steps, desc=\"Training Progress\")\n",
    "\n",
    "        # we start from step 1\n",
    "        self.global_steps += 1\n",
    "        last_val_metrics = None\n",
    "        self.max_steps_duration = 0\n",
    "\n",
    "        for epoch in range(self.config.trainer.total_epochs):\n",
    "            for batch_dict in self.train_dataloader:\n",
    "                metrics = {}\n",
    "                timing_raw = {}\n",
    "\n",
    "                do_profile = (\n",
    "                    self.global_steps in self.config.trainer.profile_steps\n",
    "                    if self.config.trainer.profile_steps is not None\n",
    "                    else False\n",
    "                )\n",
    "                with marked_timer(\"start_profile\", timing_raw):\n",
    "                    self._start_profiling(do_profile)\n",
    "\n",
    "                batch: DataProto = DataProto.from_single_dict(batch_dict)\n",
    "\n",
    "                # pop those keys for generation\n",
    "                batch_keys_to_pop = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "                non_tensor_batch_keys_to_pop = [\"raw_prompt_ids\"]\n",
    "                if \"multi_modal_data\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"multi_modal_data\")\n",
    "                if \"raw_prompt\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"raw_prompt\")\n",
    "                if \"tools_kwargs\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"tools_kwargs\")\n",
    "                if \"interaction_kwargs\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"interaction_kwargs\")\n",
    "                if \"index\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"index\")\n",
    "                if \"agent_name\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"agent_name\")\n",
    "\n",
    "                gen_batch = batch.pop(\n",
    "                    batch_keys=batch_keys_to_pop,\n",
    "                    non_tensor_batch_keys=non_tensor_batch_keys_to_pop,\n",
    "                )\n",
    "\n",
    "                # pass global_steps to trace\n",
    "                gen_batch.meta_info[\"global_steps\"] = self.global_steps\n",
    "                gen_batch = gen_batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)\n",
    "\n",
    "                is_last_step = self.global_steps >= self.total_training_steps\n",
    "\n",
    "                with marked_timer(\"step\", timing_raw):\n",
    "                    # generate a batch\n",
    "                    with marked_timer(\"gen\", timing_raw, color=\"red\"):\n",
    "                        if not self.async_rollout_mode:\n",
    "                            gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
    "                        else:\n",
    "                            gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch)\n",
    "                        timing_raw.update(gen_batch_output.meta_info[\"timing\"])\n",
    "                        gen_batch_output.meta_info.pop(\"timing\", None)\n",
    "\n",
    "                    if self.config.algorithm.adv_estimator == AdvantageEstimator.REMAX:\n",
    "                        with marked_timer(\"gen_max\", timing_raw, color=\"purple\"):\n",
    "                            gen_baseline_batch = deepcopy(gen_batch)\n",
    "                            gen_baseline_batch.meta_info[\"do_sample\"] = False\n",
    "                            if not self.async_rollout_mode:\n",
    "                                gen_baseline_output = self.actor_rollout_wg.generate_sequences(gen_baseline_batch)\n",
    "                            else:\n",
    "                                gen_baseline_output = self.async_rollout_manager.generate_sequences(gen_baseline_batch)\n",
    "                            batch = batch.union(gen_baseline_output)\n",
    "                            reward_baseline_tensor = self.reward_fn(batch)\n",
    "                            reward_baseline_tensor = reward_baseline_tensor.sum(dim=-1)\n",
    "\n",
    "                            batch.pop(batch_keys=list(gen_baseline_output.batch.keys()))\n",
    "\n",
    "                            batch.batch[\"reward_baselines\"] = reward_baseline_tensor\n",
    "\n",
    "                            del gen_baseline_batch, gen_baseline_output\n",
    "\n",
    "                    batch.non_tensor_batch[\"uid\"] = np.array(\n",
    "                        [str(uuid.uuid4()) for _ in range(len(batch.batch))], dtype=object\n",
    "                    )\n",
    "                    # repeat to align with repeated responses in rollout\n",
    "                    batch = batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)\n",
    "                    batch = batch.union(gen_batch_output)\n",
    "\n",
    "                    if \"response_mask\" not in batch.batch.keys():\n",
    "                        batch.batch[\"response_mask\"] = compute_response_mask(batch)\n",
    "                    # Balance the number of valid tokens across DP ranks.\n",
    "                    # NOTE: This usually changes the order of data in the `batch`,\n",
    "                    # which won't affect the advantage calculation (since it's based on uid),\n",
    "                    # but might affect the loss calculation (due to the change of mini-batching).\n",
    "                    # TODO: Decouple the DP balancing and mini-batching.\n",
    "                    if self.config.trainer.balance_batch:\n",
    "                        self._balance_batch(batch, metrics=metrics)\n",
    "\n",
    "                    # compute global_valid tokens\n",
    "                    batch.meta_info[\"global_token_num\"] = torch.sum(batch.batch[\"attention_mask\"], dim=-1).tolist()\n",
    "\n",
    "                    with marked_timer(\"reward\", timing_raw, color=\"yellow\"):\n",
    "                        # compute reward model score\n",
    "                        if self.use_rm:\n",
    "                            reward_tensor = self.rm_wg.compute_rm_score(batch)\n",
    "                            batch = batch.union(reward_tensor)\n",
    "\n",
    "                        if self.config.reward_model.launch_reward_fn_async:\n",
    "                            future_reward = compute_reward_async.remote(data=batch, reward_fn=self.reward_fn)\n",
    "                        else:\n",
    "                            reward_tensor, reward_extra_infos_dict = compute_reward(batch, self.reward_fn)\n",
    "\n",
    "                    # recompute old_log_probs\n",
    "                    with marked_timer(\"old_log_prob\", timing_raw, color=\"blue\"):\n",
    "                        old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\n",
    "                        entropys = old_log_prob.batch[\"entropys\"]\n",
    "                        response_masks = batch.batch[\"response_mask\"]\n",
    "                        loss_agg_mode = self.config.actor_rollout_ref.actor.loss_agg_mode\n",
    "                        entropy_agg = agg_loss(loss_mat=entropys, loss_mask=response_masks, loss_agg_mode=loss_agg_mode)\n",
    "                        old_log_prob_metrics = {\"actor/entropy\": entropy_agg.detach().item()}\n",
    "                        metrics.update(old_log_prob_metrics)\n",
    "                        old_log_prob.batch.pop(\"entropys\")\n",
    "                        batch = batch.union(old_log_prob)\n",
    "\n",
    "                        if \"rollout_log_probs\" in batch.batch.keys():\n",
    "                            # TODO: we may want to add diff of probs too.\n",
    "                            rollout_old_log_probs = batch.batch[\"rollout_log_probs\"]\n",
    "                            actor_old_log_probs = batch.batch[\"old_log_probs\"]\n",
    "                            attention_mask = batch.batch[\"attention_mask\"]\n",
    "                            responses = batch.batch[\"responses\"]\n",
    "                            response_length = responses.size(1)\n",
    "                            response_mask = attention_mask[:, -response_length:]\n",
    "\n",
    "                            rollout_probs = torch.exp(rollout_old_log_probs)\n",
    "                            actor_probs = torch.exp(actor_old_log_probs)\n",
    "                            rollout_probs_diff = torch.abs(rollout_probs - actor_probs)\n",
    "                            rollout_probs_diff = torch.masked_select(rollout_probs_diff, response_mask.bool())\n",
    "                            rollout_probs_diff_max = torch.max(rollout_probs_diff)\n",
    "                            rollout_probs_diff_mean = torch.mean(rollout_probs_diff)\n",
    "                            rollout_probs_diff_std = torch.std(rollout_probs_diff)\n",
    "                            metrics.update(\n",
    "                                {\n",
    "                                    \"training/rollout_probs_diff_max\": rollout_probs_diff_max.detach().item(),\n",
    "                                    \"training/rollout_probs_diff_mean\": rollout_probs_diff_mean.detach().item(),\n",
    "                                    \"training/rollout_probs_diff_std\": rollout_probs_diff_std.detach().item(),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                    if self.use_reference_policy:\n",
    "                        # compute reference log_prob\n",
    "                        with marked_timer(\"ref\", timing_raw, color=\"olive\"):\n",
    "                            if not self.ref_in_actor:\n",
    "                                ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n",
    "                            else:\n",
    "                                ref_log_prob = self.actor_rollout_wg.compute_ref_log_prob(batch)\n",
    "                            batch = batch.union(ref_log_prob)\n",
    "\n",
    "                    # compute values\n",
    "                    if self.use_critic:\n",
    "                        with marked_timer(\"values\", timing_raw, color=\"cyan\"):\n",
    "                            values = self.critic_wg.compute_values(batch)\n",
    "                            batch = batch.union(values)\n",
    "\n",
    "                    with marked_timer(\"adv\", timing_raw, color=\"brown\"):\n",
    "                        # we combine with rule-based rm\n",
    "                        reward_extra_infos_dict: dict[str, list]\n",
    "                        if self.config.reward_model.launch_reward_fn_async:\n",
    "                            reward_tensor, reward_extra_infos_dict = ray.get(future_reward)\n",
    "                        batch.batch[\"token_level_scores\"] = reward_tensor\n",
    "\n",
    "                        if reward_extra_infos_dict:\n",
    "                            batch.non_tensor_batch.update({k: np.array(v) for k, v in reward_extra_infos_dict.items()})\n",
    "\n",
    "                        # compute rewards. apply_kl_penalty if available\n",
    "                        if self.config.algorithm.use_kl_in_reward:\n",
    "                            batch, kl_metrics = apply_kl_penalty(\n",
    "                                batch, kl_ctrl=self.kl_ctrl_in_reward, kl_penalty=self.config.algorithm.kl_penalty\n",
    "                            )\n",
    "                            metrics.update(kl_metrics)\n",
    "                        else:\n",
    "                            batch.batch[\"token_level_rewards\"] = batch.batch[\"token_level_scores\"]\n",
    "\n",
    "                        # compute advantages, executed on the driver process\n",
    "\n",
    "                        norm_adv_by_std_in_grpo = self.config.algorithm.get(\n",
    "                            \"norm_adv_by_std_in_grpo\", True\n",
    "                        )  # GRPO adv normalization factor\n",
    "\n",
    "                        batch = compute_advantage(\n",
    "                            batch,\n",
    "                            adv_estimator=self.config.algorithm.adv_estimator,\n",
    "                            gamma=self.config.algorithm.gamma,\n",
    "                            lam=self.config.algorithm.lam,\n",
    "                            num_repeat=self.config.actor_rollout_ref.rollout.n,\n",
    "                            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n",
    "                            config=self.config.algorithm,\n",
    "                        )\n",
    "\n",
    "                    # update critic\n",
    "                    if self.use_critic:\n",
    "                        with marked_timer(\"update_critic\", timing_raw, color=\"pink\"):\n",
    "                            critic_output = self.critic_wg.update_critic(batch)\n",
    "                        critic_output_metrics = reduce_metrics(critic_output.meta_info[\"metrics\"])\n",
    "                        metrics.update(critic_output_metrics)\n",
    "\n",
    "                    # implement critic warmup\n",
    "                    if self.config.trainer.critic_warmup <= self.global_steps:\n",
    "                        # update actor\n",
    "                        with marked_timer(\"update_actor\", timing_raw, color=\"red\"):\n",
    "                            batch.meta_info[\"multi_turn\"] = self.config.actor_rollout_ref.rollout.multi_turn.enable\n",
    "                            actor_output = self.actor_rollout_wg.update_actor(batch)\n",
    "                        actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n",
    "                        metrics.update(actor_output_metrics)\n",
    "\n",
    "                    # Log rollout generations if enabled\n",
    "                    rollout_data_dir = self.config.trainer.get(\"rollout_data_dir\", None)\n",
    "                    if rollout_data_dir:\n",
    "                        with marked_timer(\"dump_rollout_generations\", timing_raw, color=\"green\"):\n",
    "                            inputs = self.tokenizer.batch_decode(batch.batch[\"prompts\"], skip_special_tokens=True)\n",
    "                            outputs = self.tokenizer.batch_decode(batch.batch[\"responses\"], skip_special_tokens=True)\n",
    "                            scores = batch.batch[\"token_level_scores\"].sum(-1).cpu().tolist()\n",
    "                            if \"request_id\" in batch.non_tensor_batch:\n",
    "                                reward_extra_infos_dict.setdefault(\n",
    "                                    \"request_id\",\n",
    "                                    batch.non_tensor_batch[\"request_id\"].tolist(),\n",
    "                                )\n",
    "                            self._dump_generations(\n",
    "                                inputs=inputs,\n",
    "                                outputs=outputs,\n",
    "                                scores=scores,\n",
    "                                reward_extra_infos_dict=reward_extra_infos_dict,\n",
    "                                dump_path=rollout_data_dir,\n",
    "                            )\n",
    "\n",
    "                    # validate\n",
    "                    if (\n",
    "                        self.val_reward_fn is not None\n",
    "                        and self.config.trainer.test_freq > 0\n",
    "                        and (is_last_step or self.global_steps % self.config.trainer.test_freq == 0)\n",
    "                    ):\n",
    "                        with marked_timer(\"testing\", timing_raw, color=\"green\"):\n",
    "                            val_metrics: dict = self._validate()\n",
    "                            if is_last_step:\n",
    "                                last_val_metrics = val_metrics\n",
    "                        metrics.update(val_metrics)\n",
    "\n",
    "                    # Check if the ESI (Elastic Server Instance)/training plan is close to expiration.\n",
    "                    esi_close_to_expiration = should_save_ckpt_esi(\n",
    "                        max_steps_duration=self.max_steps_duration,\n",
    "                        redundant_time=self.config.trainer.esi_redundant_time,\n",
    "                    )\n",
    "                    # Check if the conditions for saving a checkpoint are met.\n",
    "                    # The conditions include a mandatory condition (1) and\n",
    "                    # one of the following optional conditions (2/3/4):\n",
    "                    # 1. The save frequency is set to a positive value.\n",
    "                    # 2. It's the last training step.\n",
    "                    # 3. The current step number is a multiple of the save frequency.\n",
    "                    # 4. The ESI(Elastic Server Instance)/training plan is close to expiration.\n",
    "                    if self.config.trainer.save_freq > 0 and (\n",
    "                        is_last_step\n",
    "                        or self.global_steps % self.config.trainer.save_freq == 0\n",
    "                        or esi_close_to_expiration\n",
    "                    ):\n",
    "                        if esi_close_to_expiration:\n",
    "                            print(\"Force saving checkpoint: ESI instance expiration approaching.\")\n",
    "                        with marked_timer(\"save_checkpoint\", timing_raw, color=\"green\"):\n",
    "                            self._save_checkpoint()\n",
    "\n",
    "                with marked_timer(\"stop_profile\", timing_raw):\n",
    "                    self._stop_profiling(do_profile)\n",
    "\n",
    "                steps_duration = timing_raw[\"step\"]\n",
    "                self.max_steps_duration = max(self.max_steps_duration, steps_duration)\n",
    "\n",
    "                # training metrics\n",
    "                metrics.update(\n",
    "                    {\n",
    "                        \"training/global_step\": self.global_steps,\n",
    "                        \"training/epoch\": epoch,\n",
    "                    }\n",
    "                )\n",
    "                # collect metrics\n",
    "                metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\n",
    "                metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\n",
    "                # TODO: implement actual tflpo and theoretical tflpo\n",
    "                n_gpus = self.resource_pool_manager.get_n_gpus()\n",
    "                metrics.update(compute_throughout_metrics(batch=batch, timing_raw=timing_raw, n_gpus=n_gpus))\n",
    "\n",
    "                # this is experimental and may be changed/removed in the future in favor of a general-purpose one\n",
    "                if isinstance(self.train_dataloader.sampler, AbstractCurriculumSampler):\n",
    "                    self.train_dataloader.sampler.update(batch=batch)\n",
    "\n",
    "                # TODO: make a canonical logger that supports various backend\n",
    "                logger.log(data=metrics, step=self.global_steps)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                self.global_steps += 1\n",
    "\n",
    "                if is_last_step:\n",
    "                    pprint(f\"Final validation metrics: {last_val_metrics}\")\n",
    "                    progress_bar.close()\n",
    "                    return\n",
    "\n",
    "                # this is experimental and may be changed/removed in the future\n",
    "                # in favor of a general-purpose data buffer pool\n",
    "                if hasattr(self.train_dataset, \"on_batch_end\"):\n",
    "                    # The dataset may be changed after each training batch\n",
    "                    self.train_dataset.on_batch_end(batch=batch)\n",
    "\"\"\"\n",
    "main_self_reflective.py代码如下\n",
    "\"\"\"\n",
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Note that we don't combine the main with ray_trainer as ray_trainer is used by other main.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import socket\n",
    "\n",
    "import hydra\n",
    "import ray\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from verl.experimental.dataset.sampler import AbstractSampler\n",
    "from verl.trainer.constants_ppo import get_ppo_ray_runtime_env\n",
    "from verl.trainer.ppo.self_reflective_trainer import SelfReflectiveGRPOTrainer\n",
    "from verl.trainer.ppo.teacher_student_reflective_trainer import TeacherStudentReflectiveTrainer\n",
    "from verl.trainer.ppo.reward import load_reward_manager\n",
    "from verl.utils.device import is_cuda_available\n",
    "from verl.utils.import_utils import load_extern_type\n",
    "\n",
    "\n",
    "@hydra.main(config_path=\"config\", config_name=\"ppo_trainer\", version_base=None)\n",
    "def main(config):\n",
    "    \"\"\"Main entry point for PPO training with Hydra configuration management.\n",
    "\n",
    "    Args:\n",
    "        config_dict: Hydra configuration dictionary containing training parameters.\n",
    "    \"\"\"\n",
    "    run_ppo(config)\n",
    "\n",
    "\n",
    "# Define a function to run the PPO-like training process\n",
    "def run_ppo(config) -> None:\n",
    "    \"\"\"Initialize Ray cluster and run distributed PPO training process.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration object containing all necessary parameters\n",
    "                for distributed PPO training including Ray initialization settings,\n",
    "                model paths, and training hyperparameters.\n",
    "    \"\"\"\n",
    "    # Check if Ray is not initialized\n",
    "    if not ray.is_initialized():\n",
    "        # Initialize Ray with a local cluster configuration\n",
    "        # Set environment variables in the runtime environment to control tokenizer parallelism,\n",
    "        # NCCL debug level, VLLM logging level, and allow runtime LoRA updating\n",
    "        # `num_cpus` specifies the number of CPU cores Ray can use, obtained from the configuration\n",
    "        ray.init(\n",
    "            runtime_env=get_ppo_ray_runtime_env(),\n",
    "            num_cpus=config.ray_init.num_cpus,\n",
    "        )\n",
    "\n",
    "    # Create a remote instance of the TaskRunner class, and\n",
    "    # Execute the `run` method of the TaskRunner instance remotely and wait for it to complete\n",
    "    if (\n",
    "        is_cuda_available\n",
    "        and config.trainer.get(\"profile_steps\") is not None\n",
    "        and len(config.trainer.get(\"profile_steps\", [])) > 0\n",
    "    ):\n",
    "        nsight_options = OmegaConf.to_container(config.trainer.controller_nsight_options)\n",
    "        runner = TaskRunner.options(runtime_env={\"nsight\": nsight_options}).remote()\n",
    "    else:\n",
    "        runner = TaskRunner.remote()\n",
    "    ray.get(runner.run.remote(config))\n",
    "\n",
    "    # [Optional] get the path of the timeline trace file from the configuration, default to None\n",
    "    # This file is used for performance analysis\n",
    "    timeline_json_file = config.ray_init.get(\"timeline_json_file\", None)\n",
    "    if timeline_json_file:\n",
    "        ray.timeline(filename=timeline_json_file)\n",
    "\n",
    "\n",
    "@ray.remote(num_cpus=1)  # please make sure main_task is not scheduled on head\n",
    "class TaskRunner:\n",
    "    \"\"\"Ray remote class for executing distributed PPO training tasks.\n",
    "\n",
    "    This class encapsulates the main training logic and runs as a Ray remote actor\n",
    "    to enable distributed execution across multiple nodes and GPUs.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, config):\n",
    "        \"\"\"Execute the main PPO training workflow.\n",
    "\n",
    "        This method sets up the distributed training environment, initializes\n",
    "        workers, datasets, and reward functions, then starts the training process.\n",
    "\n",
    "        Args:\n",
    "            config: Training configuration object containing all parameters needed\n",
    "                   for setting up and running the PPO training process.\n",
    "        \"\"\"\n",
    "        # Print the initial configuration. `resolve=True` will evaluate symbolic values.\n",
    "        from pprint import pprint\n",
    "\n",
    "        from omegaconf import OmegaConf\n",
    "\n",
    "        from verl.utils.fs import copy_to_local\n",
    "\n",
    "        print(f\"TaskRunner hostname: {socket.gethostname()}, PID: {os.getpid()}\")\n",
    "        pprint(OmegaConf.to_container(config, resolve=True))\n",
    "        OmegaConf.resolve(config)\n",
    "\n",
    "        # Download the checkpoint from HDFS to the local machine.\n",
    "        # `use_shm` determines whether to use shared memory, which could lead to faster model loading if turned on\n",
    "        local_path = copy_to_local(\n",
    "            config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get(\"use_shm\", False)\n",
    "        )\n",
    "\n",
    "        # Instantiate the tokenizer and processor.\n",
    "        from verl.utils import hf_processor, hf_tokenizer\n",
    "\n",
    "        trust_remote_code = config.data.get(\"trust_remote_code\", False)\n",
    "        tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)\n",
    "        # Used for multimodal LLM, could be None\n",
    "        processor = hf_processor(local_path, trust_remote_code=trust_remote_code, use_fast=True)\n",
    "\n",
    "        # Define worker classes based on the actor strategy.\n",
    "        if config.actor_rollout_ref.actor.strategy in {\"fsdp\", \"fsdp2\"}:\n",
    "            assert config.critic.strategy in {\"fsdp\", \"fsdp2\"}\n",
    "            from verl.single_controller.ray import RayWorkerGroup\n",
    "            from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker\n",
    "\n",
    "            use_legacy_worker_impl = config.trainer.get(\"use_legacy_worker_impl\", \"auto\")\n",
    "            if use_legacy_worker_impl in [\"auto\", \"enable\"]:\n",
    "                # import warnings\n",
    "                # warnings.warn(f\"Legacy worker impl is going to be deprecated, will be removed in the future. \\\n",
    "                #   Please set trainer.use_legacy_worker_impl = false to switch to the new worker implementation.\")\n",
    "                from verl.workers.fsdp_workers import CriticWorker\n",
    "            elif use_legacy_worker_impl == \"disable\":\n",
    "                from verl.workers.roles import CriticWorker\n",
    "\n",
    "                print(\"Using new worker implementation\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid use_legacy_worker_impl: {use_legacy_worker_impl}\")\n",
    "\n",
    "            actor_rollout_cls = (\n",
    "                AsyncActorRolloutRefWorker\n",
    "                if config.actor_rollout_ref.rollout.mode == \"async\"\n",
    "                else ActorRolloutRefWorker\n",
    "            )\n",
    "            ray_worker_group_cls = RayWorkerGroup\n",
    "\n",
    "        elif config.actor_rollout_ref.actor.strategy == \"megatron\":\n",
    "            assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "            from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup\n",
    "            from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker\n",
    "\n",
    "            actor_rollout_cls = (\n",
    "                AsyncActorRolloutRefWorker\n",
    "                if config.actor_rollout_ref.rollout.mode == \"async\"\n",
    "                else ActorRolloutRefWorker\n",
    "            )\n",
    "            ray_worker_group_cls = NVMegatronRayWorkerGroup\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role\n",
    "\n",
    "        # Map roles to their corresponding remote worker classes.\n",
    "        role_worker_mapping = {\n",
    "            Role.ActorRollout: ray.remote(actor_rollout_cls),\n",
    "            Role.Critic: ray.remote(CriticWorker),\n",
    "        }\n",
    "\n",
    "        # Define the resource pool specification.\n",
    "        # Map roles to the resource pool.\n",
    "        global_pool_id = \"global_pool\"\n",
    "        resource_pool_spec = {\n",
    "            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\n",
    "        }\n",
    "        mapping = {\n",
    "            Role.ActorRollout: global_pool_id,\n",
    "            Role.Critic: global_pool_id,\n",
    "        }\n",
    "\n",
    "        # We should adopt a multi-source reward function here:\n",
    "        # - for rule-based rm, we directly call a reward score\n",
    "        # - for model-based rm, we call a model\n",
    "        # - for code related prompt, we send to a sandbox if there are test cases\n",
    "        # finally, we combine all the rewards together\n",
    "        # The reward type depends on the tag of the data\n",
    "        if config.reward_model.enable:\n",
    "            if config.reward_model.strategy in {\"fsdp\", \"fsdp2\"}:\n",
    "                from verl.workers.fsdp_workers import RewardModelWorker\n",
    "            elif config.reward_model.strategy == \"megatron\":\n",
    "                from verl.workers.megatron_workers import RewardModelWorker\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)\n",
    "            mapping[Role.RewardModel] = global_pool_id\n",
    "\n",
    "        # Add a reference policy worker if KL loss or KL reward is used.\n",
    "        # For TeacherStudentReflectiveTrainer, we ALWAYS need a RefPolicy to act as the Teacher.\n",
    "        # So we remove the conditional check or ensure it's always added.\n",
    "        \n",
    "        # 强制添加 RefPolicy\n",
    "        role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker)\n",
    "        mapping[Role.RefPolicy] = global_pool_id\n",
    "\n",
    "        # Load the reward manager for training and validation.\n",
    "        reward_fn = load_reward_manager(\n",
    "            config, tokenizer, num_examine=0, **config.reward_model.get(\"reward_kwargs\", {})\n",
    "        )\n",
    "        val_reward_fn = load_reward_manager(\n",
    "            config, tokenizer, num_examine=1, **config.reward_model.get(\"reward_kwargs\", {})\n",
    "        )\n",
    "        resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)\n",
    "\n",
    "        from verl.utils.dataset.rl_dataset import collate_fn\n",
    "\n",
    "        # Create training and validation datasets.\n",
    "        train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor, is_train=True)\n",
    "        val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor, is_train=False)\n",
    "        train_sampler = create_rl_sampler(config.data, train_dataset)\n",
    "\n",
    "        # Initialize the PPO trainer.\n",
    "        trainer = TeacherStudentReflectiveTrainer(\n",
    "            config=config,\n",
    "            tokenizer=tokenizer,\n",
    "            processor=processor,\n",
    "            role_worker_mapping=role_worker_mapping,\n",
    "            resource_pool_manager=resource_pool_manager,\n",
    "            ray_worker_group_cls=ray_worker_group_cls,\n",
    "            reward_fn=reward_fn,\n",
    "            val_reward_fn=val_reward_fn,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            collate_fn=collate_fn,\n",
    "            train_sampler=train_sampler,\n",
    "        )\n",
    "        # Initialize the workers of the trainer.\n",
    "        trainer.init_workers()\n",
    "        # Start the training process.\n",
    "        trainer.fit()\n",
    "\n",
    "\n",
    "def create_rl_dataset(data_paths, data_config, tokenizer, processor, is_train=True):\n",
    "    \"\"\"Create a dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_paths: List of paths to data files.\n",
    "        data_config: The data config.\n",
    "        tokenizer (Tokenizer): The tokenizer.\n",
    "        processor (Processor): The processor.\n",
    "\n",
    "    Returns:\n",
    "        dataset (Dataset): The dataset.\n",
    "    \"\"\"\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    from verl.utils.dataset.rl_dataset import RLHFDataset\n",
    "\n",
    "    # Check if a custom dataset class is specified in the data configuration\n",
    "    # and if the path to the custom class is provided\n",
    "    if \"custom_cls\" in data_config and data_config.custom_cls.get(\"path\", None) is not None:\n",
    "        # Dynamically load the custom dataset class\n",
    "        dataset_cls = load_extern_type(data_config.custom_cls.path, data_config.custom_cls.name)\n",
    "        # Verify that the custom dataset class inherits from torch.utils.data.Dataset\n",
    "        if not issubclass(dataset_cls, Dataset):\n",
    "            raise TypeError(\n",
    "                f\"The custom dataset class '{data_config.custom_cls.name}' from \"\n",
    "                f\"'{data_config.custom_cls.path}' must inherit from torch.utils.data.Dataset\"\n",
    "            )\n",
    "    elif \"datagen\" in data_config and data_config.datagen.get(\"path\", None) is not None and is_train:\n",
    "        # If a data generation strategy is specified, use the DynamicGenDataset class\n",
    "        from verl.utils.dataset.dynamicgen_dataset import DynamicGenDataset\n",
    "\n",
    "        dataset_cls = DynamicGenDataset\n",
    "        print(\"Using DynamicGenDataset for data generation.\")\n",
    "\n",
    "    else:\n",
    "        # Use the default RLHFDataset class if no custom class is specified\n",
    "        dataset_cls = RLHFDataset\n",
    "    print(f\"Using dataset class: {dataset_cls.__name__}\")\n",
    "\n",
    "    # Instantiate the dataset using the determined dataset class\n",
    "    dataset = dataset_cls(\n",
    "        data_files=data_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        processor=processor,\n",
    "        config=data_config,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_rl_sampler(data_config, dataset):\n",
    "    \"\"\"Create a sampler for the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_config: The data config.\n",
    "        dataset (Dataset): The dataset.\n",
    "\n",
    "    Returns:\n",
    "        sampler (Sampler): The sampler.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "    if data_config.sampler is not None and data_config.sampler.get(\"class_path\", None) is not None:\n",
    "        curriculum_class = load_extern_type(\n",
    "            data_config.sampler.class_path,\n",
    "            data_config.sampler.class_name,\n",
    "        )\n",
    "        sampler = curriculum_class(\n",
    "            data_source=dataset,\n",
    "            data_config=data_config,\n",
    "        )\n",
    "        assert isinstance(sampler, AbstractSampler)\n",
    "        assert data_config.get(\"dataloader_num_workers\", 8) == 0, (\n",
    "            \"If using curriculum, num_workers must be 0 to prevent data caching. \"\n",
    "            \"If the dataloader caches data before the batch is done the \"\n",
    "            \"curriculum sampler won't have the opportunity to reorder it. \"\n",
    "        )\n",
    "\n",
    "    # Use a sampler to facilitate checkpoint resumption.\n",
    "    # If shuffling is enabled in the data configuration, create a random sampler.\n",
    "    elif data_config.shuffle:\n",
    "        train_dataloader_generator = torch.Generator()\n",
    "        train_dataloader_generator.manual_seed(data_config.get(\"seed\", 1))\n",
    "        sampler = RandomSampler(data_source=dataset, generator=train_dataloader_generator)\n",
    "    else:\n",
    "        # If shuffling is disabled, use a sequential sampler to iterate through the dataset in order.\n",
    "        sampler = SequentialSampler(data_source=dataset)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "dp_actor.py修改代码如下\n",
    "\"\"\"\n",
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "# Copyright 2023-2024 SGLang Team\n",
    "# Copyright 2025 ModelBest Inc. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Single Process Actor\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "import verl.utils.torch_functional as verl_F\n",
    "from verl import DataProto\n",
    "from verl.trainer.ppo.core_algos import agg_loss, compute_policy_loss, get_policy_loss_fn, kl_penalty\n",
    "from verl.utils.device import get_device_name, is_cuda_available, is_npu_available\n",
    "from verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\n",
    "from verl.utils.profiler import GPUMemoryLogger\n",
    "from verl.utils.py_functional import append_to_dict\n",
    "from verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\n",
    "from verl.utils.torch_functional import logprobs_from_logits\n",
    "from verl.utils.ulysses import gather_outputs_and_unpad, ulysses_pad, ulysses_pad_and_slice_inputs\n",
    "from verl.workers.actor import BasePPOActor\n",
    "\n",
    "if is_cuda_available:\n",
    "    from flash_attn.bert_padding import index_first_axis, pad_input, rearrange, unpad_input\n",
    "elif is_npu_available:\n",
    "    from transformers.integrations.npu_flash_attention import index_first_axis, pad_input, rearrange, unpad_input\n",
    "\n",
    "\n",
    "__all__ = [\"DataParallelPPOActor\"]\n",
    "\n",
    "logger = logging.getLogger(__file__)\n",
    "logger.setLevel(os.getenv(\"VERL_LOGGING_LEVEL\", \"WARN\"))\n",
    "\n",
    "\n",
    "class DataParallelPPOActor(BasePPOActor):\n",
    "    def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):\n",
    "        \"\"\"When optimizer is None, it is Reference Policy\"\"\"\n",
    "        super().__init__(config)\n",
    "        self.actor_module = actor_module\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "\n",
    "        self.use_remove_padding = self.config.get(\"use_remove_padding\", False)\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print(f\"Actor use_remove_padding={self.use_remove_padding}\")\n",
    "        self.use_fused_kernels = self.config.get(\"use_fused_kernels\", False)\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print(f\"Actor use_fused_kernels={self.use_fused_kernels}\")\n",
    "\n",
    "        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size\n",
    "        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1\n",
    "\n",
    "        if self.config.entropy_from_logits_with_chunking:\n",
    "            entropy_from_logits = verl_F.entropy_from_logits_with_chunking\n",
    "        else:\n",
    "            entropy_from_logits = verl_F.entropy_from_logits\n",
    "\n",
    "        self.compute_entropy_from_logits = (\n",
    "            torch.compile(entropy_from_logits, dynamic=True)\n",
    "            if self.config.get(\"use_torch_compile\", True)  #  use torch compile by default\n",
    "            else entropy_from_logits\n",
    "        )\n",
    "        self.device_name = get_device_name()\n",
    "\n",
    "    def _forward_micro_batch(\n",
    "        self, micro_batch, temperature, calculate_entropy=False\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            entropy: # (bs, response_len)\n",
    "            log_probs: # (bs, response_len)\n",
    "        \"\"\"\n",
    "        response_length = micro_batch[\"responses\"].size(-1)\n",
    "        multi_modal_inputs = {}\n",
    "        if \"multi_modal_inputs\" in micro_batch.keys():\n",
    "            if \"image_bound\" in micro_batch[\"multi_modal_inputs\"][0]:  # minicpm-o logic\n",
    "                for key in micro_batch[\"multi_modal_inputs\"][0].keys():\n",
    "                    multi_modal_inputs[key] = [inputs[key] for inputs in micro_batch[\"multi_modal_inputs\"]]\n",
    "            else:\n",
    "                for key in micro_batch[\"multi_modal_inputs\"][0].keys():\n",
    "                    multi_modal_inputs[key] = torch.cat(\n",
    "                        [inputs[key] for inputs in micro_batch[\"multi_modal_inputs\"]], dim=0\n",
    "                    )\n",
    "\n",
    "        with torch.autocast(device_type=self.device_name, dtype=torch.bfloat16):\n",
    "            input_ids = micro_batch[\"input_ids\"]\n",
    "            batch_size, seqlen = input_ids.shape\n",
    "            attention_mask = micro_batch[\"attention_mask\"]\n",
    "            position_ids = micro_batch[\"position_ids\"]\n",
    "            entropy = None\n",
    "            if position_ids.dim() == 3:  # qwen2vl mrope\n",
    "                position_ids = position_ids.transpose(0, 1)  # (bsz, 3, seqlen) -> (3, bsz, seqlen)\n",
    "\n",
    "            if self.use_remove_padding:\n",
    "                input_ids_rmpad, indices, cu_seqlens, *_ = unpad_input(\n",
    "                    input_ids.unsqueeze(-1), attention_mask\n",
    "                )  # input_ids_rmpad (total_nnz, ...)\n",
    "                input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)\n",
    "\n",
    "                # unpad the position_ids to align the rotary\n",
    "                if position_ids.dim() == 3:\n",
    "                    position_ids_rmpad = (\n",
    "                        index_first_axis(rearrange(position_ids, \"c b s ... -> (b s) c ...\"), indices)\n",
    "                        .transpose(0, 1)\n",
    "                        .unsqueeze(1)\n",
    "                    )  # (3, bsz, seqlen) -> (3, 1, bsz * seqlen)\n",
    "                else:\n",
    "                    position_ids_rmpad = index_first_axis(\n",
    "                        rearrange(position_ids.unsqueeze(-1), \"b s ... -> (b s) ...\"), indices\n",
    "                    ).transpose(0, 1)\n",
    "\n",
    "                if \"image_bound\" in multi_modal_inputs:\n",
    "                    from verl.utils.dataset.vision_utils import process_multi_modal_inputs_for_minicpmo\n",
    "\n",
    "                    multi_modal_inputs = process_multi_modal_inputs_for_minicpmo(\n",
    "                        input_ids, attention_mask, position_ids, cu_seqlens, multi_modal_inputs\n",
    "                    )\n",
    "\n",
    "                # for compute the log_prob\n",
    "                input_ids_rmpad_rolled = torch.roll(input_ids_rmpad, shifts=-1, dims=1)  # (1, total_nnz)\n",
    "\n",
    "                # pad and slice the inputs if sp > 1\n",
    "                if self.use_ulysses_sp:\n",
    "                    is_vlm_model = \"multi_modal_inputs\" in micro_batch.keys()\n",
    "                    if is_vlm_model:\n",
    "                        # vlm model's inputs will be sliced after embedding\n",
    "                        input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad(\n",
    "                            input_ids_rmpad,\n",
    "                            position_ids_rmpad=position_ids_rmpad,\n",
    "                            sp_size=self.ulysses_sequence_parallel_size,\n",
    "                        )\n",
    "                    else:\n",
    "                        input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad_and_slice_inputs(\n",
    "                            input_ids_rmpad,\n",
    "                            position_ids_rmpad=position_ids_rmpad,\n",
    "                            sp_size=self.ulysses_sequence_parallel_size,\n",
    "                        )\n",
    "                    input_ids_rmpad_rolled, _, _ = ulysses_pad_and_slice_inputs(\n",
    "                        input_ids_rmpad_rolled,\n",
    "                        position_ids_rmpad=None,\n",
    "                        sp_size=self.ulysses_sequence_parallel_size,\n",
    "                    )\n",
    "\n",
    "                input_ids_rmpad_rolled = input_ids_rmpad_rolled.squeeze(0)  # ((total_nnz / sp) + pad)\n",
    "\n",
    "                # only pass input_ids and position_ids to enable flash_attn_varlen\n",
    "                extra_args = {}\n",
    "                if self.use_fused_kernels:\n",
    "                    extra_args[\"temperature\"] = temperature\n",
    "                    extra_args[\"return_dict\"] = True\n",
    "\n",
    "                output = self.actor_module(\n",
    "                    input_ids=input_ids_rmpad,\n",
    "                    attention_mask=None,\n",
    "                    position_ids=position_ids_rmpad,\n",
    "                    **multi_modal_inputs,\n",
    "                    use_cache=False,\n",
    "                    **extra_args,\n",
    "                )  # prevent model thinks we are generating\n",
    "\n",
    "                if self.use_fused_kernels:\n",
    "                    log_probs = output.log_probs.squeeze(0)  # (total_nnz,)\n",
    "                    entropy_rmpad = output.entropy.squeeze(0)  # (total_nnz,)\n",
    "\n",
    "                else:\n",
    "                    logits_rmpad = output.logits.squeeze(0)  # (total_nnz, vocab_size)\n",
    "                    logits_rmpad.div_(temperature)\n",
    "\n",
    "                    # if use_sp: ((total_nnz / sp) + pad) ; if not use_sp: (batch, seqlen)\n",
    "                    inplace_backward = True\n",
    "                    if calculate_entropy:\n",
    "                        inplace_backward = False\n",
    "                    log_probs = logprobs_from_logits(\n",
    "                        logits=logits_rmpad,\n",
    "                        labels=input_ids_rmpad_rolled,\n",
    "                        inplace_backward=inplace_backward,\n",
    "                    )\n",
    "\n",
    "                    # compute entropy\n",
    "                    if calculate_entropy:\n",
    "                        if not self.config.entropy_checkpointing:\n",
    "                            entropy_rmpad = self.compute_entropy_from_logits(logits_rmpad)  # ((total_nnz / sp) + pad)\n",
    "                        else:\n",
    "                            entropy_rmpad = torch.utils.checkpoint.checkpoint(\n",
    "                                self.compute_entropy_from_logits, logits_rmpad\n",
    "                            )\n",
    "\n",
    "                # gather log_prob if sp > 1\n",
    "                if self.use_ulysses_sp:\n",
    "                    # gather and unpad for the ulysses sp\n",
    "                    log_probs = gather_outputs_and_unpad(\n",
    "                        log_probs,\n",
    "                        gather_dim=0,\n",
    "                        unpad_dim=0,\n",
    "                        padding_size=pad_size,\n",
    "                    )\n",
    "                    if calculate_entropy:\n",
    "                        entropy_rmpad = gather_outputs_and_unpad(\n",
    "                            entropy_rmpad,\n",
    "                            gather_dim=0,\n",
    "                            unpad_dim=0,\n",
    "                            padding_size=pad_size,\n",
    "                        )\n",
    "                # pad back to (bsz, seqlen)\n",
    "                if calculate_entropy:\n",
    "                    full_entropy = pad_input(\n",
    "                        hidden_states=entropy_rmpad.unsqueeze(-1),\n",
    "                        indices=indices,\n",
    "                        batch=batch_size,\n",
    "                        seqlen=seqlen,\n",
    "                    )\n",
    "                full_log_probs = pad_input(\n",
    "                    hidden_states=log_probs.unsqueeze(-1),\n",
    "                    indices=indices,\n",
    "                    batch=batch_size,\n",
    "                    seqlen=seqlen,\n",
    "                )\n",
    "\n",
    "                # only return response part:\n",
    "                if calculate_entropy:\n",
    "                    entropy = full_entropy.squeeze(-1)[:, -response_length - 1 : -1]  # (bsz, response_length)\n",
    "                log_probs = full_log_probs.squeeze(-1)[:, -response_length - 1 : -1]  # (bsz, response_length)\n",
    "\n",
    "            else:  # not using rmpad and no ulysses sp\n",
    "                extra_args = {}\n",
    "                if self.use_fused_kernels:\n",
    "                    extra_args[\"temperature\"] = temperature\n",
    "                    extra_args[\"return_dict\"] = True\n",
    "\n",
    "                output = self.actor_module(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    **multi_modal_inputs,\n",
    "                    use_cache=False,\n",
    "                    **extra_args,\n",
    "                )  # prevent model thinks we are generating\n",
    "\n",
    "                if self.use_fused_kernels:\n",
    "                    log_probs = output.log_probs[:, -response_length - 1 : -1]\n",
    "                    entropy = output.entropy[:, -response_length - 1 : -1]  # (bsz, response_length)\n",
    "\n",
    "                else:\n",
    "                    logits = output.logits\n",
    "\n",
    "                    logits.div_(temperature)\n",
    "                    logits = logits[:, -response_length - 1 : -1, :]  # (bsz, response_length, vocab_size)\n",
    "                    log_probs = logprobs_from_logits(logits, micro_batch[\"responses\"])\n",
    "                    if calculate_entropy:\n",
    "                        if not self.config.entropy_checkpointing:\n",
    "                            entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)\n",
    "                        else:\n",
    "                            entropy = torch.utils.checkpoint.checkpoint(verl_F.entropy_from_logits, logits)\n",
    "\n",
    "            return entropy, log_probs\n",
    "\n",
    "    def _optimizer_step(self):\n",
    "        assert self.config.grad_clip is not None\n",
    "\n",
    "        if isinstance(self.actor_module, FSDP):\n",
    "            grad_norm = self.actor_module.clip_grad_norm_(max_norm=self.config.grad_clip)\n",
    "        elif isinstance(self.actor_module, FSDPModule):\n",
    "            grad_norm = fsdp2_clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.grad_clip)\n",
    "        else:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.grad_clip)\n",
    "\n",
    "        # if grad_norm is not finite, skip the update\n",
    "        if not torch.isfinite(grad_norm):\n",
    "            print(f\"WARN: rank {torch.distributed.get_rank()} grad_norm is not finite: {grad_norm}\")\n",
    "            self.actor_optimizer.zero_grad()\n",
    "        else:\n",
    "            self.actor_optimizer.step()\n",
    "        return grad_norm\n",
    "\n",
    "    @GPUMemoryLogger(role=\"dp actor\", logger=logger)\n",
    "    def compute_log_prob(self, data: DataProto, calculate_entropy=False) -> torch.Tensor:\n",
    "        \"\"\"Compute the log probability of the responses given input_ids, attention_mask and position_ids\"\"\"\n",
    "        # set to eval\n",
    "        self.actor_module.eval()\n",
    "\n",
    "        micro_batch_size = data.meta_info[\"micro_batch_size\"]\n",
    "        temperature = data.meta_info[\"temperature\"]\n",
    "        use_dynamic_bsz = data.meta_info[\"use_dynamic_bsz\"]\n",
    "        has_multi_modal_inputs = \"multi_modal_inputs\" in data.non_tensor_batch.keys()\n",
    "        select_keys = [\"responses\", \"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "        non_tensor_select_keys = [\"multi_modal_inputs\"] if has_multi_modal_inputs else []\n",
    "\n",
    "        data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)\n",
    "\n",
    "        if use_dynamic_bsz:\n",
    "            max_token_len = data.meta_info[\"max_token_len\"] * self.ulysses_sequence_parallel_size\n",
    "            micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\n",
    "        else:\n",
    "            micro_batches = data.split(micro_batch_size)\n",
    "\n",
    "        log_probs_lst = []\n",
    "        entropy_lst = []\n",
    "        for micro_batch in micro_batches:\n",
    "            model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\n",
    "            with torch.no_grad():\n",
    "                entropy, log_probs = self._forward_micro_batch(\n",
    "                    model_inputs, temperature=temperature, calculate_entropy=calculate_entropy\n",
    "                )\n",
    "            log_probs_lst.append(log_probs)\n",
    "            if calculate_entropy:\n",
    "                entropy_lst.append(entropy)\n",
    "\n",
    "        log_probs = torch.concat(log_probs_lst, dim=0)\n",
    "        entropys = None\n",
    "        if calculate_entropy:\n",
    "            entropys = torch.concat(entropy_lst, dim=0)\n",
    "\n",
    "        if use_dynamic_bsz:\n",
    "            log_probs = restore_dynamic_batch(log_probs, batch_idx_list)\n",
    "            if calculate_entropy:\n",
    "                entropys = restore_dynamic_batch(entropys, batch_idx_list)\n",
    "\n",
    "        return log_probs, entropys\n",
    "\n",
    "    @GPUMemoryLogger(role=\"dp actor\", logger=logger)\n",
    "    def update_policy(self, data: DataProto):\n",
    "        # make sure we are in training mode\n",
    "        self.actor_module.train()\n",
    "\n",
    "        temperature = data.meta_info[\"temperature\"]\n",
    "\n",
    "        select_keys = [\n",
    "            \"responses\",\n",
    "            \"response_mask\",\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"position_ids\",\n",
    "            \"old_log_probs\",\n",
    "            \"advantages\",\n",
    "        ]\n",
    "        if self.config.use_kl_loss:\n",
    "            select_keys.append(\"ref_log_prob\")\n",
    "\n",
    "        has_multi_modal_inputs = \"multi_modal_inputs\" in data.non_tensor_batch.keys()\n",
    "        non_tensor_select_keys = [\"multi_modal_inputs\"] if has_multi_modal_inputs else []\n",
    "\n",
    "        data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)\n",
    "\n",
    "        # Split to make minibatch iterator\n",
    "        mini_batches = data.split(self.config.ppo_mini_batch_size)\n",
    "\n",
    "        metrics = {}\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            for batch_idx, mini_batch in enumerate(mini_batches):\n",
    "                if self.config.use_dynamic_bsz:\n",
    "                    max_token_len = self.config.ppo_max_token_len_per_gpu * self.ulysses_sequence_parallel_size\n",
    "                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)\n",
    "                else:\n",
    "                    self.gradient_accumulation = (\n",
    "                        self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu\n",
    "                    )\n",
    "                    micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "\n",
    "                for micro_batch in micro_batches:\n",
    "                    micro_batch_metrics = {}\n",
    "                    model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\n",
    "                    response_mask = model_inputs[\"response_mask\"]\n",
    "                    old_log_prob = model_inputs[\"old_log_probs\"]\n",
    "                    advantages = model_inputs[\"advantages\"]\n",
    "\n",
    "                    clip_ratio = self.config.clip_ratio\n",
    "                    clip_ratio_low = (\n",
    "                        self.config.clip_ratio_low if self.config.clip_ratio_low is not None else clip_ratio\n",
    "                    )\n",
    "                    clip_ratio_high = (\n",
    "                        self.config.clip_ratio_high if self.config.clip_ratio_high is not None else clip_ratio\n",
    "                    )\n",
    "                    clip_ratio_c = self.config.get(\"clip_ratio_c\", 3.0)\n",
    "                    entropy_coeff = self.config.entropy_coeff\n",
    "                    loss_agg_mode = self.config.loss_agg_mode\n",
    "\n",
    "                    calculate_entropy = False\n",
    "                    if entropy_coeff != 0:\n",
    "                        calculate_entropy = True\n",
    "                    entropy, log_prob = self._forward_micro_batch(\n",
    "                        model_inputs, temperature=temperature, calculate_entropy=calculate_entropy\n",
    "                    )\n",
    "\n",
    "                    # =========================================================================\n",
    "                    # 修复: 健壮地读取 loss_mode\n",
    "                    # =========================================================================\n",
    "                    if isinstance(self.config.policy_loss, str):\n",
    "                        loss_mode = self.config.policy_loss\n",
    "                    else:\n",
    "                        loss_mode = self.config.policy_loss.get(\"loss_mode\", \"vanilla\")\n",
    "\n",
    "                    # =========================================================================\n",
    "                    # 新增: distillation 模式 (Pure Policy Gradient / No Clipping)\n",
    "                    # =========================================================================\n",
    "                    if loss_mode == \"distillation\":\n",
    "                        # 直接最大化 (advantages * log_prob)\n",
    "                        # 我们假设 advantages 已经包含了 (log P_T - log P_S) 的信号\n",
    "                        # 这是一个无偏的 KL 散度梯度估计\n",
    "                        pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
    "                        \n",
    "                        # 占位符，避免日志报错\n",
    "                        pg_clipfrac = torch.tensor(0.0, device=pg_loss.device)\n",
    "                        ppo_kl = torch.tensor(0.0, device=pg_loss.device)\n",
    "                        pg_clipfrac_lower = torch.tensor(0.0, device=pg_loss.device)\n",
    "\n",
    "                    elif loss_mode == \"vanilla\":\n",
    "                        pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(\n",
    "                            old_log_prob=old_log_prob,\n",
    "                            log_prob=log_prob,\n",
    "                            advantages=advantages,\n",
    "                            response_mask=response_mask,\n",
    "                            cliprange=clip_ratio,\n",
    "                            cliprange_low=clip_ratio_low,\n",
    "                            cliprange_high=clip_ratio_high,\n",
    "                            clip_ratio_c=clip_ratio_c,\n",
    "                            loss_agg_mode=loss_agg_mode,\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        policy_loss_fn = get_policy_loss_fn(loss_mode)\n",
    "                        pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(\n",
    "                            old_log_prob=old_log_prob,\n",
    "                            log_prob=log_prob,\n",
    "                            advantages=advantages,\n",
    "                            response_mask=response_mask,\n",
    "                            loss_agg_mode=loss_agg_mode,\n",
    "                            config=self.config,\n",
    "                        )\n",
    "\n",
    "                    if entropy_coeff != 0:\n",
    "                        entropy_loss = agg_loss(loss_mat=entropy, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)\n",
    "                        policy_loss = pg_loss - entropy_loss * entropy_coeff\n",
    "                    else:\n",
    "                        policy_loss = pg_loss\n",
    "\n",
    "                    if self.config.use_kl_loss:\n",
    "                        ref_log_prob = model_inputs[\"ref_log_prob\"]\n",
    "                        kld = kl_penalty(\n",
    "                            logprob=log_prob, ref_logprob=ref_log_prob, kl_penalty=self.config.kl_loss_type\n",
    "                        )\n",
    "                        kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)\n",
    "\n",
    "                        policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef\n",
    "                        micro_batch_metrics[\"actor/kl_loss\"] = kl_loss.detach().item()\n",
    "                        micro_batch_metrics[\"actor/kl_coef\"] = self.config.kl_loss_coef\n",
    "\n",
    "                    if self.config.use_dynamic_bsz:\n",
    "                        loss = policy_loss * (response_mask.shape[0] / self.config.ppo_mini_batch_size)\n",
    "                    else:\n",
    "                        loss = policy_loss / self.gradient_accumulation\n",
    "                    loss.backward()\n",
    "\n",
    "                    micro_batch_metrics.update(\n",
    "                        {\n",
    "                            \"actor/pg_loss\": pg_loss.detach().item(),\n",
    "                            \"actor/pg_clipfrac\": pg_clipfrac.detach().item(),\n",
    "                            \"actor/ppo_kl\": ppo_kl.detach().item(),\n",
    "                            \"actor/pg_clipfrac_lower\": pg_clipfrac_lower.detach().item(),\n",
    "                        }\n",
    "                    )\n",
    "                    append_to_dict(metrics, micro_batch_metrics)\n",
    "\n",
    "                grad_norm = self._optimizer_step()\n",
    "                mini_batch_metrics = {\"actor/grad_norm\": grad_norm.detach().item()}\n",
    "                append_to_dict(metrics, mini_batch_metrics)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        return metrics\n",
    "\"\"\"\n",
    "启动脚本代码如下\n",
    "\"\"\"\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "export DATA_DIR='/data/kcl/lpy/data'\n",
    "WAND_PROJECT=\"self_reflect\"\n",
    "export SWANLAB_API_KEY=\"DDMUFxYQU6Om1ZuHWYoxm\"           # 设置在线跟踪模式API\n",
    "export BASE_MODEL='/data/kcl/lpy/models/qwen3_4b_instruct'\n",
    "export EXPERIMENT_NAME=sr_dl_teacher_student_gsm8k_qwen3_4b_instruct\n",
    "N_NODES=1\n",
    "n_gpus_per_node=8\n",
    "export BASE_DIR=$(pwd)\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "export VLLM_USE_V1=1\n",
    "PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_self_reflective \\\n",
    " data.train_files=$DATA_DIR/gsm8k/train.parquet \\\n",
    " data.val_files=$DATA_DIR/gsm8k/test.parquet \\\n",
    " data.train_batch_size=256 \\\n",
    " data.max_prompt_length=512 \\\n",
    " data.max_response_length=512 \\\n",
    " actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    " actor_rollout_ref.actor.optim.lr=5e-7 \\\n",
    " actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n",
    " actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n",
    " actor_rollout_ref.actor.policy_loss=distillation \\\n",
    " actor_rollout_ref.rollout.name=vllm \\\n",
    " actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n",
    " actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
    " actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
    " actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
    " critic.optim.lr=1e-5 \\\n",
    " critic.model.path=$BASE_MODEL \\\n",
    " critic.ppo_micro_batch_size_per_gpu=4 \\\n",
    " algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    " trainer.logger=console \\\n",
    " trainer.val_before_train=False \\\n",
    " trainer.n_gpus_per_node=$n_gpus_per_node \\\n",
    " trainer.nnodes=$N_NODES \\\n",
    " trainer.save_freq=60 \\\n",
    " trainer.test_freq=60 \\\n",
    " trainer.logger=\"['swanlab', 'console']\" \\\n",
    " trainer.project_name=$WAND_PROJECT \\\n",
    " trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    " trainer.default_local_dir=${BASE_DIR}/verl_checkpoints/$EXPERIMENT_NAME \\\n",
    " trainer.total_epochs=15 2>&1 | tee verl_log/$EXPERIMENT_NAME.log\n",
    "\"\"\"\n",
    "现在运行我发现问题，日志输出如下\n",
    "\"\"\"\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ==================== Teacher-Student Reflection Debug (Step 1) ====================\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- [0] Summary Generation Input ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>system\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m You are a helpful math assistant.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>user\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Question: Tom receives a $12 allowance per month. In the first week, he spends a third of it; in the second week, he spends a quarter of what he has left. How much money does he have left to finish the month? Let's think step by step and output the final answer after \"####\".\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Standard Answer: 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Student Answer: Let's solve this step by step.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Tom receives a $12 allowance per month.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **First week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a third of it.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A third of $12 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{3} \\times 12 = 4\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after the first week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 12 - 4 = 8 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **Second week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a quarter of what he has left.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A quarter of $8 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{4} \\times 8 = 2\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after spending in the second week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 8 - 2 = 6 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Task: Verify the Student Answer step-by-step. Is it correct?\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Answer concisely.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>assistant\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- [1] Summary Output ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Yes, the student answer is correct.  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Step-by-step:  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Initial allowance: $12  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - First week: spends $1/3 of $12 = $4 → remaining: $12 - $4 = $8  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Second week: spends $1/4 of $8 = $2 → remaining: $8 - $2 = $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Final amount left: $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- [2] Teacher LogProb Input (Full Context) ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>system\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m You are a helpful math assistant.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>user\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Problem: Tom receives a $12 allowance per month. In the first week, he spends a third of it; in the second week, he spends a quarter of what he has left. How much money does he have left to finish the month? Let's think step by step and output the final answer after \"####\".\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Analysis/Hint: Yes, the student answer is correct.  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Step-by-step:  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Initial allowance: $12  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - First week: spends $1/3 of $12 = $4 → remaining: $12 - $4 = $8  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Second week: spends $1/4 of $8 = $2 → remaining: $8 - $2 = $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Final amount left: $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Based on the analysis above, please provide the correct, step-by-step solution.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>assistant\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Let's solve this step by step.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Tom receives a $12 allowance per month.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **First week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a third of it.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A third of $12 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{3} \\times 12 = 4\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after the first week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 12 - 4 = 8 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **Second week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a quarter of what he has left.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A quarter of $8 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{4} \\times 8 = 2\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after spending in the second week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 8 - 2 = 6 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ============================================================\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ==================== Token-Level KL Analysis (Step 1) ====================\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- Sequence Alignment Check ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m [OK] Token IDs match perfectly.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- Token-wise KL Breakdown (First 50 tokens) ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Token           | ID     | S_LogP   | T_LogP   | Diff    \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ---------------------------------------------------------------------------\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Let             | 10061  | -0.00   | -12.61   | -12.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 's              | 594    | -0.00   | -15.50   | -15.50\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  solve          | 11625  | -0.04   | -14.15   | -14.11\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  this           | 419    | -0.01   | -18.05   | -18.04\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  step           | 3019   | -0.00   | -17.83   | -17.83\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  by             | 553    | -0.00   | -17.40   | -17.40\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  step           | 3019   | -0.00   | -17.83   | -17.83\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m .\\n\\n           | 382    | -0.31   | -13.00   | -12.69\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Tom             | 24732  | -0.25   | -9.66   | -9.40\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  receives       | 21189  | -0.00   | -14.84   | -14.83\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  a              | 264    | -0.00   | -14.08   | -14.08\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  $              | 400    | -0.01   | -14.63   | -14.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 1               | 16     | -0.00   | -11.31   | -11.31\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 2               | 17     | -0.00   | -12.61   | -12.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  allowance      | 40358  | -0.00   | -18.77   | -18.77\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  per            | 817    | -0.00   | -19.28   | -19.28\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  month          | 2254   | -0.00   | -17.45   | -17.45\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m .\\n\\n           | 382    | -0.00   | -13.00   | -13.00\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **              | 334    | -0.00   | -13.63   | -13.63\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m First           | 5338   | -0.85   | -14.77   | -13.93\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  week           | 2003   | -0.00   | -22.22   | -22.22\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m :               | 25     | -0.25   | -16.95   | -16.70\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **\\n            | 1019   | -0.00   | -14.02   | -14.02\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He              | 1519   | -0.00   | -13.55   | -13.55\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  spends         | 37102  | -0.00   | -14.45   | -14.45\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  a              | 264    | -0.00   | -14.08   | -14.08\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  third          | 4843   | -0.00   | -18.66   | -18.66\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  of             | 315    | -0.00   | -15.58   | -15.58\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  it             | 432    | -0.01   | -16.15   | -16.14\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m .\\n\\n           | 382    | -0.00   | -13.00   | -13.00\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A               | 32     | -0.29   | -11.69   | -11.40\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  third          | 4843   | -0.00   | -18.66   | -18.66\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  of             | 315    | -0.00   | -15.58   | -15.58\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  $              | 400    | -0.00   | -14.63   | -14.63\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 1               | 16     | -0.00   | -11.31   | -11.31\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 2               | 17     | -0.00   | -12.61   | -12.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  is             | 374    | -0.00   | -14.92   | -14.92\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m :\\n             | 510    | -0.00   | -15.54   | -15.54\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$              | 14085  | -0.02   | -9.88   | -9.86\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\n              | 198    | -0.00   | -13.74   | -13.74\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\               | 59     | -0.00   | -11.16   | -11.16\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m frac            | 37018  | -0.00   | -10.91   | -10.91\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m {               | 90     | -0.00   | -12.13   | -12.13\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 1               | 16     | -0.00   | -11.31   | -11.31\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m }{              | 15170  | -0.00   | -9.66   | -9.66\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 3               | 18     | -0.00   | -13.41   | -13.41\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m }               | 92     | -0.00   | -8.97   | -8.97\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  \\              | 1124   | -0.00   | -12.17   | -12.17\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m times           | 15136  | -0.00   | -11.63   | -11.63\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m                 | 220    | -0.00   | -16.90   | -16.90\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ============================================================\n",
    "\"\"\"\n",
    "Token-wise KL Breakdown的日志里显示S_LogP都很低，接近0，T_LogP都很大，你分析下原因，是不是哪里出问题了。\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a394a2-8b8f-4de8-95b8-939d5f1013a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常深刻且关键的问题。\n",
      "\n",
      "简短的回答是：**不会破坏，恰恰相反，设置为 1.0 是实现正确的 On-Policy Distillation（以及 PPO/KL 散度计算）的必要条件。**\n",
      "\n",
      "如果不设置为 1.0（特别是你现在的日志显示 Student 似乎用了极低的温度），你的 Distillation 实际上是**失效**的。\n",
      "\n",
      "以下是详细的数学和逻辑解释：\n",
      "\n",
      "### 1. 区分“生成温度”与“计算温度”\n",
      "\n",
      "你需要区分两个阶段的 Temperature：\n",
      "\n",
      "1.  **Rollout / Generation 阶段（生成时）**：\n",
      "    *   这里 Temperature 控制**探索性**。\n",
      "    *   如果你设置 $T=0.1$，Student 会生成非常确定性的回答。\n",
      "    *   如果你设置 $T=1.0$，Student 会生成多样性的回答。\n",
      "    *   **On-Policy 的含义**：指数据必须是由当前 Student 策略生成的。只要数据是 Student 产生的，无论生成时用了什么温度，它都是 On-Policy 数据。\n",
      "\n",
      "2.  **Forward / Compute LogProb 阶段（计算概率时）**：\n",
      "    *   这里 Temperature 决定了**概率分布的形状**。\n",
      "    *   Distillation（蒸馏）的核心数学目标是最小化两个分布的距离（通常是 KL 散度）。\n",
      "    *   模型的**真实分布**是由原始 Logits 定义的（即 $T=1.0$）。\n",
      "\n",
      "### 2. 为什么计算概率时必须是 T=1.0？\n",
      "\n",
      "假设 Student 输出的 Logits 为 $z_S$，Teacher 输出的 Logits 为 $z_T$。\n",
      "\n",
      "#### 情况 A：如果你使用 T=1.0 (标准做法)\n",
      "概率 $P(x) = \\text{Softmax}(z)$。\n",
      "LogProb $\\log P(x) \\approx z_x - \\text{Const}$。\n",
      "你的 Reward (Reverse KL) 近似于：\n",
      "$$ R \\approx \\log P_T(x) - \\log P_S(x) \\approx z_T - z_S $$\n",
      "这意味着：**你要拉近 Student 的 Logits 和 Teacher 的 Logits**。这是蒸馏的本质——让 Student 学习 Teacher 的“软标签”（Soft Targets）。\n",
      "\n",
      "#### 情况 B：如果你使用极低的 T (例如你日志中的情况)\n",
      "你的日志显示 `S_LogP = -0.00`。这说明在计算概率时，系统使用了极低的温度（比如 $T=0.01$），导致 Softmax 变成了 Hardmax。\n",
      "$$ P_S(x) \\approx 1.0 \\implies \\log P_S(x) \\approx 0 $$\n",
      "\n",
      "此时你的 Reward 变成了：\n",
      "$$ R \\approx \\log P_T(x) - 0 $$\n",
      "**后果**：\n",
      "1.  **Student 的梯度消失/截断**：因为 $\\log P_S(x)$ 已经是 0（最大值），Student 认为自己已经“完美”确信了这个 Token，无法再优化自己的分布。\n",
      "2.  **数值不稳定性**：只要 Student 稍微偏离一点点，在低温下 LogProb 就会从 0 瞬间变成 $-\\infty$。\n",
      "3.  **失去了“蒸馏”的意义**：你不再是学习 Teacher 的分布，而是单纯地在用 Teacher 的 LogProb 给 Student 的 Sample 打分，但 Student 自身的分布信息被低温抹去了。\n",
      "\n",
      "### 3. 你的日志暴露的问题\n",
      "\n",
      "看看你的日志：\n",
      "```text\n",
      "Token | S_LogP | T_LogP | Diff\n",
      "Let   | -0.00  | -12.61 | -12.61\n",
      "```\n",
      "这意味着：\n",
      "*   Student 说：“我在 $T \\approx 0$ 的温度下，有 100% 的信心输出 'Let'。”\n",
      "*   Teacher 说：“我在 $T \\approx 1$ 的温度下（推测），觉得 'Let' 的概率很低。”\n",
      "\n",
      "由于 `S_LogP` 被人为地“锐化”到了 0，计算出的 `Diff` 完全由 `T_LogP` 主导。这实际上把 Distillation 变成了一个 **Reinforcement Learning with a Fixed Reward Model**（Teacher 变成了单纯的打分器），而不是让两个模型的分布互相靠近。\n",
      "\n",
      "### 4. 正确的 Distillation 流程\n",
      "\n",
      "为了实现真正的 On-Policy Distillation，流程应该是：\n",
      "\n",
      "1.  **Student 生成 (Rollout)**：\n",
      "    *   可以使用 $T=0.7$ 或 $1.0$（甚至 $T=0.1$ 如果你希望它探索得少一点）。这是为了产生样本 $y$。\n",
      "    *   *注：你的代码里似乎生成时就用了很低的温度，或者 meta_info 传导导致计算时用了低迷温度。*\n",
      "\n",
      "2.  **计算概率 (Forward)**：\n",
      "    *   拿着样本 $y$，计算 **Teacher** 对 $y$ 的 Logits（$T=1.0$）。\n",
      "    *   拿着样本 $y$，计算 **Student** 对 $y$ 的 Logits（$T=1.0$）。\n",
      "    *   **核心理由**：我们要比较的是两个神经网络在“原始状态”下对这个 Token 的看法差异。\n",
      "\n",
      "### 结论\n",
      "\n",
      "**将 `compute_log_prob` 时的 `temperature` 强制设为 1.0 是绝对正确的，也是必须的。**\n",
      "\n",
      "这不会破坏 On-Policy，因为“On-Policy”指的是**样本 $y$ 是由 Student 生成的**。至于我们如何计算 $y$ 的概率来更新梯度，必须使用原始 Logits ($T=1.0$) 才能保证梯度的数学正确性。\n",
      "\n",
      "**建议操作：**\n",
      "请务必按照我上一条回答，在 `fit` 函数中，在调用 `compute_log_prob` 之前，强制将 `meta_info['temperature']` 设置为 `1.0`。\n",
      "\n",
      "```python\n",
      "# 确保 Teacher 和 Student 都在同一把“尺子”（T=1.0）下衡量概率\n",
      "teacher_batch.meta_info['temperature'] = 1.0\n",
      "batch.meta_info['temperature'] = 1.0 \n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "如果设置为1.0会破坏我想要训练on policy distillation的目的吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38f74224-2fa7-480e-8bc2-cda82a07be9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常奇怪的现象。文本看起来只有几百个 Token，远未达到 1024 的限制，却突然截断了。\n",
      "\n",
      "这通常有三种可能：\n",
      "1.  **配置未生效**：代码读取 `max_response_length` 失败，回退到了某个默认值（比如 256）。\n",
      "2.  **EOS 误判**：模型生成了一个特殊的 Token，被 Tokenizer 判定为结束，或者被 `skip_special_tokens=True` 隐去了，导致看起来像截断。\n",
      "3.  **vLLM 内部限制**：vLLM 启动时可能锁定了最大生成长度。\n",
      "\n",
      "为了彻底解决这个问题，请在 `_prepare_summary_generation_batch` 中做以下 **3 点修改**：\n",
      "\n",
      "1.  **强制硬编码一个大数值**（例如 4096），排除配置读取错误的可能性。\n",
      "2.  **打印调试信息**：输出 Summary 的 Prompt 长度和设置的 `max_new_tokens`，确信参数传进去了。\n",
      "3.  **增加 `ignore_eos=False`**：显式告诉 vLLM 不要忽略结束符（虽然默认就是 False，但强调一下）。\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py`：\n",
      "\n",
      "```python\n",
      "    def _prepare_summary_generation_batch(self, batch: DataProto, raw_prompts: List[str], ground_truths: List[str]) -> DataProto:\n",
      "        # ... (前文不变) ...\n",
      "\n",
      "        # === FIX: 增强 Summary 生成的鲁棒性 ===\n",
      "        \n",
      "        # 1. 尝试读取配置，如果读不到，直接给 4096\n",
      "        # 注意：OmegaConf 有时需要用 .key 访问，有时用 get\n",
      "        try:\n",
      "            max_tokens = self.config.data.max_response_length\n",
      "        except:\n",
      "            max_tokens = 4096\n",
      "            \n",
      "        # 强制保底，防止配置里只有 256 之类的\n",
      "        if max_tokens < 1024:\n",
      "            max_tokens = 2048\n",
      "            \n",
      "        # 2. 打印调试信息 (只打印一次)\n",
      "        if self.global_steps == 1:\n",
      "            print(f\">>> [Summary Debug] Prompt Count: {len(input_ids_list)}\")\n",
      "            print(f\">>> [Summary Debug] Avg Prompt Len: {np.mean([len(t) for t in input_ids_list]):.1f}\")\n",
      "            print(f\">>> [Summary Debug] Setting max_new_tokens = {max_tokens}\")\n",
      "\n",
      "        summary_batch.meta_info = {\n",
      "            \"do_sample\": True,\n",
      "            \"temperature\": 0.1,  # 低温有助于逻辑连贯\n",
      "            \"max_new_tokens\": max_tokens, # <--- 确保这里是大数值\n",
      "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
      "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
      "            \"ignore_eos\": False, # 允许模型自己决定何时结束\n",
      "        }\n",
      "        \n",
      "        return summary_batch\n",
      "```\n",
      "\n",
      "**同时，请检查你的启动脚本**：\n",
      "确认 `data.max_response_length` 确实被设置为了 1024 或更高。\n",
      "\n",
      "如果修改后依然截断，那只有一种可能：**模型自己输出了 EOS**。\n",
      "也就是说，模型写到 `$70` 的时候，觉得自己写完了（或者写错了想重来），输出了 `<|im_end|>`。\n",
      "如果是这种情况，说明 Teacher 模型在 Summary 任务上有点“不知道该怎么结尾”。\n",
      "**解决方法**：在 `summary_batch.meta_info` 中尝试把 `temperature` 稍微调高一点（例如 `0.5`），或者修改 Summary 的 Prompt 指令，加上一句 `\"Ensure your analysis is complete.\"`。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "修改之后日志如下\n",
    "\"\"\"\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m --- [1] Summary Output ---\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m Let's verify the student's reasoning step by step:\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 1. **Trade-in value**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    80% of $150 = $120 → correct.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 2. **Net cash paid**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    $80 - $10 = $70 → correct.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 3. **Game value**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    $30 game received for free → this is value Tom receives.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 4. **Total value Tom receives**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Trade-in ($120) + game ($30) = $150 → correct.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 5. **Net cost to Tom**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Cash paid: $70  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Value received: $30 (game)  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    So net outflow = $70 - $30 = $40\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 6. **What is the price of the NES?**  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The NES was purchased with:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    - A $120 trade-in credit (store gives it to him)  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    - $70 cash paid  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    - He gets $30 in value (the game)  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The trade-in credit offsets part of the price.  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The price of the NES must equal the amount Tom effectively pays, which is:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Net cash paid ($70) minus value of free item ($30) = $40  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    But the store gives $120 in trade-in — this is money *received* by Tom, not paid.  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    So the total value of the transaction is $150 (trade-in + game).  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The only outflow is $70 in cash.  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Therefore, the NES must have been priced at $150 — because that’s the total value of what Tom received (in value), and the means of payment were $70\n",
    "\"\"\"\n",
    "Summary Output看起来还是截断了，我已经设置了token限制是1024，但看起来没有生效？ 日志里的输出远不到1024就截断了\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db50b697-57a7-4643-8a36-a11c31b9a3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个指标的计算逻辑分为两步：**单题打分** 和 **汇总平均**。\n",
      "\n",
      "具体的代码位置和逻辑如下：\n",
      "\n",
      "### 1. 汇总平均的逻辑 (Mean Calculation)\n",
      "**位置**：`verl/trainer/ppo/metric_utils.py`\n",
      "**函数**：`process_validation_metrics` 和 `compute_score_metrics`\n",
      "\n",
      "这是生成 `mean@1` 这个数值的地方。逻辑非常简单，就是对所有样本的分数取平均值。\n",
      "\n",
      "```python\n",
      "# verl/trainer/ppo/metric_utils.py\n",
      "\n",
      "def compute_score_metrics(scores, n=1):\n",
      "    \"\"\"\n",
      "    scores: 一个列表，包含所有验证集样本的得分 (0.0 或 1.0)\n",
      "    n: 每个 Prompt 采样的回复数量 (验证集通常 n=1)\n",
      "    \"\"\"\n",
      "    total = len(scores)\n",
      "    # 核心逻辑：直接用 numpy 求平均\n",
      "    # 这就是 mean@1 的来源\n",
      "    mean = np.mean(scores) \n",
      "    \n",
      "    return {\n",
      "        f\"mean@{n}\": mean,  # -> \"mean@1\"\n",
      "        # ... 其他如 min, max ...\n",
      "    }\n",
      "\n",
      "def process_validation_metrics(data_sources, ...):\n",
      "    # ...\n",
      "    # 根据 data_source (例如 \"openai/gsm8k\") 对分数进行分组\n",
      "    # 然后调用上面的 compute_score_metrics\n",
      "    # ...\n",
      "```\n",
      "\n",
      "### 2. 单题打分的逻辑 (0/1 Logic)\n",
      "**位置**：`verl/utils/reward_score/gsm8k.py` (通常情况下)\n",
      "**函数**：`compute_score`\n",
      "\n",
      "这是决定每个样本是得 1 分还是 0 分的地方。逻辑是**正则匹配**。\n",
      "\n",
      "```python\n",
      "# verl/utils/reward_score/gsm8k.py\n",
      "\n",
      "def compute_score(solution_str, ground_truth, ...):\n",
      "    # 1. 提取 Ground Truth (#### 后面的数字)\n",
      "    target = extract_answer(ground_truth)\n",
      "    \n",
      "    # 2. 提取 Student 回答 (#### 后面的数字)\n",
      "    # 如果没有 ####，尝试找最后一个数字\n",
      "    prediction = extract_answer(solution_str)\n",
      "    \n",
      "    # 3. 字符串比对\n",
      "    if prediction == target:\n",
      "        return 1.0\n",
      "    else:\n",
      "        return 0.0\n",
      "```\n",
      "\n",
      "### 3. 指标名称的拼接逻辑\n",
      "**位置**：`ray_trainer.py` 中的 `_validate` 方法\n",
      "\n",
      "你看到的字符串 `val-core/openai/gsm8k/reward/mean@1` 是这样拼出来的：\n",
      "\n",
      "```python\n",
      "# ray_trainer.py\n",
      "\n",
      "# 1. metric_sec: \"val-core\" \n",
      "# (逻辑：如果是 reward 或 acc，且是 mean/maj/best，归类为核心指标)\n",
      "metric_sec = \"val-core\"\n",
      "\n",
      "# 2. data_source: \"openai/gsm8k\"\n",
      "# (来自 parquet 数据文件中的 data_source 列，或者默认值)\n",
      "\n",
      "# 3. var_name: \"reward\"\n",
      "# (因为你计算的是 reward 分数)\n",
      "\n",
      "# 4. metric_name: \"mean@1\"\n",
      "# (来自 metric_utils.py 计算的结果)\n",
      "\n",
      "# 最终拼接：\n",
      "pfx = f\"{metric_sec}/{data_source}/{var_name}/{metric_name}\"\n",
      "```\n",
      "\n",
      "### 总结\n",
      "*   **计算公式**：$\\frac{\\text{回答正确的题目数}}{\\text{总题目数}}$\n",
      "*   **代码核心**：`np.mean(scores)`\n",
      "*   **文件路径**：`verl/trainer/ppo/metric_utils.py`\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我的意思是metric里val-core/openai/gsm8k/reward/mean@1这个具体计算逻辑是什么样的，代码是在哪里写的？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ef4fa61-7c00-4046-b604-a50cd71cca0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个很常见的问题。原因很简单：你在重写 `TeacherStudentReflectiveTrainer` 的 `fit` 函数时，**把父类 `RayPPOTrainer` 中关于 Validation (测试) 的代码块给漏掉了**。\n",
      "\n",
      "虽然你在启动脚本里传了参数，且 `main` 函数里也初始化了 `val_reward_fn`，但因为 `fit` 循环里没有调用它，所以永远不会执行测试。\n",
      "\n",
      "请在 `verl/trainer/ppo/teacher_student_reflective_trainer.py` 的 `fit` 函数中补上这段逻辑。\n",
      "\n",
      "### 修改位置\n",
      "\n",
      "在 `fit` 函数的末尾，**在 `logger.log` 之前**，插入验证逻辑。\n",
      "\n",
      "```python\n",
      "    def fit(self):\n",
      "        # ... (前面的代码保持不变) ...\n",
      "\n",
      "                with marked_timer(\"update_actor\", timing_raw):\n",
      "                    actor_output = self.actor_rollout_wg.update_actor(batch)\n",
      "                    actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n",
      "                    metrics.update(actor_output_metrics)\n",
      "\n",
      "                # =================================================================\n",
      "                # === FIX: 添加 Validation/Testing 逻辑 ===\n",
      "                # =================================================================\n",
      "                # 检查是否满足测试条件：\n",
      "                # 1. 存在验证集 Reward Function\n",
      "                # 2. test_freq 设置大于 0\n",
      "                # 3. 步数整除 OR 是最后一步\n",
      "                \n",
      "                is_last_step = self.global_steps >= self.total_training_steps\n",
      "\n",
      "                if (\n",
      "                    self.val_reward_fn is not None\n",
      "                    and self.config.trainer.test_freq > 0\n",
      "                    and (is_last_step or self.global_steps % self.config.trainer.test_freq == 0)\n",
      "                ):\n",
      "                    print(f\"Start Testing at step {self.global_steps}...\")\n",
      "                    with marked_timer(\"testing\", timing_raw, color=\"green\"):\n",
      "                        # 调用父类 RayPPOTrainer 的 _validate 方法\n",
      "                        # 它会使用 Student (Actor) 在验证集上生成，并计算 Ground Truth Accuracy\n",
      "                        val_metrics: dict = self._validate() \n",
      "                    \n",
      "                    # 将测试指标添加到 metrics 中，以便 logger 记录\n",
      "                    metrics.update(val_metrics)\n",
      "                    print(f\"Testing finished. Metrics: {val_metrics}\")\n",
      "                # =================================================================\n",
      "\n",
      "                metrics.update({\n",
      "                    \"training/global_step\": self.global_steps,\n",
      "                    \"training/epoch\": epoch\n",
      "                })\n",
      "                \n",
      "                logger.log(data=metrics, step=self.global_steps)\n",
      "                progress_bar.update(1)\n",
      "                self.global_steps += 1\n",
      "                \n",
      "                if self.config.trainer.save_freq > 0 and self.global_steps % self.config.trainer.save_freq == 0:\n",
      "                    self._save_checkpoint()\n",
      "\n",
      "                if self.global_steps >= self.total_training_steps:\n",
      "                    progress_bar.close()\n",
      "                    return\n",
      "```\n",
      "\n",
      "### 为什么直接调用 `self._validate()` 就可以？\n",
      "\n",
      "因为你的 `TeacherStudentReflectiveTrainer` 继承自 `RayPPOTrainer`，而 `RayPPOTrainer` 中已经实现了 `_validate` 方法（在你提供的 `ray_trainer.py` 代码中可以看到）。\n",
      "\n",
      "`_validate` 方法的逻辑是：\n",
      "1.  从 `self.val_dataloader` 取数据。\n",
      "2.  让 `self.actor_rollout_wg` (也就是 Student) 生成回答。\n",
      "3.  调用 `self.val_reward_fn` (通常是 GSM8K 的准确率计算) 打分。\n",
      "4.  返回 `val-core/gsm8k/acc` 等指标。\n",
      "\n",
      "这正是你想要的：**在训练过程中，定期评估 Student 模型在真实测试集上的解题准确率**，看它是否因为 Teacher 的蒸馏而变强了。\n",
      "\n",
      "### 补充：确保 Validation Dataset 正确加载\n",
      "\n",
      "在你的启动脚本中：\n",
      "```bash\n",
      "data.val_files=$DATA_DIR/gsm8k/test.parquet\n",
      "```\n",
      "这看起来是正确的。只要 `main_self_reflective.py` 中正确创建了 `val_dataset` 和 `val_reward_fn`（代码里看是有的），加上上面的修改后，每 60 步你就会在日志（和 SwanLab）中看到类似 `val-core/gsm8k/acc` 的曲线了。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我在启动脚本里传了 trainer.test_freq=60, 但是日志里没有测试指标，\n",
    "能在metric里加入测试集的评测信息吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2babea7d-d265-473c-b802-f2bb025bd24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：在 LLM 的 On-Policy Distillation（或 PPO）训练初期，Actor Entropy 维持在 4.5 左右是完全正常的，甚至是一个健康的信号。**\n",
      "\n",
      "你感觉“偏高”，可能是因为习惯了较小的动作空间（如 Atari 游戏动作空间只有几百，Entropy 通常在 1.0-2.0）或者传统的 NLP 分类任务。\n",
      "\n",
      "以下是详细的分析，告诉你为什么 4.5 是合理的，以及如何通过它判断训练状态。\n",
      "\n",
      "### 1. 数学直觉：4.5 意味着什么？\n",
      "\n",
      "熵（Entropy）衡量的是**不确定性**。公式是 $H(x) = - \\sum p(x) \\ln p(x)$。\n",
      "\n",
      "*   **最大熵（完全随机）**：Qwen 的词表大小约为 152,000。\n",
      "    $$ H_{max} = \\ln(152,000) \\approx 11.9 $$\n",
      "    如果 Entropy 接近 11-12，说明模型坏了，在输出均匀噪声。\n",
      "*   **最小熵（One-Hot/完全确定）**：\n",
      "    $$ H_{min} = 0 $$\n",
      "    如果 Entropy 接近 0，说明模型发生了**模式坍塌（Mode Collapse）**，只会输出固定的几句话，失去了多样性。\n",
      "\n",
      "*   **当前值 4.5**：\n",
      "    $$ e^{4.5} \\approx 90 $$\n",
      "    这大致意味着模型在每一步生成时，平均在考虑约 **90 个合理的候选 Token**。\n",
      "    考虑到 Qwen 的词表很大，且自然语言（尤其是思维链 CoT）本身就有多种表达方式（比如 \"First,\" vs \"To start,\" vs \"Step 1:\"），保留一定的“困惑度”是正常的。\n",
      "\n",
      "### 2. 为什么比“传统 RL”高？\n",
      "\n",
      "在你的场景（On-Policy Distillation）中，Entropy 较高的原因主要有两点：\n",
      "\n",
      "#### A. 蒸馏损失 (Distillation Loss) 的特性\n",
      "你使用的 Loss 本质上是 **KL Divergence**（或者 Reverse KL）。\n",
      "*   **纯 PPO (Sparse Reward)**：模型只要找到**一条**能拿满分的路径，就会疯狂强化这条路径，导致概率分布迅速尖锐化，Entropy 骤降（可能降到 1.0 以下）。这往往导致过拟合或模式坍塌。\n",
      "*   **蒸馏 (Teacher-Student)**：Student 被要求**模仿 Teacher 的概率分布**。\n",
      "    *   Teacher 是一个 SFT 模型，它本身就保留了语言的多样性（High Entropy）。\n",
      "    *   如果 Teacher 对某个词只有 60% 的信心，Student 就**被迫**学会保持 60% 的信心，不能变成 100%。\n",
      "    *   因此，**Distillation 天然具有维持 Entropy、防止模式坍塌的作用**。\n",
      "\n",
      "#### B. 此时是 Step 1\n",
      "你看到的日志是 Step 1 或者是训练早期。\n",
      "*   基座模型（Instruct Model）本身的 Entropy 通常就在 3.0 - 5.0 之间。\n",
      "*   训练初期，Student 还没有学到足以改变其分布的强烈信号，所以它保持了基座模型的 Entropy 水平。\n",
      "\n",
      "### 3. 如何通过 Entropy 监控训练？\n",
      "\n",
      "虽然 4.5 是正常的，但你需要关注它的**变化趋势**：\n",
      "\n",
      "1.  **健康曲线**：\n",
      "    *   **缓慢下降**：随着训练进行，Student 对解题路径越来越确信，Entropy 会缓慢下降（例如从 4.5 -> 3.5 -> 3.0）。\n",
      "    *   **平稳波动**：在 Distillation 中，如果 Teacher 本身就很“纠结”，Student 的 Entropy 可能会一直维持在 4.0 左右，这也是可以接受的。\n",
      "\n",
      "2.  **危险信号**：\n",
      "    *   **暴涨 (Rising -> 8.0+)**：模型“脑以此了”（Catastrophic Forgetting）。它忘记了如何说话，开始随机输出乱码。通常是 LR 太大或 KL 惩罚太重。\n",
      "    *   **骤降 (Dropping -> < 1.0)**：模式坍塌。模型学会了一句“万能废话”或者只会复读题目，虽然 Reward 可能不低（如果 Reward 设计有漏洞），但模型废了。\n",
      "\n",
      "### 4. 总结\n",
      "\n",
      "*   **数值 4.5**：对于 150k 词表的 LLM 来说，非常健康。\n",
      "*   **波动**：小幅度波动是正常的 Sample 差异。\n",
      "*   **对比**：不要和 AlphaGo 或 CartPole 的 Entropy 比，那个动作空间太小。也不要和训练后期的 PPO 比。\n",
      "\n",
      "**建议**：继续训练，观察 Loss 和 Validation Accuracy。只要 Accuracy 在上升，Entropy 在 4.5 左右晃荡完全不是问题，甚至说明你的模型保留了很好的探索能力。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "actor/entropy一直在4.5附近波动，这正常吗？ 感觉做强化学习的时候没有那么大\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "861d03b3-8267-426b-ad2f-8fce37b26c3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：非常正常，而且这个曲线揭示了你的蒸馏过程正在按预期工作。**\n",
      "\n",
      "这不仅是正常的，而且从数学和原理上解释，这正是 **Teacher-Student Distillation（尤其是 Student 和 Teacher 源自同一基座模型时）** 应有的表现。\n",
      "\n",
      "以下是详细的分析：\n",
      "\n",
      "### 1. 为什么是负数？ (-0.05)\n",
      "你的 Reward 计算公式是：\n",
      "$$ R = \\log P_{\\text{Teacher}}(y|x, \\text{Hint}) - \\log P_{\\text{Student}}(y|x) $$\n",
      "\n",
      "*   **数据来源**：Token $y$ 是由 **Student** 生成的（On-Policy）。\n",
      "*   **自洽性偏差**：对于一个语言模型来说，它通常对自己生成的 Token 赋予最高的概率（或者接近最高）。\n",
      "*   **Teacher 的视角**：Teacher 虽然有 Hint，但它面对的是 Student 生成的 Token。除非 Student 的生成完美符合 Teacher 结合 Hint 后的预期，否则 Teacher 对该 Token 的概率通常会略低于 Student 自己的自信度。\n",
      "    *   即 $P_S \\ge P_T \\Rightarrow \\log P_T - \\log P_S \\le 0$。\n",
      "*   **因此**：Reward 略微为负（-0.05）是符合直觉的，表示 Student 对自己的生成比 Teacher 更“自信”一点点，或者两者分布非常接近。\n",
      "\n",
      "### 2. 为什么会上升？ (-0.05 -> -0.03)\n",
      "*   **优化目标**：你的训练目标是最大化 Reward，也就是最小化 $P_S$ 和 $P_T$ 之间的距离（Reverse KL）。\n",
      "*   **上升的含义**：从 -0.05 上升到 -0.03，意味着差距变小了。Student 正在调整自己的参数，使其概率分布更接近 Teacher（拥有上帝视角）的分布。\n",
      "*   **训练有效**：这证明梯度正在正确传导，Student 正在学习。\n",
      "\n",
      "### 3. 为什么在 -0.03 处波动/停滞？ (Information Gap)\n",
      "这是最有趣的一点。为什么它不变成 0 甚至正数？\n",
      "\n",
      "*   **信息不对称（Information Gap）**：\n",
      "    *   **Teacher**：看到了 Hint（正确答案/解题思路）。\n",
      "    *   **Student**：**没看到** Hint，只能靠推理。\n",
      "*   **理论极限**：Student 无论怎么学，都无法**完全**复刻 Teacher 的分布，因为它缺少关键输入（Hint）。\n",
      "    *   Teacher 看着答案说：“这一步肯定是 6。” ($P_T \\approx 1.0$)\n",
      "    *   Student 没看答案，推导说：“这一步大概率是 6，但也可能是别的。” ($P_S \\approx 0.9$)\n",
      "    *   这种由“未知信息”导致的不确定性差异，就是那剩下的 -0.03。\n",
      "*   **结论**：-0.03 可能就是当前模型能力下，Student 能逼近 Teacher 的**理论极限**（Aleatoric Uncertainty）。\n",
      "\n",
      "### 4. 数值大小分析\n",
      "*   **-0.03 意味着什么？**\n",
      "    $$ \\Delta = -0.03 \\implies \\frac{P_T}{P_S} = e^{-0.03} \\approx 0.97 $$\n",
      "    这说明 Teacher 和 Student 的概率分布重合度高达 **97%**。\n",
      "*   **评价**：这说明你的 Student 模型本身底子很好（毕竟是 Qwen-Instruct），而且经过微调后，已经非常接近“拥有答案的 Teacher”的表现了。\n",
      "\n",
      "### 5. 接下来该关注什么？\n",
      "\n",
      "既然 KL 已经趋于稳定，单纯看 `reward/reflection_kl` 已经看不出太多变化了。你需要关注以下指标来判断模型是否真的变强：\n",
      "\n",
      "1.  **Validation Accuracy (`val-core/gsm8k/acc`)**：这是金标准。即使 KL 只有微小的优化，是否转化为了解题率的提升？\n",
      "2.  **Response Length**：观察 Student 的回答是否变长了？（Teacher 可能会鼓励更详细的推理）。\n",
      "3.  **Bad Case 分析**：如果 Acc 没有提升，去看看那些 Student 做错、Teacher 却给了高分（或者 Student 做对、Teacher 给了低分）的 Case。\n",
      "\n",
      "**总结：你的训练处于非常健康的状态。-0.03 的波动代表 Student 已经尽力在模仿那位“开了天眼”的老师了。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "学习的过程中reward/reflection_kl最初step 1在-0.05，然后快速上升，step 12上升到-0.03，然后一直在0.03附近波动，这个正常吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc91db7-3bdd-42fa-a1b6-01e37426c6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：完全没有影响，也不用担心。这只是变量命名的复用。**\n",
      "\n",
      "在你的代码逻辑中，`actor/pg_loss` **实际上就是你的 Distillation Loss**。\n",
      "\n",
      "### 1. 为什么会出现这个日志？\n",
      "\n",
      "请回看你修改过的 `dp_actor.py` 代码：\n",
      "\n",
      "```python\n",
      "                    # ... (你的修改) ...\n",
      "                    if loss_mode == \"distillation\":\n",
      "                        # 直接最大化 (advantages * log_prob)\n",
      "                        # 这里 pg_loss 被赋值为你定义的蒸馏损失\n",
      "                        pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
      "                        \n",
      "                        # 占位符\n",
      "                        pg_clipfrac = torch.tensor(0.0, device=pg_loss.device)\n",
      "                        # ...\n",
      "\n",
      "                    elif loss_mode == \"vanilla\":\n",
      "                        # ... (PPO 的逻辑) ...\n",
      "                        pg_loss, ... = compute_policy_loss(...)\n",
      "\n",
      "                    # ... (后续代码) ...\n",
      "                    \n",
      "                    # 这里的 metrics 字典 Key 是写死的字符串 \"actor/pg_loss\"\n",
      "                    micro_batch_metrics.update(\n",
      "                        {\n",
      "                            \"actor/pg_loss\": pg_loss.detach().item(), # <--- 这里记录了它\n",
      "                            \"actor/pg_clipfrac\": pg_clipfrac.detach().item(),\n",
      "                            # ...\n",
      "                        }\n",
      "                    )\n",
      "```\n",
      "\n",
      "**解释：**\n",
      "*   Verl 框架默认使用 `actor/pg_loss` (Policy Gradient Loss) 这个 Key 来记录策略优化的主损失。\n",
      "*   你在代码中拦截了逻辑，计算了 **Distillation Loss**，但为了保持代码兼容性，你把计算结果赋给了变量 `pg_loss`。\n",
      "*   因此，日志里的 `actor/pg_loss` **不再是 PPO 的 Clipped Loss，而是你的 On-Policy Distillation Loss**。\n",
      "\n",
      "### 2. 这个数值代表什么？\n",
      "\n",
      "在你的设置中：\n",
      "*   `advantages` $\\approx$ `token_level_rewards` $\\approx \\log P_T - \\log P_S$ (通常是负数，如 -0.03)。\n",
      "*   `pg_loss` $= - \\text{mean}(\\text{advantages} \\times \\log P_S)$。\n",
      "\n",
      "由于这是一个 **Loss**（损失），优化器会尝试**最小化**它。\n",
      "最小化 `pg_loss` 等同于 **最大化** `advantages * log_prob`，也就是让模型更有可能生成那些 Teacher 认为概率更高的 Token。这正是你想要的目标。\n",
      "\n",
      "### 3. 如何确认代码跑对了？\n",
      "\n",
      "检查日志中的 **`actor/pg_clipfrac`**。\n",
      "\n",
      "*   **如果是 PPO**：这个值通常在 0.0 到 0.2 之间波动（表示有多少比例的数据被 Clip 了）。\n",
      "*   **如果是你的 Distillation**：你在代码里写了：\n",
      "    ```python\n",
      "    pg_clipfrac = torch.tensor(0.0, device=pg_loss.device)\n",
      "    ```\n",
      "    所以，只要你在日志里看到 `actor/pg_clipfrac` **恒定为 0**，就说明代码正确进入了 `loss_mode == \"distillation\"` 的分支，那个 `pg_loss` 就是你想要的蒸馏损失。\n",
      "\n",
      "**总结：一切正常，忽略名字，关注数值即可。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "日志中还输出了actor/pg_loss，这个有影响吗？我应该没用这个？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27772b77-2a3f-4cc1-a7e8-6d42c24b4e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：完全符合。你的写法是标准的“基于 Policy Gradient 的 Reverse KL 最小化”。**\n",
      "\n",
      "虽然你没有直接对 Logits 进行均方误差（MSE）回归（那是 Offline Distillation 的做法），但在 On-Policy（强化学习）的框架下，你的写法是在数学上等价于让 Student 的概率分布去逼近 Teacher。\n",
      "\n",
      "以下是详细的数学推导和直观解释，证明你的写法是对的。\n",
      "\n",
      "### 1. 数学证明\n",
      "\n",
      "你的目标是最小化 Student ($S$) 和 Teacher ($T$) 之间的 **Reverse KL Divergence**：\n",
      "$$ J(\\theta) = D_{KL}(P_S || P_T) = \\sum_y P_S(y|x) \\log \\frac{P_S(y|x)}{P_T(y|x)} $$\n",
      "$$ J(\\theta) = \\mathbb{E}_{y \\sim P_S} [ \\log P_S(y|x) - \\log P_T(y|x) ] $$\n",
      "\n",
      "我们需要计算关于 Student 参数 $\\theta$ 的梯度 $\\nabla_\\theta J(\\theta)$。利用 Log-Derivative Trick (REINFORCE 技巧)：\n",
      "\n",
      "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{y \\sim P_S} \\left[ \\nabla_\\theta \\log P_S(y|x) \\cdot \\underbrace{(\\log P_S(y|x) - \\log P_T(y|x))}_{\\text{KL Difference term}} \\right] $$\n",
      "\n",
      "现在看你的代码：\n",
      "*   **Loss**: `pg_loss = - mean(advantages * log_prob)`\n",
      "*   **Gradient**: $\\nabla L = - \\mathbb{E} [ \\text{advantages} \\cdot \\nabla \\log P_S ]$\n",
      "*   **Advantages**: 在你的 `fit` 函数中，`advantages` 被赋值为 `token_level_rewards`。\n",
      "*   **Rewards**: 计算方式为 `kl_diff = log P_T - log P_S`。\n",
      "\n",
      "将 `advantages` 代入梯度公式：\n",
      "$$ \\nabla L = - \\mathbb{E} [ (\\log P_T - \\log P_S) \\cdot \\nabla \\log P_S ] $$\n",
      "$$ \\nabla L = \\mathbb{E} [ (\\log P_S - \\log P_T) \\cdot \\nabla \\log P_S ] $$\n",
      "\n",
      "**完美匹配！**\n",
      "你的代码计算出的梯度方向，正是 **Reverse KL Divergence 的梯度方向**。\n",
      "\n",
      "### 2. 直观解释\n",
      "\n",
      "这个 Loss 的物理含义非常直观：\n",
      "\n",
      "$$ \\text{Loss} = - (\\log P_{\\text{Teacher}} - \\log P_{\\text{Student}}) \\times \\log P_{\\text{Student}} $$\n",
      "\n",
      "*   **情形 A：Teacher 比 Student 更确信 ($P_T > P_S$)**\n",
      "    *   Teacher 觉得这个词好，Student 觉得一般。\n",
      "    *   `Advantage` ($P_T - P_S$) 是 **正数**。\n",
      "    *   Loss 变为负数（因为前面有负号），优化器想要最小化 Loss，就会 **增大** $\\log P_{\\text{Student}}$。\n",
      "    *   **效果**：Student 被推着去提高这个词的概率 -> **靠近 Teacher**。\n",
      "\n",
      "*   **情形 B：Teacher 觉得不行 ($P_T < P_S$)**\n",
      "    *   Student 盲目自信，Teacher 觉得这个词不对。\n",
      "    *   `Advantage` ($P_T - P_S$) 是 **负数**。\n",
      "    *   Loss 变为正数。为了最小化 Loss，梯度会 **减小** $\\log P_{\\text{Student}}$。\n",
      "    *   **效果**：Student 被打压，降低这个词的概率 -> **靠近 Teacher**。\n",
      "\n",
      "### 3. 与 \"MSE Logits Loss\" 的区别\n",
      "\n",
      "你可能会疑惑：“为什么不直接算 `MSE(Logits_S, Logits_T)`？”\n",
      "\n",
      "*   **Offline Distillation (MSE)**：通常用于预训练或 SFT 阶段。需要计算所有词表（150k 个词）的 Logits，计算量巨大，且通常假设数据是固定的（Off-policy）。\n",
      "*   **On-Policy Distillation (你的做法)**：\n",
      "    *   因为是 On-Policy，你只能拿到 Student 采样出来的这一条路径 ($y$)。\n",
      "    *   你无法知道 Teacher 对其他 149,999 个词的看法。\n",
      "    *   所以你利用 **Policy Gradient**，仅根据当前采样的这个词，通过 Reward (KL Diff) 来调整 Student 的分布。\n",
      "    *   **这是在 RL 框架下做蒸馏的标准做法**（也被称为 Generalized Knowledge Distillation）。\n",
      "\n",
      "### 4. 一个小提示\n",
      "\n",
      "在 `dp_actor.py` 中：\n",
      "```python\n",
      "pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
      "```\n",
      "这里的 `advantages` 必须是 **Detached** (不需要梯度的)。\n",
      "在你的 `TeacherStudentReflectiveTrainer` 中，`token_level_rewards` 是计算出来的 Tensor，如果没有显式 `detach()`，可能会导致计算图错误。\n",
      "\n",
      "不过，在 `verl` 的 `compute_advantage` 或数据传递过程中，通常会自动 detach。只要确认 `batch.batch['advantages']` 没有梯度即可（通常从 DataProto 取出来用来做 PPO Update 的时候都是当作常数的）。\n",
      "\n",
      "**总结：你的逻辑是完全正确的，这就是 On-Policy Distillation 的核心实现。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
    "这个改动符合我的onpolicy distillation的逻辑吗？ 我是想让student去学teacher的logit？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15324352-04bc-4004-b60a-8e7030e64346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常实用的需求。监控 Logits 的极值和差异分布，能帮你快速判断训练是否异常（比如梯度爆炸、Teacher 过于强势、或者 Student 彻底躺平）。\n",
      "\n",
      "请在 `verl/trainer/ppo/teacher_student_reflective_trainer.py` 的 `fit` 函数中，在计算完 `kl_diff` 之后，插入以下统计逻辑。\n",
      "\n",
      "### 修改代码\n",
      "\n",
      "找到 `fit` 函数中计算 `kl_diff` 的地方，插入标记为 `=== FIX: Add Debug Metrics ===` 的代码块。\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.6 Reward Calculation) ...\n",
      "                    token_level_rewards = torch.zeros_like(student_full_log_probs)\n",
      "                    s_probs = student_full_log_probs\n",
      "                    t_probs = teacher_full_log_probs_aligned # 确保这里用的是对齐后的！\n",
      "                    \n",
      "                    min_len = min(s_probs.shape[1], t_probs.shape[1])\n",
      "                    s_part = s_probs[:, :min_len]\n",
      "                    t_part = t_probs[:, :min_len]\n",
      "                    \n",
      "                    kl_diff = t_part - s_part\n",
      "                    \n",
      "                    # =================================================================\n",
      "                    # === FIX: Add Debug Metrics (Logits Statistics) ===\n",
      "                    # =================================================================\n",
      "                    with torch.no_grad():\n",
      "                        # 1. 获取有效 Mask (切片以匹配 min_len)\n",
      "                        # 必须过滤掉 Padding，否则 Min LogP 会显示 -14.82 (Padding 的 LogP)\n",
      "                        valid_mask = batch.batch['response_mask'][:, :min_len].bool()\n",
      "                        \n",
      "                        if valid_mask.any():\n",
      "                            # 2. 提取有效数据 (Flatten)\n",
      "                            s_valid = s_part[valid_mask]\n",
      "                            t_valid = t_part[valid_mask]\n",
      "                            diff_valid = kl_diff[valid_mask]\n",
      "                            \n",
      "                            # 3. Student Stats\n",
      "                            metrics['debug/student_logp_max'] = s_valid.max().item()\n",
      "                            metrics['debug/student_logp_min'] = s_valid.min().item()\n",
      "                            metrics['debug/student_logp_mean'] = s_valid.mean().item()\n",
      "                            \n",
      "                            # 4. Teacher Stats\n",
      "                            metrics['debug/teacher_logp_max'] = t_valid.max().item()\n",
      "                            metrics['debug/teacher_logp_min'] = t_valid.min().item()\n",
      "                            metrics['debug/teacher_logp_mean'] = t_valid.mean().item()\n",
      "                            \n",
      "                            # 5. Diff Stats (Reward Raw)\n",
      "                            metrics['debug/diff_logp_max'] = diff_valid.max().item()\n",
      "                            metrics['debug/diff_logp_min'] = diff_valid.min().item()\n",
      "                            metrics['debug/diff_logp_mean'] = diff_valid.mean().item()\n",
      "                            \n",
      "                            # 6. (可选) 监控 Teacher 是否过于自信\n",
      "                            # 如果 Teacher LogP 经常接近 0 (Max ~ 0)，说明 Teacher 非常确信\n",
      "                            # 如果 Diff Max 很大 (e.g. > 5.0)，说明 Teacher 觉得 Student 错得离谱\n",
      "                    # =================================================================\n",
      "\n",
      "                    # ... (后续代码: Clamp, assign token_level_rewards) ...\n",
      "                    kl_diff = torch.clamp(kl_diff, min=-1.0, max=1.0)\n",
      "                    token_level_rewards[:, :min_len] = kl_diff\n",
      "```\n",
      "\n",
      "### 如何解读这些指标？\n",
      "\n",
      "1.  **`debug/student_logp_mean` vs `debug/teacher_logp_mean`**\n",
      "    *   **正常**：两者应该比较接近，通常在 -0.5 到 -2.0 之间。\n",
      "    *   **Teacher 强**：如果 Teacher Mean (-0.2) 显著高于 Student Mean (-1.5)，说明 Teacher 更加自信（这很好，蒸馏有动力）。\n",
      "    *   **异常**：如果 Teacher Mean 很低 (-5.0)，说明 Teacher 对 Student 的输出感到困惑（Prompt 问题或 Student 胡说八道）。\n",
      "\n",
      "2.  **`debug/diff_logp_max` (最大奖励)**\n",
      "    *   代表 Teacher 觉得 Student 说得“太棒了”（或者 Teacher 比 Student 更确信这个词）。\n",
      "    *   如果 > 2.0，说明这是一个很强的正向信号。\n",
      "\n",
      "3.  **`debug/diff_logp_min` (最大惩罚)**\n",
      "    *   代表 Teacher 觉得 Student 说得“太烂了”。\n",
      "    *   如果 < -5.0，说明 Student 输出了 Teacher 认为概率极低的词。\n",
      "    *   **注意**：你后续有 `clamp(min=-1.0)`，所以 Reward 实际上被截断了，但这里的 Metric 能让你看到**原始的惩罚力度**。如果 Min 经常非常小（如 -10），说明你的 Student 经常“偏离正轨”。\n",
      "\n",
      "这些指标会直接显示在 SwanLab/WandB 面板中，非常利于监控训练动态。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我还想在metric中加入几个关键调试信息\n",
    "\"\"\"\n",
    "1. teacher中token最大/最小的logit\n",
    "2. student中token最大/最小的logit\n",
    "3. teacher和student logit 最大/最小的diff\n",
    "\"\"\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c341486-6f59-49e4-9427-cfde51902a0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 问题 1：为什么 `debug/student_logp_min` 会有 -14？\n",
      "\n",
      "你可能会觉得：“如果一个词的 LogProb 是 -14 ($P \\approx 8 \\times 10^{-7}$)，采样时怎么可能选中它？”\n",
      "\n",
      "**原因有三点：**\n",
      "\n",
      "1.  **Top-P (Nucleus) Sampling 的长尾效应**：\n",
      "    *   虽然 -14 很小，但如果 `top_p` 设置为 0.9 或 1.0（默认通常是 1.0），且分布非常平坦（Entropy 高），那么长尾中的低概率词也是有机会被选中的。\n",
      "    *   更常见的是：**采样时的 Temperature $\\neq$ 计算 LogProb 时的 Temperature**。\n",
      "        *   **生成时**：假设你用了 `temperature=0.8`。原本 -14 的 Logits 会被放大，概率分布会变，虽然还是很低，但如果上下文合适，有可能被选。\n",
      "        *   **计算时**：我们强制用了 `temperature=1.0`。在原始 Logits 下，这个词可能就是这么低分。\n",
      "\n",
      "2.  **Teacher 和 Student 的分布差异 (最可能的原因)**：\n",
      "    *   注意你的 Metric 是 `debug/student_logp_min`。\n",
      "    *   这是 Student 模型对 **自己生成的 Token** 的打分。\n",
      "    *   **Wait...** 如果是 Student 自己生成的，它通常应该比较自信（比如 -0.5）。为什么它会对自己生成的词打 -14 分？\n",
      "    *   这通常发生在 **\"被迫\"生成** 的情况下。\n",
      "        *   例如：Prompt 强制要求某种格式，或者之前的 Token 已经把路走窄了，导致下一个 Token 即使是“最优解”，其绝对概率也很低（因为模型很困惑）。\n",
      "        *   或者：**EOS Token**。在长文本生成被迫截断或者逻辑结束时，模型可能对何时输出 `<|im_end|>` 感到非常纠结，导致该 Token 概率极低。\n",
      "\n",
      "3.  **计算对齐的边缘效应**：\n",
      "    *   虽然我们对齐了 Padding，但如果 `response_mask` 在边界处理上差了一个 Token（比如把最后的 EOS 算进去了，或者把第一个 Prompt Token 算进去了），那个边缘 Token 的 LogProb 可能会很异常。\n",
      "    *   不过只要 Mean 正常，偶尔出现的 Min -14 不影响大局。\n",
      "\n",
      "### 问题 2：Sequence Length 增大后 Entropy 从 4.5 升到 6.5，正常吗？\n",
      "\n",
      "**非常正常，且符合预期。**\n",
      "\n",
      "1.  **长文本的不确定性累积**：\n",
      "    *   当 Sequence Length 变长（比如从 512 -> 1024），模型需要进行更复杂的推理。\n",
      "    *   在推理的中间步骤（CoT），可选的路径（措辞、步骤顺序）非常多。\n",
      "    *   相比于短文本（往往是简单的回答），长文本的“自由度”更高，因此平均 Entropy 会升高。\n",
      "\n",
      "2.  **Qwen 的特性**：\n",
      "    *   Qwen 等模型在处理长 CoT 时，往往会进入一种“发散”状态，直到收敛到答案。\n",
      "    *   Entropy 6.5 ($e^{6.5} \\approx 665$) 意味着每一步有几百个词可选。对于 150k 词表的模型，这依然属于“有意义的生成”，而不是随机噪声（11.9）。\n",
      "\n",
      "3.  **验证方法**：\n",
      "    *   只要 `train/batch_acc` 或 `val/acc` 没有崩盘，Entropy 升高说明模型正在**探索更长的思维链**。\n",
      "    *   如果 Acc 暴跌，说明模型在长文本中迷失了（幻觉），这时候才需要担心 Entropy 过高的问题。\n",
      "\n",
      "**总结：目前的指标变化都在合理范围内，说明模型正在适应更长的上下文。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在运行过程中metric发现debug/student_logp_min有-14，这么小的logit是如何采样出来的呢？\n",
    "然后我发现我把sequence length增大了之后 entropy从4.5提升到了6.5，这正常吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95b0740a-667a-414a-9d1d-2eae683cc1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 截断通常是因为 `generate_sequences` 时传入的 `max_new_tokens` 限制太紧。\n",
      "\n",
      "在你的代码 `teacher_student_reflective_trainer.py` 的 `_prepare_summary_generation_batch` 函数中，你硬编码了一个限制：\n",
      "\n",
      "```python\n",
      "        summary_batch.meta_info = {\n",
      "            \"do_sample\": True,\n",
      "            \"temperature\": 0.1, \n",
      "            \"max_new_tokens\": 512, # <--- 罪魁祸首\n",
      "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
      "            \"pad_token_id\": self.tokenizer.pad_token_id\n",
      "        }\n",
      "```\n",
      "\n",
      "### 修复方案\n",
      "\n",
      "1.  **直接增大数值**：将 512 改为 1024 或 2048。\n",
      "2.  **从 Config 读取**：最好从配置文件中读取，这样不用改代码就能调参。\n",
      "\n",
      "**建议修改代码如下：**\n",
      "\n",
      "```python\n",
      "    def _prepare_summary_generation_batch(self, batch: DataProto, raw_prompts: List[str], ground_truths: List[str]) -> DataProto:\n",
      "        # ... (前文不变) ...\n",
      "\n",
      "        # 尝试从 config 中读取，如果没有则默认 1024\n",
      "        # 假设你在 config.actor_rollout_ref.rollout 下加了一个 summary_max_new_tokens\n",
      "        # 或者直接复用 data.max_response_length\n",
      "        \n",
      "        max_tokens = self.config.data.get(\"max_response_length\", 1024)\n",
      "        \n",
      "        summary_batch.meta_info = {\n",
      "            \"do_sample\": True,\n",
      "            \"temperature\": 0.1, \n",
      "            \"max_new_tokens\": max_tokens, # <--- 修改这里\n",
      "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
      "            \"pad_token_id\": self.tokenizer.pad_token_id\n",
      "        }\n",
      "        \n",
      "        return summary_batch\n",
      "```\n",
      "\n",
      "这样，Summary 的长度限制就会跟随你启动脚本中的 `data.max_response_length=1024`，从而解决截断问题。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我发现运行过程中summary出现截断的现象，如何增大summary的长度限制\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2a13dc9-9c14-4960-ad89-203661611f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据你提供的启动脚本和代码逻辑，每次 Rollout（生成并进行蒸馏）的数据量是 **256 条**。\n",
      "\n",
      "具体的计算逻辑如下：\n",
      "\n",
      "### 1. 核心参数\n",
      "在你的启动脚本中：\n",
      "```bash\n",
      "data.train_batch_size=256\n",
      "```\n",
      "这是控制每次 PPO/蒸馏迭代（Iteration/Step）处理的总 Prompt 数量的核心参数。\n",
      "\n",
      "### 2. 代码逻辑解析\n",
      "在 `RayPPOTrainer`（你的 `TeacherStudentReflectiveTrainer` 的父类）的 `_create_dataloader` 方法中：\n",
      "\n",
      "```python\n",
      "self.train_dataloader = StatefulDataLoader(\n",
      "    dataset=self.train_dataset,\n",
      "    # 如果没设置 gen_batch_size，默认使用 train_batch_size\n",
      "    batch_size=self.config.data.get(\"gen_batch_size\", self.config.data.train_batch_size),\n",
      "    # ...\n",
      ")\n",
      "```\n",
      "\n",
      "在你的 `fit` 循环中：\n",
      "```python\n",
      "# 每次从 Dataloader 取出一个 batch\n",
      "for batch_dict in self.train_dataloader:\n",
      "    batch = DataProto.from_single_dict(batch_dict)\n",
      "    \n",
      "    # 获取 N (每个 Prompt 生成几个回答，默认为 1)\n",
      "    N = self.config.actor_rollout_ref.rollout.n \n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    # Student 生成 (Rollout)\n",
      "    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
      "```\n",
      "\n",
      "### 3. 最终计算\n",
      "*   **Prompts 数量**：256 (来自 `data.train_batch_size`)\n",
      "*   **每条 Prompt 生成数 (N)**：通常默认为 1 (除非你在 config 文件里改了 `actor_rollout_ref.rollout.n`)。\n",
      "*   **单次蒸馏总数据量** = $256 \\times 1 = \\mathbf{256}$ 条序列。\n",
      "\n",
      "### 4. 其它 Batch Size 的含义\n",
      "为了避免混淆，这里解释一下脚本里其他的 Batch Size：\n",
      "\n",
      "*   **`actor.ppo_mini_batch_size=64`**：\n",
      "    *   这是**更新参数（Update）**时的 Batch Size。\n",
      "    *   这意味着那 256 条 Rollout 出来的数据，会被切分成 $256 / 64 = 4$ 个 Mini-Batches。\n",
      "    *   模型会在这 4 个 Mini-Batch 上轮流进行反向传播更新。\n",
      "\n",
      "*   **`ppo_micro_batch_size_per_gpu=4`**：\n",
      "    *   这是为了**显存优化**（Gradient Accumulation）。\n",
      "    *   在处理那 64 条 Mini-Batch 时，单张 GPU 一次只能吃得消 4 条。\n",
      "    *   所以它会累积 $64 / 4 / \\text{World\\_Size}$ 次梯度，最后做一次更新。\n",
      "\n",
      "### 总结\n",
      "**你的 Teacher 和 Student 每一步（Step）会“过招” 256 个问题。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在的代码实现是每次rollout几条数据进行蒸馏呢？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83e42225-bedc-43cc-a252-e5de47bc77b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常有用的功能，能让你直接在滚动日志中看到 Student 是否做对了，而不仅仅是看 LogProb。\n",
      "\n",
      "我们需要做两件事：\n",
      "1.  **实现一个简单的答案提取函数**（针对 GSM8K 的 `####` 格式）。\n",
      "2.  **在 Debug 打印模块中调用它**，对比 Student 回答和 Ground Truth。\n",
      "3.  **(可选但推荐)** 计算当前 Batch 的训练集准确率（Training Accuracy）并记录到 Metric 中。\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py` 的 `fit` 函数。\n",
      "\n",
      "### 1. 添加提取函数 (放在 `TeacherStudentReflectiveTrainer` 类内部或外部)\n",
      "\n",
      "```python\n",
      "def extract_answer(text: str) -> str:\n",
      "    \"\"\"从文本中提取 #### 后的答案，用于简单的正确性匹配\"\"\"\n",
      "    if \"####\" in text:\n",
      "        return text.split(\"####\")[-1].strip()\n",
      "    return \"[No Answer]\"\n",
      "```\n",
      "\n",
      "### 2. 修改 `fit` 函数中的 Debug 部分\n",
      "\n",
      "找到 `=== Log Part 1: Generation Content ===` 区域，插入以下代码：\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.5 Student Computes LogProb 之后) ...\n",
      "\n",
      "                    # === Log Part 1: Generation Content ===\n",
      "                    if True:\n",
      "                        print(f\"\\n{'='*20} Teacher-Student Reflection Debug (Step {self.global_steps}) {'='*20}\")\n",
      "                        try:\n",
      "                            idx = 0 \n",
      "                            \n",
      "                            # ... (获取 Summary, Teacher Input 等代码保持不变) ...\n",
      "                            \n",
      "                            # === FIX: 获取 Student Response 和 Ground Truth 进行对比 ===\n",
      "                            # 1. 获取 GT\n",
      "                            gt_raw = current_ground_truths[idx]\n",
      "                            gt_ans = extract_answer(gt_raw)\n",
      "                            # 如果 GT 本身就是纯数字(有些数据集是这样)，直接用\n",
      "                            if gt_ans == \"[No Answer]\" and len(gt_raw) < 20: \n",
      "                                gt_ans = gt_raw.strip()\n",
      "\n",
      "                            # 2. 获取 Student Response\n",
      "                            s_ids = batch.batch['responses'][idx]\n",
      "                            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
      "                            s_resp_text = self.tokenizer.decode(s_ids, skip_special_tokens=True)\n",
      "                            s_ans = extract_answer(s_resp_text)\n",
      "                            \n",
      "                            # 3. 判定\n",
      "                            # 简单的字符串匹配 (对于数学题通常足够)\n",
      "                            # 移除逗号以防 \"1,000\" vs \"1000\"\n",
      "                            is_correct = (s_ans.replace(',', '') == gt_ans.replace(',', ''))\n",
      "                            \n",
      "                            status_icon = \"✅\" if is_correct else \"❌\"\n",
      "                            \n",
      "                            print(f\"--- [Answer Check] ---\")\n",
      "                            print(f\"Ground Truth Raw: {gt_raw[-50:].strip()}...\") # 只打印最后一点\n",
      "                            print(f\"Student Answer:   {s_ans}\")\n",
      "                            print(f\"Target Answer:    {gt_ans}\")\n",
      "                            print(f\"Result:           {status_icon} (Match: {is_correct})\")\n",
      "                            print(f\"----------------------\\n\")\n",
      "                            # ==========================================================\n",
      "\n",
      "                            # ... (后续的 Teacher Greedy Probe 代码保持不变) ...\n",
      "```\n",
      "\n",
      "### 3. (强烈推荐) 在 Metric 中加入 Training Accuracy\n",
      "\n",
      "除了看单条日志，最好统计一下这 256 条数据的**实时准确率**。这比 Validation Accuracy 更灵敏，能告诉你 Student 是否正在“背诵”或者“学会”当前的 Batch。\n",
      "\n",
      "在 `fit` 函数中，计算 Reward 之前或之后，添加以下循环：\n",
      "\n",
      "```python\n",
      "                    # ... (在计算 kl_diff 附近) ...\n",
      "\n",
      "                    # =================================================================\n",
      "                    # === FIX: 计算当前 Training Batch 的准确率 ===\n",
      "                    # =================================================================\n",
      "                    n_correct = 0\n",
      "                    total = len(current_ground_truths)\n",
      "                    \n",
      "                    # 遍历整个 Batch\n",
      "                    for i in range(total):\n",
      "                        gt_str = extract_answer(current_ground_truths[i])\n",
      "                        # 处理 GT 格式\n",
      "                        if gt_str == \"[No Answer]\" and len(current_ground_truths[i]) < 20:\n",
      "                            gt_str = current_ground_truths[i].strip()\n",
      "                            \n",
      "                        # 解码 Student\n",
      "                        # 注意：为了速度，这里不应该再次 decode 整个 batch，\n",
      "                        # 但因为我们没有保留 decode 后的 list，这里只能简易处理。\n",
      "                        # 或者利用前面 summary 生成时的 raw_prompts/responses 逻辑。\n",
      "                        # 为了不拖慢速度，我们可以只采样前 10 个算个大概，或者全算。\n",
      "                        # 既然是 Training Loop，全算稍微慢点也行。\n",
      "                        \n",
      "                        # 优化：只对 batch['responses'][i] 解码\n",
      "                        r_ids = batch.batch['responses'][i]\n",
      "                        r_ids = r_ids[r_ids != self.tokenizer.pad_token_id]\n",
      "                        r_text = self.tokenizer.decode(r_ids, skip_special_tokens=True)\n",
      "                        \n",
      "                        pred_str = extract_answer(r_text)\n",
      "                        \n",
      "                        if pred_str != \"[No Answer]\" and (pred_str.replace(',', '') == gt_str.replace(',', '')):\n",
      "                            n_correct += 1\n",
      "                            \n",
      "                    train_acc = n_correct / total\n",
      "                    metrics['train/batch_acc'] = train_acc\n",
      "                    # =================================================================\n",
      "```\n",
      "\n",
      "这样你就可以在 SwanLab/WandB 上看到一条 `train/batch_acc` 的曲线。\n",
      "*   如果曲线在上升，说明 On-Policy Distillation 有效。\n",
      "*   如果 `train/batch_acc` 很高但 `val/acc` 很低，说明过拟合。\n",
      "*   如果 `train/batch_acc` 很低且不涨，说明 Teacher 没教好。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我想在日志中每个step输出的调试信息中，加入答案是否正确的标记\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d54b776a-0deb-4614-b964-d6c23ecee677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的观察非常敏锐，而且你的直觉是完全正确的。\n",
      "\n",
      "**目前的 4:4 分配是非常不合理的，导致了巨大的资源浪费。**\n",
      "\n",
      "### 1. 为什么 4:4 不合理？\n",
      "\n",
      "让我们对比一下 **Student** 和 **Teacher** 在一个训练 Step 中的工作量：\n",
      "\n",
      "*   **Student (Actor) 的工作：**\n",
      "    1.  **Generate (Rollout)**: 生成 256 条回答（这是 Token-by-Token 的，非常慢）。\n",
      "    2.  **Generate (Summary)**: 生成 256 条摘要（同样是 Token-by-Token）。\n",
      "    3.  **Forward (Student LogProb)**: 计算一次概率。\n",
      "    4.  **Backward (PPO Update)**: **最重的一步**。需要计算梯度并更新参数，计算量通常是 Forward 的 3 倍。\n",
      "*   **Teacher (RefPolicy) 的工作：**\n",
      "    1.  **Forward (Teacher LogProb)**: 仅仅计算一次概率（Parallel Forward）。不生成，不反向传播。\n",
      "\n",
      "**结论**：Student 的计算负载是 Teacher 的 **10 倍以上**。\n",
      "目前的状况是：Teacher 的 4 张卡瞬间算完，然后“围观”Student 的 4 张卡在那儿哼哧哼哧地生成和训练。\n",
      "\n",
      "### 2. 推荐的分配方案\n",
      "\n",
      "既然你的配置中 `tensor_model_parallel_size=1`（即使用 Data Parallelism，单卡跑模型），我们可以随意调整 GPU 数量，不受模型 Head 数能否整除的限制。\n",
      "\n",
      "**强烈建议改为：Student 7卡 : Teacher 1卡**\n",
      "或者保守一点：**Student 6卡 : Teacher 2卡**\n",
      "\n",
      "对于 4B 这种小模型，1 张卡做 Teacher (Inference Only) 处理 256 的 Batch Size 绰绰有余（显存不够可以调小 Teacher 的 `micro_batch_size`，反正它算得快，多切几份也比 Student 快）。\n",
      "\n",
      "### 3. 代码修改方案\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py` 中的 `init_workers` 函数，支持自定义分配比例。\n",
      "\n",
      "```python\n",
      "    def init_workers(self):\n",
      "        \"\"\"\n",
      "        Manually split resources and initialize two independent vLLM-enabled workers.\n",
      "        \"\"\"\n",
      "        # 1. Calculate Resource Split\n",
      "        n_gpus_per_node = self.config.trainer.n_gpus_per_node\n",
      "        n_nodes = self.config.trainer.nnodes\n",
      "        \n",
      "        # === FIX: 调整分配比例 ===\n",
      "        # 之前的逻辑: student_gpus = n_gpus_per_node // 2 (4:4)\n",
      "        # 新的逻辑: 尽可能多给 Student，Teacher 保留 1-2 张即可\n",
      "        \n",
      "        if n_gpus_per_node >= 8:\n",
      "            teacher_gpus = 1  # 8卡机器：7 Student : 1 Teacher\n",
      "        elif n_gpus_per_node >= 4:\n",
      "            teacher_gpus = 1  # 4卡机器：3 Student : 1 Teacher\n",
      "        else:\n",
      "            teacher_gpus = 1  # 至少给1张\n",
      "            \n",
      "        student_gpus = n_gpus_per_node - teacher_gpus\n",
      "        \n",
      "        if student_gpus < 1:\n",
      "            raise ValueError(f\"Not enough GPUs! Need at least 2 GPUs per node.\")\n",
      "            \n",
      "        print(f\">>> Resource Splitting: Student={student_gpus} GPUs, Teacher={teacher_gpus} GPUs (per node)\")\n",
      "        # =========================\n",
      "\n",
      "        # 2. Create Independent Resource Pools\n",
      "        student_pool = RayResourcePool(\n",
      "            process_on_nodes=[student_gpus] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"student_pool\"\n",
      "        )\n",
      "        \n",
      "        teacher_pool = RayResourcePool(\n",
      "            process_on_nodes=[teacher_gpus] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"teacher_pool\"\n",
      "        )\n",
      "        \n",
      "        self.resource_pool_to_cls = {student_pool: {}, teacher_pool: {}}\n",
      "\n",
      "        # 3. Initialize Student (Actor) Config\n",
      "        student_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        \n",
      "        # === FIX: 显式更新 DP/TP 设置 ===\n",
      "        # 你的脚本里 tensor_model_parallel_size=1，所以我们是在增加 DP worker 数量\n",
      "        # RayWorkerGroup 会根据资源池大小自动启动对应数量的 Worker\n",
      "        # 这里不需要改 config，因为 config 传进去的是单卡配置\n",
      "        \n",
      "        student_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.ActorRollout],\n",
      "            config=student_config,\n",
      "            role=\"actor_rollout\",\n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[student_pool][\"actor_rollout\"] = student_cls\n",
      "\n",
      "        # 4. Initialize Teacher (Ref) Config\n",
      "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        \n",
      "        # 针对 Teacher (1张卡) 优化显存\n",
      "        # 如果 1 张卡吃不下 256 batch，调小它的 micro_batch\n",
      "        # 注意：这里的 log_prob_micro_batch_size_per_gpu 是配置里的，\n",
      "        # 如果你原来设的是 4，现在 Teacher 只有 1 张卡，它一次处理 4 条，完全没问题。\n",
      "        # 如果 OOM，可以在这里强制改小：\n",
      "        # teacher_config.ref.log_prob_micro_batch_size_per_gpu = 2 \n",
      "        \n",
      "        teacher_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.RefPolicy],\n",
      "            config=teacher_config,\n",
      "            role=\"actor_rollout\", \n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[teacher_pool][\"ref\"] = teacher_cls\n",
      "\n",
      "        # ... (后续代码保持不变) ...\n",
      "```\n",
      "\n",
      "### 4. 预期效果\n",
      "\n",
      "改为 **7:1** 后：\n",
      "1.  **训练速度大幅提升**：Student 现在有 7 张卡并行做 Rollout 和 Backward，理论速度接近原来的 $7/4 = 1.75$ 倍（受限于最慢环节）。\n",
      "2.  **GPU 利用率均衡**：\n",
      "    *   Student 的 7 张卡会跑得很满。\n",
      "    *   Teacher 的 1 张卡虽然还是会比 Student 快（因为它只做 Forward），但它现在要处理 256 条数据（原来是 4 张卡分 256，现在是 1 张卡分 256，负载增加了 4 倍），所以它的空闲时间会大幅减少。\n",
      "3.  **显存注意**：Teacher 那张卡现在要独自承担所有 Teacher 模型的 KV Cache 和 Forward 计算。对于 4B 模型问题不大，如果是 70B 模型可能需要 2 张卡做 Teacher。\n",
      "\n",
      "**总结：请立刻改为 7:1 或 6:2，你的训练效率会起飞。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在是4卡做student，4卡做teacher，均衡分配的，但我看到有4张卡经常空置，感觉是不是其中某一项的GPU任务会更重一些？这样均衡分配不合理？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8758e325-7db5-4c5d-a302-407cb02be09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del chat_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d28f01d6-c5da-4026-aace-6a7fbb2782db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9656054162575665)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(-0.035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94837090-7a19-4fa0-afed-0fdafc9a5921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是最关键的一步。之前的修改其实已经让 `DP_Actor` 正确计算了概率（因为 Left Padding 配合了切片逻辑），但**返回的结果仍然是 Left Padded 的**。\n",
      "\n",
      "而你的 Student 数据（以及后续的 PPO 逻辑、日志打印）全部假设数据是 **Right Padded**（有效数据在左，Padding 在右）。\n",
      "\n",
      "这就导致了：\n",
      "1.  **日志误导**：你打印第 0 个位置，Student 是 \"Let\"，Teacher 是 \"Pad\"。你看到的是 \"Pad\" 的概率（-14.82），却以为是 \"Let\" 的概率。\n",
      "2.  **Reward 错误**：直接相减 `T - S`，等于用 `LogP(Pad)` 减去 `LogP(Let)`，完全算错。\n",
      "\n",
      "### 解决方案：在 `fit` 中对齐 Tensor\n",
      "\n",
      "我们需要在拿到 `teacher_full_log_probs` 后，立刻将其**从 Left Padding 转为 Right Padding**，使其与 Student 完美对齐。\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py` 的 `fit` 函数。\n",
      "\n",
      "#### 1. 添加一个辅助函数 (放在类外或类内均可)\n",
      "\n",
      "```python\n",
      "def left_to_right_padding(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    将 Left Padded 的 Tensor 转换为 Right Padded。\n",
      "    tensor: [B, L]\n",
      "    mask: [B, L] (1 for valid, 0 for pad)\n",
      "    \"\"\"\n",
      "    B, L = tensor.shape\n",
      "    device = tensor.device\n",
      "    \n",
      "    # 计算每个样本的有效长度\n",
      "    lens = mask.sum(dim=-1).long() # [B]\n",
      "    \n",
      "    # 创建新的 Right Padded Tensor\n",
      "    new_tensor = torch.zeros_like(tensor)\n",
      "    \n",
      "    for i in range(B):\n",
      "        l = lens[i]\n",
      "        if l > 0:\n",
      "            # 取出有效部分 (Left Padded 的有效部分在末尾)\n",
      "            valid_part = tensor[i, -l:]\n",
      "            # 放到开头 (Right Padded)\n",
      "            new_tensor[i, :l] = valid_part\n",
      "            \n",
      "    return new_tensor\n",
      "```\n",
      "\n",
      "#### 2. 修改 `fit` 函数\n",
      "\n",
      "在 Step 3.4 和 3.5 之间，对 Teacher 的 LogProb 进行转换。\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.4 Teacher Computes LogProb) ...\n",
      "                    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries)\n",
      "                    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
      "                    teacher_full_log_probs = teacher_log_prob_output.batch['old_log_probs']\n",
      "                    \n",
      "                    # === FIX: 将 Teacher 的 LogProbs 从 Left Pad 转为 Right Pad ===\n",
      "                    # 这里的 teacher_batch['response_mask'] 是 Left Padded 的，正好用来提取有效 LogProbs\n",
      "                    t_resp_mask = teacher_batch.batch['response_mask']\n",
      "                    \n",
      "                    # 转换!\n",
      "                    teacher_full_log_probs_aligned = left_to_right_padding(teacher_full_log_probs, t_resp_mask)\n",
      "                    \n",
      "                    # 现在的 teacher_full_log_probs_aligned 是 [Let, 's, ..., Pad, Pad]\n",
      "                    # 与 Student 的结构一致了\n",
      "                    # ==========================================================\n",
      "\n",
      "                    # ... (Step 3.5 Student Computes LogProb) ...\n",
      "                    batch.meta_info['temperature'] = 1.0 \n",
      "                    student_log_prob_output = self.actor_rollout_wg.compute_log_prob(batch)\n",
      "                    student_full_log_probs = student_log_prob_output.batch['old_log_probs']\n",
      "                    \n",
      "                    # ... (Reward Calculation) ...\n",
      "                    # 注意：下面计算 Reward 时，要用 teacher_full_log_probs_aligned\n",
      "                    \n",
      "                    token_level_rewards = torch.zeros_like(student_full_log_probs)\n",
      "                    s_probs = student_full_log_probs\n",
      "                    t_probs = teacher_full_log_probs_aligned # <--- 使用对齐后的 Tensor\n",
      "                    \n",
      "                    min_len = min(s_probs.shape[1], t_probs.shape[1])\n",
      "                    s_part = s_probs[:, :min_len]\n",
      "                    t_part = t_probs[:, :min_len]\n",
      "                    \n",
      "                    kl_diff = t_part - s_part\n",
      "```\n",
      "\n",
      "#### 3. 同时也修正 Log 打印部分\n",
      "\n",
      "在 Debug Log 的 `Token-wise KL Breakdown` 部分，确保使用的是 `t_part` (已经是对齐过的)。\n",
      "\n",
      "```python\n",
      "                            # ... inside Log loop ...\n",
      "                            s_vals = s_part[idx].tolist()\n",
      "                            t_vals = t_part[idx].tolist() # 这里引用的 t_part 已经是 Right Padded 的了\n",
      "                            diff_vals = kl_diff[idx].tolist()\n",
      "                            \n",
      "                            # 现在的 t_vals[0] 对应的是 \"Let\" 的 LogProb，而不是 Pad 的\n",
      "```\n",
      "\n",
      "### 总结\n",
      "1.  **Left Padding (Input & Labels)**：保证了 `DP_Actor` 能算对每个 Token 的 LogProb。\n",
      "2.  **Left-to-Right Shift (Post-processing)**：保证了计算出的 LogProb 能与 Student 对齐，从而正确计算 KL Divergence。\n",
      "\n",
      "这次修改后，`T_LogP` 在日志中应该会显示为正常数值（比如 -0.5），而不是 -14.82。这才是真正的 On-Policy Distillation。\n"
     ]
    }
   ],
   "source": [
    "print(chat_history[-3].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e1b69-fb0c-4db1-9e0a-433d010d2213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a3cf888-9e4d-4876-96e5-4fff7498e333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个显存溢出（OOM）问题。\n",
      "\n",
      "### 原因分析\n",
      "1.  **序列长度暴增**：你将 Prompt 和 Response 长度都改为了 4096，总上下文长度达到 **8192**。相比之前的 1024，显存需求增加了 8 倍。\n",
      "2.  **Logits Tensor 巨大**：\n",
      "    *   Qwen 的词表大小约 152,000。\n",
      "    *   Teacher 的 Micro Batch 设为 4。\n",
      "    *   Logits Tensor 大小 = $4 \\times 8192 \\times 152000 \\times 2 \\text{ Bytes (bf16)} \\approx \\mathbf{9.3 \\text{ GB}}$。\n",
      "    *   在计算 Entropy 时（`entropy_from_logits`），PyTorch 需要创建临时 Tensor 进行 `logsumexp` 等操作，峰值显存可能需要 Logits 的 2-3 倍（约 20-30 GB）。\n",
      "3.  **显存已被占用**：\n",
      "    *   Teacher 运行在单张 GPU 上。\n",
      "    *   由于为了支持 Summary 生成，Teacher 同时也加载了 vLLM。\n",
      "    *   vLLM 预占用了 40% 显存（约 32GB）。\n",
      "    *   PyTorch 模型权重占用约 8GB。\n",
      "    *   剩余可用显存可能不足以支撑 30GB 的计算峰值，加上内存碎片，导致 OOM。\n",
      "\n",
      "### 解决方案\n",
      "\n",
      "你需要**大幅减小计算 LogProb 时的 Micro Batch Size**，并开启**分块计算（Chunking）**以节省显存。\n",
      "\n",
      "请修改启动脚本中的以下参数：\n",
      "\n",
      "#### 1. 减小 Teacher 的 Micro Batch (最关键)\n",
      "将 Teacher 的计算批次从 4 降为 **1**。对于 8k 长度的序列，单卡一次只能处理一条。\n",
      "```bash\n",
      "actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1\n",
      "```\n",
      "\n",
      "#### 2. 减小 Student 的 LogProb Micro Batch\n",
      "Student 虽然有 7 张卡，但为了安全，建议也减小。\n",
      "```bash\n",
      "actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=2\n",
      "```\n",
      "\n",
      "#### 3. 开启 Logits 分块计算 (节省峰值显存)\n",
      "Verl 支持对 Logits 进行切片计算，防止瞬间 OOM。需要在配置中开启。\n",
      "在启动脚本中追加：\n",
      "```bash\n",
      "actor_rollout_ref.actor.entropy_from_logits_with_chunking=True\n",
      "```\n",
      "\n",
      "#### 4. (可选) 进一步压榨 vLLM 显存\n",
      "如果 Teacher 还是 OOM，可以进一步降低 vLLM 的显存占用（Teacher 的 vLLM 只用来生成 Summary，不需要太大的 KV Cache）。\n",
      "在代码 `teacher_student_reflective_trainer.py` 的 `init_workers` 中，强制修改 Teacher Config：\n",
      "\n",
      "```python\n",
      "        # 在 init_workers 中\n",
      "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        # === FIX: 降低 Teacher vLLM 显存占用 ===\n",
      "        teacher_config.rollout.gpu_memory_utilization = 0.2 # 从 0.4 降到 0.2\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 推荐的修正后启动命令参数\n",
      "\n",
      "请在你的启动脚本中更新以下部分：\n",
      "\n",
      "```bash\n",
      "# 显存优化配置\n",
      "actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=1 \\\n",
      "actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=2 \\\n",
      "actor_rollout_ref.actor.entropy_from_logits_with_chunking=True \\\n",
      "```\n",
      "\n",
      "**解释**：\n",
      "*   `batch_size=1` 意味着每次只计算一条 8k 长度的数据，显存需求从 30GB 降到 7GB 左右，可以轻松塞进剩余显存。\n",
      "*   这不会影响训练的正确性，只会稍微增加一点点 Teacher Forward 的时间（但在 7:1 比例下，Teacher 依然比 Student 快）。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "修改之后报错\n",
    "\"\"\"\n",
    "Error executing job with overrides: ['data.train_files=/data/kcl/lpy/data/OpenThoughts/train_verl.parquet', 'data.val_files=/data/kcl/lpy/data/AIME_2024/test_verl.parquet', 'data.train_batch_size=224', 'data.max_prompt_length=4096', 'data.max_response_length=4096', '+data.summary_max_new_tokens=4096', 'actor_rollout_ref.model.path=/data/kcl/lpy/models/qwen3_4b_instruct', 'actor_rollout_ref.actor.optim.lr=5e-7', 'actor_rollout_ref.actor.ppo_mini_batch_size=56', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4', 'actor_rollout_ref.actor.policy_loss=distillation', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4', 'critic.optim.lr=1e-5', 'critic.model.path=/data/kcl/lpy/models/qwen3_4b_instruct', 'critic.ppo_micro_batch_size_per_gpu=4', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.logger=console', 'trainer.val_before_train=False', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=30', 'trainer.test_freq=30', \"trainer.logger=['swanlab', 'console']\", 'trainer.project_name=self_reflect', 'trainer.experiment_name=sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_fixed', 'trainer.default_local_dir=/data/kcl/lpy/verl/verl_checkpoints/sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_fixed', 'trainer.total_epochs=15']\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 41, in main\n",
    "    run_ppo(config)\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 75, in run_ppo\n",
    "    ray.get(runner.run.remote(config))\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
    "    return fn(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 2967, in get\n",
    "    values, debugger_breakpoint = worker.get_objects(\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 1015, in get_objects\n",
    "    raise value.as_instanceof_cause()\n",
    "ray.exceptions.RayTaskError(OutOfMemoryError): ray::TaskRunner.run() (pid=1872212, ip=192.168.131.187, actor_id=e3f06eedabfe8faf685bd13b01000000, repr=<main_self_reflective.TaskRunner object at 0x7f09430fddf0>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 245, in run\n",
    "    trainer.fit()\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/ppo/teacher_student_reflective_trainer.py\", line 694, in fit\n",
    "    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
    "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 50, in __call__\n",
    "    output = ray.get(output)\n",
    "             ^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "ray.exceptions.RayTaskError(OutOfMemoryError): ray::WorkerDict.ref_compute_log_prob() (pid=1874403, ip=192.168.131.187, actor_id=85d4ece21bf572ebaf3cd2f701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f812f1b5eb0>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "    return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 782, in compute_log_prob\n",
    "    output, entropys = self.actor.compute_log_prob(data=data, calculate_entropy=True)\n",
    "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/utils/profiler/performance.py\", line 89, in f\n",
    "    return self.log(decorated_function, *args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/utils/profiler/performance.py\", line 102, in log\n",
    "    output = func(*args, **kwargs)\n",
    "             ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/actor/dp_actor.py\", line 316, in compute_log_prob\n",
    "    entropy, log_probs = self._forward_micro_batch(\n",
    "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/actor/dp_actor.py\", line 266, in _forward_micro_batch\n",
    "    entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)\n",
    "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/utils/torch_functional.py\", line 148, in entropy_from_logits\n",
    "    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)\n",
    "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 18.55 GiB. GPU 0 has a total capacity of 79.35 GiB of which 7.17 GiB is free. Process 2511605 has 72.15 GiB memory in use. Of the allocated memory 81.05 GiB is allocated by PyTorch, with 799.75 MiB allocated in private pools (e.g., CUDA Graphs), and 685.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "\n",
    "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
    "\"\"\"\n",
    "怎么解决\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3857ad0-fe37-49dc-b552-1b15c4dede2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e368fb-4b45-4c14-be42-33f22a6dea27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4615ac30-4746-4eba-9f06-eb83e513c0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个错误非常明确：**训练数据中的 Prompt 长度超过了配置的最大长度限制**。\n",
      "\n",
      "### 错误分析\n",
      "```text\n",
      "NotImplementedError: sequence_length=1049 is larger than max_length=512\n",
      "```\n",
      "*   **OpenThoughts 数据集**：包含复杂的数学/推理问题，Prompt 往往很长（尤其是带有 Few-shot 或详细描述时）。\n",
      "*   **当前配置**：`data.max_prompt_length=512`。\n",
      "*   **冲突**：有一条数据的 Prompt 长度达到了 1049 Token，超过了 512 的限制。Verl 的 `postprocess_data` 默认行为是抛出异常而不是截断（为了保证数据完整性）。\n",
      "\n",
      "### 解决方案\n",
      "\n",
      "你需要**增大 `data.max_prompt_length`**。\n",
      "\n",
      "对于 OpenThoughts 这样的推理数据集，建议至少设置为 **2048** 甚至 **4096**。\n",
      "\n",
      "**修改启动脚本**：\n",
      "\n",
      "```bash\n",
      "# 原来的\n",
      "data.max_prompt_length=512\n",
      "\n",
      "# 修改为\n",
      "data.max_prompt_length=2048\n",
      "# 或者更稳妥的\n",
      "data.max_prompt_length=4096\n",
      "```\n",
      "\n",
      "**注意：**\n",
      "增大 `max_prompt_length` 会增加显存占用。\n",
      "*   如果你之前的 `max_response_length` 是 1024，那么总序列长度将达到 3072+。\n",
      "*   如果显存吃紧（OOM），你可能需要：\n",
      "    1.  减小 `ppo_mini_batch_size`（比如从 56 减到 28）。\n",
      "    2.  减小 `micro_batch_size`（从 4 减到 2）。\n",
      "    3.  或者开启 Gradient Checkpointing（如果支持）。\n",
      "\n",
      "**建议配置更新：**\n",
      "```bash\n",
      "data.max_prompt_length=2048\n",
      "data.max_response_length=2048  # OpenThoughts 的回答也很长，建议同步增大\n",
      "# 如果显存不够：\n",
      "actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我想把训练数据改成OpenThoughts数据集，我改了之后发现目前代码报错\n",
    "\"\"\"\n",
    "Error executing job with overrides: ['data.train_files=/data/kcl/lpy/data/OpenThoughts/train_verl.parquet', 'data.val_files=/data/kcl/lpy/data/AIME_2024/test_verl.parquet', 'data.train_batch_size=224', 'data.max_prompt_length=512', 'data.max_response_length=1024', '+data.summary_max_new_tokens=2048', 'actor_rollout_ref.model.path=/data/kcl/lpy/models/qwen3_4b_instruct', 'actor_rollout_ref.actor.optim.lr=5e-7', 'actor_rollout_ref.actor.ppo_mini_batch_size=56', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4', 'actor_rollout_ref.actor.policy_loss=distillation', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4', 'critic.optim.lr=1e-5', 'critic.model.path=/data/kcl/lpy/models/qwen3_4b_instruct', 'critic.ppo_micro_batch_size_per_gpu=4', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.logger=console', 'trainer.val_before_train=False', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=30', 'trainer.test_freq=30', \"trainer.logger=['swanlab', 'console']\", 'trainer.project_name=self_reflect', 'trainer.experiment_name=sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_fixed', 'trainer.default_local_dir=/data/kcl/lpy/verl/verl_checkpoints/sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_fixed', 'trainer.total_epochs=15']\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 41, in main\n",
    "    run_ppo(config)\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 75, in run_ppo\n",
    "    ray.get(runner.run.remote(config))\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
    "    return fn(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 2967, in get\n",
    "    values, debugger_breakpoint = worker.get_objects(\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 1015, in get_objects\n",
    "    raise value.as_instanceof_cause()\n",
    "ray.exceptions.RayTaskError(NotImplementedError): ray::TaskRunner.run() (pid=1841605, ip=192.168.131.187, actor_id=0476be1d1c330558b46c1c4701000000, repr=<main_self_reflective.TaskRunner object at 0x7f2bd19450d0>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 245, in run\n",
    "    trainer.fit()\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/ppo/teacher_student_reflective_trainer.py\", line 605, in fit\n",
    "    for batch_dict in self.train_dataloader:\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torchdata/stateful_dataloader/stateful_dataloader.py\", line 450, in __next__\n",
    "    return super().__next__()\n",
    "           ^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
    "    data = self._next_data()\n",
    "           ^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torchdata/stateful_dataloader/stateful_dataloader.py\", line 1456, in _next_data\n",
    "    return self._process_data(data, worker_id, state_dict)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torchdata/stateful_dataloader/stateful_dataloader.py\", line 1543, in _process_data\n",
    "    data.reraise()\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torch/_utils.py\", line 733, in reraise\n",
    "    raise exception\n",
    "NotImplementedError: Caught NotImplementedError in DataLoader worker process 0.\n",
    "Original Traceback (most recent call last):\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torchdata/stateful_dataloader/worker.py\", line 242, in _worker_loop\n",
    "    data = fetcher.fetch(index)  # type: ignore[union-attr]\n",
    "           ^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
    "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
    "            ~~~~~~~~~~~~^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/utils/dataset/rl_dataset.py\", line 266, in __getitem__\n",
    "    input_ids, attention_mask = verl_F.postprocess_data(\n",
    "                                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/utils/torch_functional.py\", line 377, in postprocess_data\n",
    "    raise NotImplementedError(f\"{sequence_length=} is larger than {max_length=}\")\n",
    "NotImplementedError: sequence_length=1049 is larger than max_length=512\n",
    "\"\"\"\n",
    "怎么解决?\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01df5f13-d86c-4abb-a594-904331025ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是因为 Verl 的评测系统是**数据驱动（Data-Driven）**的。指标名称和计算逻辑会自动根据你加载的数据集类型进行调整。\n",
      "\n",
      "### 1. 为什么指标名称变了？\n",
      "\n",
      "**原因**：你的验证数据集 Parquet 文件中的 `data_source` 列发生了变化。\n",
      "*   之前 GSM8K 数据集里，`data_source` 列的值是 `\"openai/gsm8k\"`。\n",
      "*   现在 AIME 数据集里，`data_source` 列的值应该是 `\"math_dapo\"`（或者类似的值）。\n",
      "\n",
      "**代码逻辑**：\n",
      "在 `ray_trainer.py` 的 `_validate` 函数最后：\n",
      "```python\n",
      "# 提取数据源名称\n",
      "data_source_lst.append(test_batch.non_tensor_batch.get(\"data_source\", ...))\n",
      "# ...\n",
      "# 拼接 Metric 名称\n",
      "pfx = f\"{metric_sec}/{data_source}/{var_name}/{metric_name}\"\n",
      "```\n",
      "所以 `val-core/math_dapo/...` 中的 `math_dapo` 就是直接从你的数据文件里读出来的。\n",
      "\n",
      "### 2. 指标含义解析\n",
      "\n",
      "*   **`val-core/math_dapo/acc/mean@32`**\n",
      "    *   **含义**：**Pass@32 准确率**（或者更准确地说是 N=32 时的平均准确率）。\n",
      "    *   **来源**：这意味着在验证时，针对每道 AIME 题目，模型生成了 **32 个回答**（`n=32`）。\n",
      "    *   **计算**：如果这 32 个回答中有任何一个做对了，或者计算这 32 个回答的平均正确率（取决于具体实现，通常 `acc` 指的是 Accuracy，`mean` 指的是平均分）。但在这里，结合 `@32`，通常是指 Pass@K 的某种变体或者是 Majority Vote 的基础数据。\n",
      "    *   **注意**：AIME 这种难题，通常需要多次采样（Pass@K）才能评估模型能力，因为单次采样的方差太大。\n",
      "\n",
      "*   **`val-aux/math_dapo/reward/worst@2/mean`**\n",
      "    *   **含义**：最差的 2 个回答的平均 Reward。\n",
      "    *   **用途**：用于监控模型的下限。\n",
      "\n",
      "### 3. 代码是怎么自动切换采样数 (N) 的？\n",
      "\n",
      "你可能会问：“我没改 N 啊，为什么变成 @32 了？”\n",
      "\n",
      "这通常是由 **Dataset Config** 或者 **Data Load 逻辑** 决定的。\n",
      "在 `main_self_reflective.py` 中：\n",
      "```python\n",
      "val_dataset = create_rl_dataset(config.data.val_files, ...)\n",
      "```\n",
      "如果你的 AIME Parquet 文件中，每条数据实际上包含了 **32 个重复的 Prompt**（或者你在 Config 里设置了 `val_kwargs.n=32`），那么验证循环就会针对每道题跑 32 次。\n",
      "\n",
      "**检查你的 Config**：\n",
      "看一下 `actor_rollout_ref.rollout.val_kwargs.n` 的值。如果它是 32，那就是这里控制的。\n",
      "或者，你的 AIME 数据集本身就是为了 Pass@K 评测准备的，数据加载器自动识别了这一点。\n",
      "\n",
      "### 总结\n",
      "*   **名称变化**：源于数据文件中的 `data_source` 字段。\n",
      "*   **@32**：源于验证时的采样次数（`n=32`），适合 AIME 这种高难度数学题。\n",
      "*   **计算逻辑**：依然是 `Reward Function`（正则匹配答案）的结果聚合。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "修改验证数据为AIME-2024之后，validate的metric自动变了，变成了\n",
    "val-core/math_dapo/acc/mean@32\n",
    "val-aux/math_dapo/reward/worst@2/mean\n",
    "val-core/math_dapo/acc/mean@32 \n",
    "这些指标是怎么算的？ 具体是什么含义？ 代码是怎么自动切换的？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dfbd417-c647-463b-81c6-f54c0ef4eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del chat_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dd314e8-86e7-4b04-9819-b58d7cbefb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的 `init_workers` 代码逻辑在设计时已经考虑到了 `n_nodes`，因此**核心分配逻辑（7:1 切分）在多机多卡下是通用的，不需要重写**。\n",
      "\n",
      "但是，从单机扩展到多机，**Batch Size 的整除约束**会发生变化，如果不提前检查，启动后会直接报错（和之前一样的 `ActorDiedError`）。\n",
      "\n",
      "建议对 `init_workers` 做如下**增强性修改**，加入自动检查和拓扑打印功能，确保多机启动一次成功。\n",
      "\n",
      "### 1. 修改 `init_workers` (增强版)\n",
      "\n",
      "请用以下代码替换 `teacher_student_reflective_trainer.py` 中的 `init_workers`。\n",
      "\n",
      "**主要改动点**：\n",
      "1.  自动计算 Student 的总 World Size（节点数 × 单节点卡数）。\n",
      "2.  **提前检查 Batch Size 整除性**，如果配置不对，直接抛出带有修改建议的 Error，避免等 Ray 启动半天后才报错。\n",
      "3.  打印清晰的多机拓扑结构。\n",
      "\n",
      "```python\n",
      "    def init_workers(self):\n",
      "        \"\"\"\n",
      "        初始化 Worker，支持多机多卡 (Multi-Node) 自动分配与检查。\n",
      "        \"\"\"\n",
      "        # 1. 获取资源配置\n",
      "        n_gpus_per_node = self.config.trainer.n_gpus_per_node\n",
      "        n_nodes = self.config.trainer.nnodes\n",
      "        \n",
      "        # === 资源切分策略 (7:1) ===\n",
      "        if n_gpus_per_node >= 8:\n",
      "            teacher_gpus_per_node = 1\n",
      "        elif n_gpus_per_node >= 4:\n",
      "            teacher_gpus_per_node = 1\n",
      "        else:\n",
      "            teacher_gpus_per_node = 1 # 至少给1张\n",
      "            \n",
      "        student_gpus_per_node = n_gpus_per_node - teacher_gpus_per_node\n",
      "        \n",
      "        if student_gpus_per_node < 1:\n",
      "            raise ValueError(f\"Not enough GPUs per node! Got {n_gpus_per_node}, need at least 2.\")\n",
      "\n",
      "        # 计算总的 World Size\n",
      "        student_world_size = student_gpus_per_node * n_nodes\n",
      "        teacher_world_size = teacher_gpus_per_node * n_nodes\n",
      "        \n",
      "        print(f\"\\n{'='*20} Multi-Node Topology Setup {'='*20}\")\n",
      "        print(f\">>> Nodes: {n_nodes} | GPUs per Node: {n_gpus_per_node}\")\n",
      "        print(f\">>> Student (Actor): {student_gpus_per_node} GPUs/node * {n_nodes} nodes = {student_world_size} Total GPUs\")\n",
      "        print(f\">>> Teacher (Ref):   {teacher_gpus_per_node} GPUs/node * {n_nodes} nodes = {teacher_world_size} Total GPUs\")\n",
      "\n",
      "        # === 关键检查: Batch Size 整除性 (防止启动后报错) ===\n",
      "        # 规则: ppo_mini_batch_size 必须能被 (Student_World_Size * Micro_Batch) 整除\n",
      "        # 或者至少: (ppo_mini_batch_size / Student_World_Size) 必须是整数，且能被 Micro_Batch 整除\n",
      "        \n",
      "        mini_batch = self.config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
      "        micro_batch = self.config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu\n",
      "        \n",
      "        # 1. 检查每张卡分到的 Batch 是否为整数\n",
      "        if mini_batch % student_world_size != 0:\n",
      "            suggested = (mini_batch // student_world_size + 1) * student_world_size\n",
      "            raise ValueError(\n",
      "                f\"\\n[Config Error] ppo_mini_batch_size ({mini_batch}) cannot be divided evenly by Student World Size ({student_world_size}).\\n\"\n",
      "                f\"Student GPUs = {student_world_size} (Nodes: {n_nodes} * GPUs: {student_gpus_per_node})\\n\"\n",
      "                f\"Suggested ppo_mini_batch_size: {suggested} or {suggested + student_world_size}\"\n",
      "            )\n",
      "            \n",
      "        local_batch = mini_batch // student_world_size\n",
      "        \n",
      "        # 2. 检查每张卡的 Local Batch 是否能被 Micro Batch 整除\n",
      "        if local_batch % micro_batch != 0:\n",
      "            raise ValueError(\n",
      "                f\"\\n[Config Error] Local Batch ({local_batch}) cannot be divided by Micro Batch ({micro_batch}).\\n\"\n",
      "                f\"Global Mini Batch: {mini_batch}, Student World Size: {student_world_size}\\n\"\n",
      "                f\"Please adjust ppo_mini_batch_size or ppo_micro_batch_size_per_gpu.\"\n",
      "            )\n",
      "            \n",
      "        print(f\">>> Batch Check Passed: Global={mini_batch} -> PerGPU={local_batch} -> Micro={micro_batch} (Accum={local_batch//micro_batch})\")\n",
      "        print(f\"{'='*60}\\n\")\n",
      "\n",
      "        # 2. 创建资源池 (RayResourcePool 会自动处理跨节点调度)\n",
      "        # process_on_nodes=[N, N] 表示 Node 0 需要 N 张卡，Node 1 需要 N 张卡...\n",
      "        student_pool = RayResourcePool(\n",
      "            process_on_nodes=[student_gpus_per_node] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"student_pool\"\n",
      "        )\n",
      "        \n",
      "        teacher_pool = RayResourcePool(\n",
      "            process_on_nodes=[teacher_gpus_per_node] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"teacher_pool\"\n",
      "        )\n",
      "        \n",
      "        self.resource_pool_to_cls = {student_pool: {}, teacher_pool: {}}\n",
      "\n",
      "        # 3. 初始化 Config\n",
      "        student_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        student_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.ActorRollout],\n",
      "            config=student_config,\n",
      "            role=\"actor_rollout\",\n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[student_pool][\"actor_rollout\"] = student_cls\n",
      "\n",
      "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        # 显存优化: Teacher 显存占用降低\n",
      "        teacher_config.rollout.gpu_memory_utilization = 0.2\n",
      "        \n",
      "        teacher_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.RefPolicy],\n",
      "            config=teacher_config,\n",
      "            role=\"actor_rollout\",\n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[teacher_pool][\"ref\"] = teacher_cls\n",
      "\n",
      "        # 4. 启动 Workers\n",
      "        all_wg = {}\n",
      "        wg_kwargs = {\n",
      "            \"ray_wait_register_center_timeout\": self.config.trainer.ray_wait_register_center_timeout,\n",
      "            \"device_name\": self.device_name\n",
      "        }\n",
      "        \n",
      "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
      "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
      "            wg_dict = self.ray_worker_group_cls(\n",
      "                resource_pool=resource_pool,\n",
      "                ray_cls_with_init=worker_dict_cls,\n",
      "                **wg_kwargs,\n",
      "            )\n",
      "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
      "            all_wg.update(spawn_wg)\n",
      "\n",
      "        self.actor_rollout_wg = all_wg[\"actor_rollout\"]\n",
      "        self.actor_rollout_wg.init_model()\n",
      "        \n",
      "        self.ref_policy_wg = all_wg[\"ref\"]\n",
      "        self.ref_policy_wg.init_model()\n",
      "            \n",
      "        self.async_rollout_mode = False\n",
      "        if self.config.actor_rollout_ref.rollout.mode == 'async':\n",
      "            from verl.experimental.agent_loop import AgentLoopManager\n",
      "            self.async_rollout_mode = True\n",
      "            self.async_rollout_manager = AgentLoopManager(\n",
      "                config=self.config,\n",
      "                worker_group=self.actor_rollout_wg\n",
      "            )\n",
      "```\n",
      "\n",
      "### 2. 启动脚本修改 (Param Changes)\n",
      "\n",
      "假设你有 **2 台机器** (N_NODES=2)，每台 **8 张卡**。\n",
      "那么：\n",
      "*   Student 总卡数 = $7 \\times 2 = 14$ 张。\n",
      "*   Teacher 总卡数 = $1 \\times 2 = 2$ 张。\n",
      "\n",
      "你需要调整 `ppo_mini_batch_size` 使得它能被 14 整除，且商能被 4 (Micro Batch) 整除。即被 56 整除。\n",
      "\n",
      "**推荐配置 (2 机 16 卡)**：\n",
      "\n",
      "```bash\n",
      "export N_NODES=2\n",
      "export n_gpus_per_node=8\n",
      "\n",
      "# Batch Size 计算:\n",
      "# Student Total GPUs = 14\n",
      "# Micro Batch = 4\n",
      "# 最小 Mini Batch = 14 * 4 = 56\n",
      "# 建议 Mini Batch = 112 (每卡分 8 条，累积 2 次)\n",
      "\n",
      "# 启动参数覆盖:\n",
      "trainer.nnodes=$N_NODES \\\n",
      "trainer.n_gpus_per_node=$n_gpus_per_node \\\n",
      "data.train_batch_size=224 \\  # 224 / 112 = 2 个 Mini-Batch\n",
      "actor_rollout_ref.actor.ppo_mini_batch_size=112 \\\n",
      "actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4\n",
      "```\n",
      "\n",
      "### 3. 多机启动命令 (Ray Cluster)\n",
      "\n",
      "多机训练不是直接跑 python 脚本，需要先搭建 Ray 集群。\n",
      "\n",
      "**Step 1: 在主节点 (Head Node) 启动 Ray**\n",
      "```bash\n",
      "# 假设主节点 IP 是 192.168.1.100\n",
      "ray start --head --port=6379 --num-gpus=8 --block\n",
      "```\n",
      "\n",
      "**Step 2: 在从节点 (Worker Node) 启动 Ray**\n",
      "```bash\n",
      "# 连接到主节点\n",
      "ray start --address='192.168.1.100:6379' --num-gpus=8 --block\n",
      "```\n",
      "\n",
      "**Step 3: 在主节点运行 Python 训练脚本**\n",
      "```bash\n",
      "# 你的 python3 -m verl.trainer.main_self_reflective ... 命令\n",
      "# 确保加上 trainer.nnodes=2\n",
      "```\n",
      "\n",
      "**注意**：\n",
      "*   确保两台机器的代码、路径、Python 环境完全一致。\n",
      "*   确保 `export VLLM_USE_V1=1` 等环境变量在两台机器上都生效。\n",
      "*   Verl 会自动检测 Ray 集群并分发任务。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我现在要改成多机多卡运行，有什么地方要修改吗？ 比如init_workers?\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13698626-a0ce-4c34-b67a-c25b241663e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
