{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4057e08b-7e7c-4471-bb83-8d808c2cf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf83dd81-ea48-443b-84b8-a602068d2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置API密钥（需替换为你的实际密钥）\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-Gzo6arNf2T7NmrUYZUcxcQgRnIYclT5tqmgu9NfB34HfBkRd\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api2.aigcbest.top/v1\"  # 必填代理地址\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import os\n",
    "# claude-opus-4-1-20250805\n",
    "# claude-opus-4-1-thinking-all\n",
    "# 初始化 GPT-4o 模型\n",
    "model = ChatOpenAI(\n",
    "    model=\"gemini-3-pro-preview-thinking\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 创建对话历史记录\n",
    "chat_history = []\n",
    "\n",
    "def chat(user_input):\n",
    "    # 添加用户消息到历史\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    # 生成回复\n",
    "    response = model.invoke(chat_history)\n",
    "    # 提取并显示 AI 回复\n",
    "    ai_reply = response.content\n",
    "    # 保存 AI 消息到历史\n",
    "    chat_history.append(AIMessage(content=ai_reply))\n",
    "    print(ai_reply)\n",
    "    return ai_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d74d5d-a045-45d8-9dc9-6e7dd68e95c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常敏锐的观察。日志中的数据确实揭示了一个潜在的严重问题，主要表现为 **Student LogProb (`S_LogP`) 异常地接近 0**（例如 `-0.00`），而 **Teacher LogProb (`T_LogP`) 却非常低**（例如 `-12.61`）。\n",
      "\n",
      "这通常意味着以下两个问题之一或同时存在：\n",
      "1.  **Temperature（温度）设置不一致**：Student 在计算 LogProb 时可能使用了极低的温度（接近 0），导致分布极度尖锐（Probability $\\approx 1.0$），而 Teacher 使用了默认温度（通常为 1.0）。\n",
      "2.  **Prompt 上下文严重偏移（Context Shift）**：Teacher 看到的 Prompt（包含 Hint 和 Instruction）使得 Student 生成的 Token（例如 \"Let\"）在 Teacher 眼里变得极度不可能。\n",
      "\n",
      "以下是详细的分析和修复方案。\n",
      "\n",
      "---\n",
      "\n",
      "### 1. 问题分析\n",
      "\n",
      "#### A. 为什么 `S_LogP` 是 `-0.00`？\n",
      "*   **现象**：`S_LogP` 为 `-0.00` 意味着模型对该 Token 的预测概率接近 100%。对于一个未经多轮强化学习训练的基座模型（Qwen-Instruct），在 Step 1 就能达到这种确定性是不正常的，除非使用了 **Greedy Decoding** 或者 **极低的 Temperature**。\n",
      "*   **原因**：\n",
      "    *   在 `TeacherStudentReflectiveTrainer.fit` 中，Student 的 `batch` 是从 `gen_batch` 继承而来的。\n",
      "    *   `gen_batch` 的 `meta_info` 包含了生成时的参数。如果在配置文件中 `rollout` 的 `temperature` 设置得很低（或者 `do_sample=False`），那么这个低温度会被带入 `compute_log_prob`。\n",
      "    *   在 `DP_Actor.compute_log_prob` 中，代码执行了 `logits.div_(temperature)`。如果 `temperature` 例如是 `0.01`，Logits 会被放大 100 倍，Softmax 后概率分布会变成 One-Hot，导致 LogProb 变成 0。\n",
      "\n",
      "#### B. 为什么 `T_LogP` 是 `-12.61`？\n",
      "*   **现象**：`-12.61` 意味着 $P(token) \\approx e^{-12.61} \\approx 3 \\times 10^{-6}$。这是一个极低的概率，甚至低于随机猜测（词表大小 150k 时，均匀分布 LogProb 约为 -11.9）。\n",
      "*   **原因**：\n",
      "    1.  **Temperature**：Teacher 的 `teacher_batch` 是在 `_prepare_teacher_forward_batch` 中新创建的，**代码中没有显式设置 `meta_info['temperature']`**。如果 `DataProto` 或 `DP_Actor` 默认使用 `1.0`，那么这个 LogProb 是在正常温度下计算的。\n",
      "    2.  **Prompt 差异**：\n",
      "        *   Student 看到的 Prompt 结尾是：`Question... <|im_end|>\\n<|im_start|>assistant\\n`。它生成了 `Let`。\n",
      "        *   Teacher 看到的 Prompt 结尾是：`Analysis... solution.<|im_end|>\\n<|im_start|>assistant\\n`。\n",
      "        *   虽然结尾都是 `assistant\\n`，但 Teacher 上下文中有 \"Analysis/Hint\"。Qwen 模型在看到 \"Analysis\" 后，可能倾向于输出 \"The correct solution is...\" 或者 \"Based on...\"，而不是直接开始 \"Let's solve\"。\n",
      "        *   **但是**，`-12` 实在太低了，这说明 Teacher 几乎认为 \"Let\" 是个错误的 Token。这通常是 **Temperature 不匹配** 加上 **上下文偏移** 共同作用的结果。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. 修复方案\n",
      "\n",
      "我们需要做两件事：\n",
      "1.  **强制统一 Temperature**：在计算 LogProb（无论是 Student 还是 Teacher）时，必须使用 `temperature=1.0`，以获得真实的 Logits 分布用于 KL 散度计算。\n",
      "2.  **修复 Teacher Batch 的 Meta Info**：确保 `teacher_batch` 携带正确的参数。\n",
      "\n",
      "请修改 `verl/trainer/ppo/teacher_student_reflective_trainer.py` 中的 `fit` 函数和 `_prepare_teacher_forward_batch` 函数。\n",
      "\n",
      "#### 修改 1: `_prepare_teacher_forward_batch` (添加 Meta Info)\n",
      "\n",
      "在 `_prepare_teacher_forward_batch` 函数的最后，返回 `teacher_batch` 之前，添加 `meta_info`。\n",
      "\n",
      "```python\n",
      "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor) -> DataProto:\n",
      "        # ... (前面的代码保持不变) ...\n",
      "\n",
      "        teacher_batch = DataProto.from_dict({\n",
      "            \"input_ids\": input_ids,\n",
      "            \"attention_mask\": attention_mask,\n",
      "            \"position_ids\": position_ids,\n",
      "            \"response_mask\": response_mask,\n",
      "            \"prompts\": prompts_padded,   \n",
      "            \"responses\": responses_padded \n",
      "        })\n",
      "        \n",
      "        # === FIX: 显式设置 Meta Info，确保计算 LogProb 时使用 Temp=1.0 ===\n",
      "        teacher_batch.meta_info = {\n",
      "            \"temperature\": 1.0,  # 关键：用于 LogProb 计算的标准温度\n",
      "            \"micro_batch_size\": batch.meta_info.get(\"micro_batch_size\", 1), # 继承或设默认\n",
      "            \"use_dynamic_bsz\": batch.meta_info.get(\"use_dynamic_bsz\", False),\n",
      "            \"max_token_len\": batch.meta_info.get(\"max_token_len\", 2048)\n",
      "        }\n",
      "        \n",
      "        return teacher_batch\n",
      "```\n",
      "\n",
      "#### 修改 2: `fit` 函数 (强制 Student Batch Temperature)\n",
      "\n",
      "在 `fit` 函数的 Step 3.5 (Student Computes LogProb) 之前，强制设置 `batch` 的 temperature。\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.4 Teacher LogProb 代码) ...\n",
      "                    # 3.4 Teacher Computes LogProb\n",
      "                    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries)\n",
      "                    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
      "                    teacher_full_log_probs = teacher_log_prob_output.batch['old_log_probs']\n",
      "\n",
      "                    # === FIX: 强制 Student LogProb 计算使用 Temp=1.0 ===\n",
      "                    # 避免继承生成时的低温参数导致 LogProb 为 0\n",
      "                    batch.meta_info['temperature'] = 1.0 \n",
      "\n",
      "                    # 3.5 Student Computes LogProb\n",
      "                    student_log_prob_output = self.actor_rollout_wg.compute_log_prob(batch)\n",
      "                    student_full_log_probs = student_log_prob_output.batch['old_log_probs']\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 3. 验证与预期\n",
      "\n",
      "应用上述修复后，你应该会看到：\n",
      "\n",
      "1.  **S_LogP 变得正常**：不再是 `-0.00`，而是变成类似 `-0.5` 到 `-2.0` 的值（取决于模型对该 Token 的置信度）。\n",
      "2.  **T_LogP 保持或略有变化**：它仍然会比 `S_LogP` 低（因为 Context 不同），但不应该低到 `-15` 这种离谱的程度（除非 Teacher 真的极其确信该 Token 不该出现）。\n",
      "3.  **Token-Level KL Diff**：`T_LogP - S_LogP`。\n",
      "    *   如果 `S` 是 `-0.5`，`T` 是 `-5.0`，Diff 是 `-4.5`。\n",
      "    *   这将给出一个负的 Reward（惩罚），这是合理的：Teacher 认为 Student 在这种情况下说 \"Let\" 不太好。\n",
      "\n",
      "**关于 Teacher Prompt 的额外建议：**\n",
      "如果修复 Temperature 后，`T_LogP` 依然极低（例如 `-10`），说明 Teacher 在看到 Hint 后确实非常不想输出 \"Let\"。你可以尝试微调 Teacher 的 Prompt，引导它去“补全”学生的回答，而不是“提供”一个新的解答。\n",
      "\n",
      "例如，修改 `_prepare_teacher_forward_batch` 中的 `teacher_content`：\n",
      "\n",
      "```python\n",
      "            teacher_content = (\n",
      "                f\"Problem: {p_text_clean}\\n\\n\"\n",
      "                f\"Analysis/Hint: {s_text}\\n\\n\"\n",
      "                f\"Based on the analysis, verify the following solution step-by-step:\" # 稍微改动指令，让它更倾向于跟随\n",
      "            )\n",
      "```\n",
      "或者，由于这是 On-Policy Distillation，Teacher 的 Context 差异导致的 KL 差异本身就是一种信号，可能不需要过度修正 Prompt，而是依赖 KL Loss 让 Student 学习适应 Teacher 的分布（但这可能导致 Student 学会忽略 Prompt 直接背诵 Teacher 的偏好）。\n",
      "\n",
      "**总结：最核心的 Bug 是 LogProb 计算时的 `temperature` 不一致且未显式设置为 1.0。请先应用上述代码修复。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我在用verl框架实现on policy distillation，目标如下\n",
    "\"\"\"\n",
    "1.student先根据query生成response，这里面肯定没有hack的问题，因为query里没有ground truth\n",
    "2.把query + student response + ground truth做个摘要\n",
    "3.用teacher mode重新算一遍student的response的概率，输入是 query + 上面的摘要 + student的response，这里不做生成，只前向传播计算概率\n",
    "4.最小化student的response和第3步中的概率的reverse KL loss，注意这里不是用第3步中的所有概率，而是只取第3步中response部分的前向传播概率，所以要做截断。\n",
    "\"\"\"\n",
    "我写的teacher_student_reflective_trainer.py代码如下\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional\n",
    "from copy import deepcopy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from verl import DataProto\n",
    "from verl.trainer.ppo.ray_trainer import RayPPOTrainer, compute_response_mask, compute_advantage, Role\n",
    "from verl.utils.metric import (\n",
    "    reduce_metrics,\n",
    ")\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from verl.trainer.ppo.core_algos import AdvantageEstimator\n",
    "from verl.utils.debug import marked_timer\n",
    "from verl.utils.torch_functional import masked_mean\n",
    "from verl.utils.tracking import Tracking\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\n",
    "from verl.single_controller.ray.base import create_colocated_worker_cls\n",
    "\n",
    "class TeacherStudentReflectiveTrainer(RayPPOTrainer):\n",
    "    \"\"\"\n",
    "    Teacher-Student Reflective Trainer.\n",
    "    \n",
    "    Architecture:\n",
    "    - Student (Actor): Trainable. Generates Response.\n",
    "    - Teacher (RefPolicy): Frozen. Generates Summary AND Computes LogProb.\n",
    "    \n",
    "    Resource Strategy:\n",
    "    - Splits available GPUs into two pools (Student Pool & Teacher Pool) to allow \n",
    "      two simultaneous vLLM instances without conflict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert self.use_reference_policy, \"TeacherStudentReflectiveTrainer requires a Reference Policy (Teacher)!\"\n",
    "        self.use_critic = False\n",
    "        self.use_rm = False\n",
    "        \n",
    "        print(\">>> TeacherStudentReflectiveTrainer Initialized.\")\n",
    "        print(\">>> Strategy: Frozen Teacher generates Summary & Scores.\")\n",
    "\n",
    "    def init_workers(self):\n",
    "        \"\"\"\n",
    "        Manually split resources and initialize two independent vLLM-enabled workers.\n",
    "        \"\"\"\n",
    "        # 1. Calculate Resource Split\n",
    "        # We assume homogeneous nodes. We split GPUs evenly between Student and Teacher.\n",
    "        n_gpus_per_node = self.config.trainer.n_gpus_per_node\n",
    "        n_nodes = self.config.trainer.nnodes\n",
    "        \n",
    "        if n_gpus_per_node < 2:\n",
    "            raise ValueError(f\"Need at least 2 GPUs per node to split Student/Teacher. Got {n_gpus_per_node}.\")\n",
    "            \n",
    "        student_gpus = n_gpus_per_node // 2\n",
    "        teacher_gpus = n_gpus_per_node - student_gpus\n",
    "        \n",
    "        print(f\">>> Resource Splitting: Student={student_gpus} GPUs, Teacher={teacher_gpus} GPUs (per node)\")\n",
    "\n",
    "        # 2. Create Independent Resource Pools\n",
    "        # Note: We create new RayResourcePools, ignoring the one passed from main.\n",
    "        # This ensures Ray allocates non-overlapping GPUs to each pool.\n",
    "        student_pool = RayResourcePool(\n",
    "            process_on_nodes=[student_gpus] * n_nodes,\n",
    "            use_gpu=True,\n",
    "            max_colocate_count=1,\n",
    "            name_prefix=\"student_pool\"\n",
    "        )\n",
    "        \n",
    "        teacher_pool = RayResourcePool(\n",
    "            process_on_nodes=[teacher_gpus] * n_nodes,\n",
    "            use_gpu=True,\n",
    "            max_colocate_count=1,\n",
    "            name_prefix=\"teacher_pool\"\n",
    "        )\n",
    "        \n",
    "        self.resource_pool_to_cls = {student_pool: {}, teacher_pool: {}}\n",
    "\n",
    "        # 3. Initialize Student (Actor) Config\n",
    "        # Deepcopy to prevent config pollution\n",
    "        student_config = deepcopy(self.config.actor_rollout_ref)\n",
    "        # Adjust micro_batch_size if necessary (optional, but good practice since world_size changed)\n",
    "        # student_config.actor.ppo_mini_batch_size //= 2 # Logic handled by worker usually\n",
    "        \n",
    "        student_cls = RayClassWithInitArgs(\n",
    "            cls=self.role_worker_mapping[Role.ActorRollout],\n",
    "            config=student_config,\n",
    "            role=\"actor_rollout\",\n",
    "            profile_option=self.config.trainer.npu_profile.options,\n",
    "        )\n",
    "        self.resource_pool_to_cls[student_pool][\"actor_rollout\"] = student_cls\n",
    "\n",
    "        # 4. Initialize Teacher (Ref) Config\n",
    "        # We define it as 'actor_rollout' role to FORCE vLLM initialization.\n",
    "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
    "        \n",
    "        teacher_cls = RayClassWithInitArgs(\n",
    "            cls=self.role_worker_mapping[Role.RefPolicy],\n",
    "            config=teacher_config,\n",
    "            role=\"actor_rollout\", # <--- Hack: Pretend to be Actor to enable vLLM\n",
    "            profile_option=self.config.trainer.npu_profile.options,\n",
    "        )\n",
    "        self.resource_pool_to_cls[teacher_pool][\"ref\"] = teacher_cls\n",
    "\n",
    "        # 5. Spawn Workers\n",
    "        all_wg = {}\n",
    "        wg_kwargs = {\n",
    "            \"ray_wait_register_center_timeout\": self.config.trainer.ray_wait_register_center_timeout,\n",
    "            \"device_name\": self.device_name\n",
    "        }\n",
    "        \n",
    "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
    "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
    "            wg_dict = self.ray_worker_group_cls(\n",
    "                resource_pool=resource_pool,\n",
    "                ray_cls_with_init=worker_dict_cls,\n",
    "                **wg_kwargs,\n",
    "            )\n",
    "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
    "            all_wg.update(spawn_wg)\n",
    "\n",
    "        # 6. Bind to Trainer\n",
    "        self.actor_rollout_wg = all_wg[\"actor_rollout\"]\n",
    "        self.actor_rollout_wg.init_model()\n",
    "        \n",
    "        self.ref_policy_wg = all_wg[\"ref\"]\n",
    "        self.ref_policy_wg.init_model()\n",
    "            \n",
    "        # Async Rollout Manager (Only for Student)\n",
    "        self.async_rollout_mode = False\n",
    "        if self.config.actor_rollout_ref.rollout.mode == 'async':\n",
    "            from verl.experimental.agent_loop import AgentLoopManager\n",
    "            self.async_rollout_mode = True\n",
    "            self.async_rollout_manager = AgentLoopManager(\n",
    "                config=self.config,\n",
    "                worker_group=self.actor_rollout_wg\n",
    "            )\n",
    "\n",
    "    def _prepare_summary_generation_batch(self, batch: DataProto, raw_prompts: List[str], ground_truths: List[str]) -> DataProto:\n",
    "        \"\"\"\n",
    "        构造用于生成总结 (Reflection) 的 Batch。\n",
    "        包含: 强力正则清洗，去除 System Prompt 和嵌套标记。\n",
    "        \"\"\"\n",
    "        responses = batch.batch['responses']\n",
    "        \n",
    "        assert len(raw_prompts) == len(responses)\n",
    "        assert len(ground_truths) == len(responses)\n",
    "\n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "\n",
    "        for i, (r_ids, gt_text) in enumerate(zip(responses, ground_truths)):\n",
    "            p_text_dirty = raw_prompts[i]\n",
    "            \n",
    "            # --- 正则清洗逻辑 (提取纯 User Query) ---\n",
    "            p_text_clean = p_text_dirty\n",
    "            \n",
    "            # 1. 尝试截取最后一个 User 标记之后的内容\n",
    "            split_patterns = [\n",
    "                r\"<\\|im_start\\|>user\\s*\",  # Qwen/ChatML\n",
    "                r\"user\\s*\\n\",              # Qwen decode\n",
    "                r\"User:\\s*\",               # Common\n",
    "                r\"\\[INST\\]\\s*\",            # Llama\n",
    "                r\"Human:\\s*\"               # Anthropic\n",
    "            ]\n",
    "            \n",
    "            found_user = False\n",
    "            for pattern in split_patterns:\n",
    "                matches = list(re.finditer(pattern, p_text_dirty, re.IGNORECASE))\n",
    "                if matches:\n",
    "                    last_match = matches[-1]\n",
    "                    p_text_clean = p_text_dirty[last_match.end():]\n",
    "                    found_user = True\n",
    "                    break\n",
    "            \n",
    "            # 2. 移除可能残留的 Assistant 标记 (Prompt 结尾)\n",
    "            stop_patterns = [\n",
    "                r\"<\\|im_start\\|>assistant\",\n",
    "                r\"assistant\\s*\\n\",\n",
    "                r\"Assistant:\",\n",
    "                r\"\\[/INST\\]\"\n",
    "            ]\n",
    "            for pattern in stop_patterns:\n",
    "                match = re.search(pattern, p_text_clean, re.IGNORECASE) # 使用 escape 防止正则错误\n",
    "                if match:\n",
    "                    p_text_clean = p_text_clean[:match.start()]\n",
    "            \n",
    "            # 3. 移除可能残留的 User 结束标记 (如 <|im_end|>)\n",
    "            if \"<|im_end|>\" in p_text_clean:\n",
    "                p_text_clean = p_text_clean.replace(\"<|im_end|>\", \"\")\n",
    "            \n",
    "            p_text_clean = p_text_clean.strip()\n",
    "            # ----------------------------------------\n",
    "\n",
    "            r_text = self.tokenizer.decode(r_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Prompt for the Teacher (Summary Generation)\n",
    "            content = (\n",
    "                f\"Question: {p_text_clean}\\n\\n\"\n",
    "                f\"Standard Answer: {gt_text}\\n\\n\"\n",
    "                f\"Student Answer: {r_text}\\n\\n\"\n",
    "                f\"Task: Verify the Student Answer step-by-step. Is it correct?\\n\"\n",
    "                f\"Answer concisely.\"\n",
    "            )\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": content}\n",
    "            ]\n",
    "\n",
    "            enc_ids = self.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            if enc_ids.dim() == 2:\n",
    "                enc_ids = enc_ids[0]\n",
    "\n",
    "            input_ids_list.append(enc_ids)\n",
    "            attention_mask_list.append(torch.ones_like(enc_ids))\n",
    "\n",
    "        # --- Manual Left Padding & Position IDs ---\n",
    "        max_len = max([len(t) for t in input_ids_list])\n",
    "        pad_token_id = self.tokenizer.pad_token_id\n",
    "        \n",
    "        padded_input_ids = []\n",
    "        padded_attention_mask = []\n",
    "        padded_position_ids = []\n",
    "        \n",
    "        for ids, mask in zip(input_ids_list, attention_mask_list):\n",
    "            pad_len = max_len - len(ids)\n",
    "            \n",
    "            pad_ids = torch.full((pad_len,), pad_token_id, dtype=ids.dtype, device=ids.device)\n",
    "            pad_mask = torch.full((pad_len,), 0, dtype=mask.dtype, device=mask.device)\n",
    "            \n",
    "            final_ids = torch.cat([pad_ids, ids])\n",
    "            final_mask = torch.cat([pad_mask, mask])\n",
    "            \n",
    "            padded_input_ids.append(final_ids)\n",
    "            padded_attention_mask.append(final_mask)\n",
    "            \n",
    "            seq_len = len(ids)\n",
    "            pos_content = torch.arange(seq_len, dtype=torch.long, device=ids.device)\n",
    "            pos_pad = torch.zeros(pad_len, dtype=torch.long, device=ids.device)\n",
    "            final_pos = torch.cat([pos_pad, pos_content])\n",
    "            \n",
    "            padded_position_ids.append(final_pos)\n",
    "            \n",
    "        input_ids = torch.stack(padded_input_ids)\n",
    "        attention_mask = torch.stack(padded_attention_mask)\n",
    "        position_ids = torch.stack(padded_position_ids)\n",
    "\n",
    "        summary_batch = DataProto.from_dict({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids\n",
    "        })\n",
    "        \n",
    "        summary_batch.meta_info = {\n",
    "            \"do_sample\": True,\n",
    "            \"temperature\": 0.1, \n",
    "            \"max_new_tokens\": 512,\n",
    "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id\n",
    "        }\n",
    "        \n",
    "        return summary_batch\n",
    "\n",
    "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor) -> DataProto:\n",
    "        \"\"\"\n",
    "        构造 Teacher Forward 的 Batch。\n",
    "        逻辑：[Question] + [Summary (Hint)] -> Teacher 预测 [Correct Response]\n",
    "        实际输入：[Question] + [Summary] + [Student Response]\n",
    "        \n",
    "        如果 Student Response 是错的，Teacher (试图生成正确答案) 会给出低 LogProb。\n",
    "        如果 Student Response 是对的，Teacher (试图生成正确答案) 会给出高 LogProb。\n",
    "        \"\"\"\n",
    "        prompts = batch.batch['prompts']\n",
    "        responses = batch.batch['responses']\n",
    "        \n",
    "        new_input_ids = []\n",
    "        new_attention_mask = []\n",
    "        new_response_masks = []\n",
    "        new_prompts_list = []\n",
    "        new_responses_list = []\n",
    "\n",
    "        # 分隔符\n",
    "        sep_ids = self.tokenizer.encode(\"\\n\\nSolution:\\n\", add_special_tokens=False)\n",
    "        separator = torch.tensor(sep_ids, device=prompts.device)\n",
    "\n",
    "        for i in range(len(prompts)):\n",
    "            # 1. 解码 Prompt (包含 System + User Query)\n",
    "            p_ids = prompts[i]\n",
    "            p_ids = p_ids[p_ids != self.tokenizer.pad_token_id]\n",
    "            p_text_dirty = self.tokenizer.decode(p_ids, skip_special_tokens=False)\n",
    "            \n",
    "            # --- 强力清洗逻辑 (只提取 User Query) ---\n",
    "            p_text_clean = p_text_dirty\n",
    "            # 尝试截取最后一个 User 标记之后的内容\n",
    "            split_patterns = [r\"<\\|im_start\\|>user\\s*\", r\"user\\s*\\n\", r\"User:\\s*\", r\"\\[INST\\]\\s*\", r\"Human:\\s*\"]\n",
    "            for pattern in split_patterns:\n",
    "                matches = list(re.finditer(pattern, p_text_dirty, re.IGNORECASE))\n",
    "                if matches:\n",
    "                    p_text_clean = p_text_dirty[matches[-1].end():]\n",
    "                    break\n",
    "            \n",
    "            stop_patterns = [r\"<\\|im_start\\|>assistant\", r\"assistant\\s*\\n\", r\"Assistant:\", r\"\\[/INST\\]\"]\n",
    "            for pattern in stop_patterns:\n",
    "                match = re.search(pattern, p_text_clean, re.IGNORECASE)\n",
    "                if match:\n",
    "                    p_text_clean = p_text_clean[:match.start()]\n",
    "            \n",
    "            if \"<|im_end|>\" in p_text_clean:\n",
    "                p_text_clean = p_text_clean.replace(\"<|im_end|>\", \"\")\n",
    "            \n",
    "            p_text_clean = p_text_clean.strip()\n",
    "            # ----------------------------------------\n",
    "            \n",
    "            # 2. 获取 Summary (作为 Hint/Analysis)\n",
    "            s_ids = summaries[i]\n",
    "            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
    "            s_text = self.tokenizer.decode(s_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # 3. 获取 Student Response (Target)\n",
    "            r_ids = responses[i]\n",
    "            r_ids = r_ids[r_ids != self.tokenizer.pad_token_id]\n",
    "            \n",
    "            # 4. 构造 Teacher 的 Context (Corrective Guidance)\n",
    "            # 这里的 Prompt 设计至关重要\n",
    "            teacher_content = (\n",
    "                f\"Problem: {p_text_clean}\\n\\n\"\n",
    "                f\"Analysis/Hint: {s_text}\\n\\n\"\n",
    "                f\"Based on the analysis above, please provide the correct, step-by-step solution.\"\n",
    "            )\n",
    "            \n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": teacher_content}\n",
    "            ]\n",
    "            \n",
    "            # 5. Apply Chat Template\n",
    "            prefix_ids = self.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=True, \n",
    "                add_generation_prompt=True,\n",
    "                return_tensors='pt'\n",
    "            )[0].to(r_ids.device)\n",
    "            \n",
    "            # 6. 拼接: [Prefix] + [Separator] + [Student Response]\n",
    "            # Teacher 看到的是：\n",
    "            # User: Problem + Hint + \"Please provide correct solution\"\n",
    "            # Assistant: [Student Response]\n",
    "            \n",
    "            # 如果 Student Response 是错的，Teacher 本来想生成对的，结果看到错的 -> LogProb 低\n",
    "            \n",
    "            # 注意：separator 可以直接拼在 prefix_ids 后面，或者作为 response 的开头\n",
    "            # 这里我们不加额外的 separator，因为 apply_chat_template 已经加了 assistant 标签\n",
    "            # 直接拼接 r_ids 即可\n",
    "            \n",
    "            teacher_full = torch.cat([prefix_ids, r_ids])\n",
    "            \n",
    "            # 7. Masks\n",
    "            att_mask = torch.ones_like(teacher_full)\n",
    "            resp_mask = torch.zeros_like(teacher_full)\n",
    "            \n",
    "            # 只计算 Response 部分的 LogProb\n",
    "            start_idx = len(prefix_ids)\n",
    "            resp_mask[start_idx:] = 1 \n",
    "            \n",
    "            new_input_ids.append(teacher_full)\n",
    "            new_attention_mask.append(att_mask)\n",
    "            new_response_masks.append(resp_mask)\n",
    "            \n",
    "            new_prompts_list.append(prefix_ids)\n",
    "            new_responses_list.append(r_ids)\n",
    "\n",
    "        # Padding\n",
    "        input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        attention_mask = pad_sequence(new_attention_mask, batch_first=True, padding_value=0)\n",
    "        response_mask = pad_sequence(new_response_masks, batch_first=True, padding_value=0)\n",
    "        \n",
    "        prompts_padded = pad_sequence(new_prompts_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        responses_padded = pad_sequence(new_responses_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        \n",
    "        position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 0)\n",
    "\n",
    "        teacher_batch = DataProto.from_dict({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"response_mask\": response_mask,\n",
    "            \"prompts\": prompts_padded,   \n",
    "            \"responses\": responses_padded \n",
    "        })\n",
    "        \n",
    "        return teacher_batch\n",
    "\n",
    "    def fit(self):\n",
    "        logger = Tracking(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "            default_backend=self.config.trainer.logger,\n",
    "            config=OmegaConf.to_container(self.config, resolve=True),\n",
    "        )\n",
    "\n",
    "        self.global_steps = 0\n",
    "        self._load_checkpoint()\n",
    "\n",
    "        progress_bar = tqdm(total=self.total_training_steps, initial=self.global_steps, desc=\"Training Progress\")\n",
    "        self.global_steps += 1\n",
    "\n",
    "        # === Sanity Check: Is Teacher Brain-Dead? ===\n",
    "        print(\"Running Teacher Sanity Check...\")\n",
    "        test_prompts = [\"1+1=\"]\n",
    "        test_responses = [\"2\"]\n",
    "        \n",
    "        # 1. 构造单个样本\n",
    "        p_ids = self.tokenizer.encode(test_prompts[0], return_tensors='pt')[0]\n",
    "        r_ids = self.tokenizer.encode(test_responses[0], return_tensors='pt')[0]\n",
    "        \n",
    "        input_ids = torch.cat([p_ids, r_ids])\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        position_ids = torch.arange(len(input_ids))\n",
    "        response_mask = torch.zeros_like(input_ids)\n",
    "        response_mask[len(p_ids):] = 1\n",
    "        \n",
    "        # 2. 获取 Worker 数量 (World Size)\n",
    "        # 我们需要让 batch size 能够被 world size 整除\n",
    "        world_size = self.ref_policy_wg.world_size\n",
    "        \n",
    "        # 3. 复制数据以匹配 World Size\n",
    "        input_ids = input_ids.unsqueeze(0).repeat(world_size, 1)\n",
    "        attention_mask = attention_mask.unsqueeze(0).repeat(world_size, 1)\n",
    "        position_ids = position_ids.unsqueeze(0).repeat(world_size, 1)\n",
    "        response_mask = response_mask.unsqueeze(0).repeat(world_size, 1)\n",
    "        \n",
    "        # Prompts 和 Responses 在 DataProto 中主要是占位，但也需要对齐\n",
    "        # 注意：DataProto.from_dict 对 tensor 的处理比较严格，这里我们只放必要的 keys\n",
    "        # 为了避免复杂，我们只放 input_ids 等核心 tensor，prompts/responses 可以留空或者也 repeat\n",
    "        # compute_log_prob 主要依赖 input_ids, attention_mask, position_ids, responses(用于mask)\n",
    "        \n",
    "        # 重新构造 responses (tensor)\n",
    "        responses_tensor = r_ids.unsqueeze(0).repeat(world_size, 1)\n",
    "\n",
    "        sanity_batch = DataProto.from_dict({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"response_mask\": response_mask,\n",
    "            \"responses\": responses_tensor # 必须有，用于 compute_log_prob 内部切片\n",
    "        })\n",
    "        \n",
    "        # 补充 meta_info\n",
    "        sanity_batch.meta_info = {\n",
    "            \"micro_batch_size\": world_size, # 一次性跑完\n",
    "            \"temperature\": 1.0,\n",
    "            \"use_dynamic_bsz\": False\n",
    "        }\n",
    "        \n",
    "        # Teacher Forward\n",
    "        out = self.ref_policy_wg.compute_log_prob(sanity_batch)\n",
    "        \n",
    "        # 取第一个样本的结果\n",
    "        log_prob = out.batch['old_log_probs'][0, 0].item() \n",
    "        \n",
    "        print(f\"Teacher Sanity Check: LogProb('2' | '1+1=') = {log_prob:.4f}\")\n",
    "        \n",
    "        if log_prob < -5.0:\n",
    "            print(\"!!! CRITICAL WARNING: Teacher LogProb is extremely low. Model might be randomly initialized!\")\n",
    "            # raise RuntimeError(\"Teacher Model is broken!\")\n",
    "        else:\n",
    "            print(\"Teacher seems healthy.\")\n",
    "        # ============================================\n",
    "\n",
    "        for epoch in range(self.config.trainer.total_epochs):\n",
    "            for batch_dict in self.train_dataloader:\n",
    "                metrics = {}\n",
    "                timing_raw = {}\n",
    "\n",
    "                batch: DataProto = DataProto.from_single_dict(batch_dict)\n",
    "                \n",
    "                # --- Step 1: Data Alignment ---\n",
    "                B = batch.batch.batch_size[0]\n",
    "                N = self.config.actor_rollout_ref.rollout.n\n",
    "                \n",
    "                raw_prompts_list = []\n",
    "                for p_ids in batch.batch['input_ids']: \n",
    "                    raw_prompts_list.append(self.tokenizer.decode(p_ids, skip_special_tokens=True))\n",
    "                \n",
    "                ground_truths_list = []\n",
    "                possible_gt_keys = [\"ground_truth\", \"solution\", \"reward_model\"]\n",
    "                found_gt = False\n",
    "                for key in possible_gt_keys:\n",
    "                    if key in batch.non_tensor_batch:\n",
    "                        data = batch.non_tensor_batch[key]\n",
    "                        if key == \"reward_model\":\n",
    "                            ground_truths_list = [item.get('ground_truth', '') if isinstance(item, dict) else '' for item in data]\n",
    "                        else:\n",
    "                            ground_truths_list = data.tolist() if isinstance(data, np.ndarray) else data\n",
    "                        found_gt = True\n",
    "                        break\n",
    "                if not found_gt:\n",
    "                    ground_truths_list = [\"\"] * B\n",
    "\n",
    "                expanded_raw_prompts = [p for p in raw_prompts_list for _ in range(N)]\n",
    "                expanded_ground_truths = [g for g in ground_truths_list for _ in range(N)]\n",
    "                \n",
    "                batch_keys_to_pop = ['input_ids', 'attention_mask', 'position_ids']\n",
    "                possible_non_tensor_keys = ['raw_prompt_ids', 'multi_modal_data', 'raw_prompt', 'tools_kwargs', 'interaction_kwargs', 'index', 'agent_name']\n",
    "                non_tensor_batch_keys_to_pop = [k for k in possible_non_tensor_keys if k in batch.non_tensor_batch]\n",
    "                \n",
    "                gen_batch = batch.pop(batch_keys=batch_keys_to_pop, non_tensor_batch_keys=non_tensor_batch_keys_to_pop)\n",
    "                gen_batch.meta_info['global_steps'] = self.global_steps\n",
    "                gen_batch = gen_batch.repeat(repeat_times=N, interleave=True)\n",
    "\n",
    "                # --- Step 2: Student Generation ---\n",
    "                with marked_timer(\"gen_student\", timing_raw):\n",
    "                    if not self.async_rollout_mode:\n",
    "                        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
    "                    else:\n",
    "                        gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch)\n",
    "                    \n",
    "                    batch = batch.repeat(repeat_times=N, interleave=True)\n",
    "                    batch = batch.union(gen_batch_output)\n",
    "\n",
    "                batch.non_tensor_batch[\"uid\"] = np.array(\n",
    "                    [str(uuid.uuid4()) for _ in range(len(batch.batch))], dtype=object\n",
    "                )\n",
    "                if \"response_mask\" not in batch.batch.keys():\n",
    "                    batch.batch[\"response_mask\"] = compute_response_mask(batch)\n",
    "                batch.meta_info['global_token_num'] = torch.sum(batch.batch['attention_mask'], dim=-1).tolist()\n",
    "\n",
    "                # --- Step 3: Reflection & Reward ---\n",
    "                with marked_timer(\"reward_reflection\", timing_raw, color=\"yellow\"):\n",
    "                    # 3.1 Extract Prompts\n",
    "                    tensor_prompts = batch.batch['prompts']\n",
    "                    decoded_prompts = []\n",
    "                    for p_ids in tensor_prompts:\n",
    "                        decoded_prompts.append(self.tokenizer.decode(p_ids, skip_special_tokens=False))\n",
    "                    \n",
    "                    # 3.2 Extract GT\n",
    "                    current_ground_truths = []\n",
    "                    if \"reward_model\" in batch.non_tensor_batch:\n",
    "                        rm_data = batch.non_tensor_batch[\"reward_model\"]\n",
    "                        current_ground_truths = [item.get('ground_truth', '') if isinstance(item, dict) else '' for item in rm_data]\n",
    "                    elif \"solution\" in batch.non_tensor_batch:\n",
    "                        current_ground_truths = batch.non_tensor_batch[\"solution\"].tolist()\n",
    "                    elif \"ground_truth\" in batch.non_tensor_batch:\n",
    "                        current_ground_truths = batch.non_tensor_batch[\"ground_truth\"].tolist()\n",
    "                    else:\n",
    "                        current_ground_truths = [\"\"] * len(tensor_prompts)\n",
    "\n",
    "                    # 3.3 Teacher Generates Summary\n",
    "                    summary_input_batch = self._prepare_summary_generation_batch(\n",
    "                        batch, \n",
    "                        decoded_prompts, \n",
    "                        current_ground_truths\n",
    "                    )\n",
    "                    \n",
    "                    summary_output = self.actor_rollout_wg.generate_sequences(summary_input_batch)\n",
    "                    summaries = summary_output.batch['responses']\n",
    "                    \n",
    "                    # 3.4 Teacher Computes LogProb\n",
    "                    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries)\n",
    "                    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
    "                    teacher_full_log_probs = teacher_log_prob_output.batch['old_log_probs']\n",
    "\n",
    "                    # 3.5 Student Computes LogProb\n",
    "                    student_log_prob_output = self.actor_rollout_wg.compute_log_prob(batch)\n",
    "                    student_full_log_probs = student_log_prob_output.batch['old_log_probs']\n",
    "                    \n",
    "                    # === Log Part 1: Generation Content ===\n",
    "                    if True:\n",
    "                        print(f\"\\n{'='*20} Teacher-Student Reflection Debug (Step {self.global_steps}) {'='*20}\")\n",
    "                        try:\n",
    "                            idx = 0 \n",
    "                            # Summary Input\n",
    "                            sum_in_ids = summary_input_batch.batch['input_ids'][idx]\n",
    "                            sum_in_ids = sum_in_ids[sum_in_ids != self.tokenizer.pad_token_id]\n",
    "                            sum_in_text = self.tokenizer.decode(sum_in_ids, skip_special_tokens=False)\n",
    "                            \n",
    "                            # Summary Output\n",
    "                            s_ids = summaries[idx]\n",
    "                            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
    "                            s_text = self.tokenizer.decode(s_ids, skip_special_tokens=True)\n",
    "                            \n",
    "                            # Teacher Input (Prompt + Summary + Response)\n",
    "                            # 这里展示 Teacher LogProb 看到的完整输入\n",
    "                            t_full_ids = teacher_batch.batch['input_ids'][idx]\n",
    "                            t_full_ids = t_full_ids[t_full_ids != self.tokenizer.pad_token_id]\n",
    "                            t_full_text = self.tokenizer.decode(t_full_ids, skip_special_tokens=False)\n",
    "\n",
    "                            print(f\"--- [0] Summary Generation Input ---\\n{sum_in_text.strip()}\\n\")\n",
    "                            print(f\"--- [1] Summary Output ---\\n{s_text.strip()}\\n\")\n",
    "                            print(f\"--- [2] Teacher LogProb Input (Full Context) ---\\n{t_full_text.strip()}\\n\") # 只看最后500字符\n",
    "                        except Exception as e:\n",
    "                            print(f\"Log Error 1: {e}\")\n",
    "                        print(f\"{'='*60}\\n\")\n",
    "                    # ======================================\n",
    "\n",
    "                    # 3.6 Reward Calculation\n",
    "                    token_level_rewards = torch.zeros_like(student_full_log_probs)\n",
    "                    s_probs = student_full_log_probs\n",
    "                    t_probs = teacher_full_log_probs\n",
    "                    \n",
    "                    min_len = min(s_probs.shape[1], t_probs.shape[1])\n",
    "                    s_part = s_probs[:, :min_len]\n",
    "                    t_part = t_probs[:, :min_len]\n",
    "                    \n",
    "                    kl_diff = t_part - s_part\n",
    "                    \n",
    "                    # === Log Part 2: Token-Level KL Analysis ===\n",
    "                    if True:\n",
    "                        print(f\"\\n{'='*20} Token-Level KL Analysis (Step {self.global_steps}) {'='*20}\")\n",
    "                        try:\n",
    "                            idx = 0 \n",
    "                            # 1. Student Response (Remove Pad)\n",
    "                            s_ids = batch.batch['responses'][idx]\n",
    "                            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
    "                            \n",
    "                            # 2. Teacher Response (Extract using Mask)\n",
    "                            t_full_ids = teacher_batch.batch['input_ids'][idx]\n",
    "                            t_mask = teacher_batch.batch['response_mask'][idx]\n",
    "                            t_ids = t_full_ids[t_mask.bool()]\n",
    "                            t_ids = t_ids[t_ids != self.tokenizer.pad_token_id]\n",
    "                            \n",
    "                            # 3. Decode\n",
    "                            s_text_check = self.tokenizer.decode(s_ids, skip_special_tokens=False)\n",
    "                            t_text_check = self.tokenizer.decode(t_ids, skip_special_tokens=False)\n",
    "                            \n",
    "                            print(f\"--- Sequence Alignment Check ---\")\n",
    "\n",
    "                            \n",
    "                            if len(s_ids) != len(t_ids) or not torch.equal(s_ids, t_ids):\n",
    "                                print(\"[WARNING] ID Mismatch! Teacher sees different tokens than Student generated!\")\n",
    "                                print(f\"S IDs: {s_ids.tolist()}\")\n",
    "                                print(f\"T IDs: {t_ids.tolist()}\")\n",
    "                                print(f\"Student Seq (Len={len(s_ids)}): {s_text_check}...\")\n",
    "                                print(f\"Teacher Seq (Len={len(t_ids)}): {t_text_check}...\")\n",
    "                            else:\n",
    "                                print(\"[OK] Token IDs match perfectly.\")\n",
    "\n",
    "                            print(f\"\\n--- Token-wise KL Breakdown (First 50 tokens) ---\")\n",
    "                            print(f\"{'Token':<15} | {'ID':<6} | {'S_LogP':<8} | {'T_LogP':<8} | {'Diff':<8}\")\n",
    "                            print(\"-\" * 75)\n",
    "                            \n",
    "                            s_vals = s_part[idx].tolist()\n",
    "                            t_vals = t_part[idx].tolist()\n",
    "                            diff_vals = kl_diff[idx].tolist()\n",
    "                            \n",
    "                            valid_count = 0\n",
    "                            for i in range(len(s_ids)):\n",
    "                                if valid_count >= 50: break\n",
    "                                if i >= len(s_vals): break\n",
    "                                \n",
    "                                tid = s_ids[i].item()\n",
    "                                token_str = self.tokenizer.decode([tid]).replace('\\n', '\\\\n')\n",
    "                                print(f\"{token_str:<15} | {tid:<6} | {s_vals[i]:.2f}   | {t_vals[i]:.2f}   | {diff_vals[i]:.2f}\")\n",
    "                                valid_count += 1\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            print(f\"Log Error 2: {e}\")\n",
    "                        print(f\"{'='*60}\\n\")\n",
    "                    # ==========================================\n",
    "\n",
    "                    kl_diff = torch.clamp(kl_diff, min=-1.0, max=1.0)\n",
    "                    token_level_rewards[:, :min_len] = kl_diff\n",
    "                    \n",
    "                    batch.batch['token_level_rewards'] = token_level_rewards\n",
    "                    \n",
    "                    valid_mask = batch.batch['response_mask']\n",
    "                    with torch.no_grad():\n",
    "                        mean_kl = (token_level_rewards * valid_mask).sum() / (valid_mask.sum() + 1e-6)\n",
    "                        metrics['reward/reflection_kl'] = mean_kl.item()\n",
    "\n",
    "                # --- Step 4: PPO Flow ---\n",
    "                entropys = student_log_prob_output.batch['entropys']\n",
    "                entropy_agg = torch.mean(entropys)\n",
    "                metrics['actor/entropy'] = entropy_agg.item()\n",
    "                \n",
    "                student_log_prob_output.batch.pop('entropys', None)\n",
    "                batch = batch.union(student_log_prob_output)\n",
    "\n",
    "                if \"values\" not in batch.batch.keys():\n",
    "                    batch.batch[\"values\"] = torch.zeros_like(batch.batch[\"token_level_rewards\"])\n",
    "\n",
    "                with marked_timer(\"adv\", timing_raw):\n",
    "                    batch = compute_advantage(\n",
    "                        batch,\n",
    "                        adv_estimator=self.config.algorithm.adv_estimator,\n",
    "                        gamma=self.config.algorithm.gamma,\n",
    "                        lam=self.config.algorithm.lam,\n",
    "                        num_repeat=self.config.actor_rollout_ref.rollout.n,\n",
    "                        norm_adv_by_std_in_grpo=self.config.algorithm.get('norm_adv_by_std_in_grpo', True),\n",
    "                        config=self.config.algorithm\n",
    "                    )\n",
    "                \n",
    "                batch.batch['advantages'] = batch.batch['token_level_rewards']\n",
    "\n",
    "                with marked_timer(\"update_actor\", timing_raw):\n",
    "                    actor_output = self.actor_rollout_wg.update_actor(batch)\n",
    "                    actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n",
    "                    metrics.update(actor_output_metrics)\n",
    "\n",
    "                metrics.update({\n",
    "                    \"training/global_step\": self.global_steps,\n",
    "                    \"training/epoch\": epoch\n",
    "                })\n",
    "                \n",
    "                logger.log(data=metrics, step=self.global_steps)\n",
    "                progress_bar.update(1)\n",
    "                self.global_steps += 1\n",
    "                \n",
    "                if self.config.trainer.save_freq > 0 and self.global_steps % self.config.trainer.save_freq == 0:\n",
    "                    self._save_checkpoint()\n",
    "\n",
    "                if self.global_steps >= self.total_training_steps:\n",
    "                    progress_bar.close()\n",
    "                    return\n",
    "\"\"\"\n",
    "ray_trainer.py的代码如下\n",
    "\"\"\"\n",
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "# Copyright 2023-2024 SGLang Team\n",
    "# Copyright 2025 ModelBest Inc. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "PPO Trainer with Ray-based single controller.\n",
    "This trainer supports model-agonistic model initialization with huggingface\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "from torchdata.stateful_dataloader import StatefulDataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from verl import DataProto\n",
    "from verl.experimental.dataset.sampler import AbstractCurriculumSampler\n",
    "from verl.protocol import pad_dataproto_to_divisor, unpad_dataproto\n",
    "from verl.single_controller.base import Worker\n",
    "from verl.single_controller.ray import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\n",
    "from verl.single_controller.ray.base import create_colocated_worker_cls\n",
    "from verl.trainer.config import AlgoConfig\n",
    "from verl.trainer.ppo import core_algos\n",
    "from verl.trainer.ppo.core_algos import AdvantageEstimator, agg_loss\n",
    "from verl.trainer.ppo.metric_utils import (\n",
    "    compute_data_metrics,\n",
    "    compute_throughout_metrics,\n",
    "    compute_timing_metrics,\n",
    "    process_validation_metrics,\n",
    ")\n",
    "from verl.trainer.ppo.reward import compute_reward, compute_reward_async\n",
    "from verl.utils.checkpoint.checkpoint_manager import find_latest_ckpt_path, should_save_ckpt_esi\n",
    "from verl.utils.debug import marked_timer\n",
    "from verl.utils.metric import (\n",
    "    reduce_metrics,\n",
    ")\n",
    "from verl.utils.seqlen_balancing import get_seqlen_balanced_partitions, log_seqlen_unbalance\n",
    "from verl.utils.torch_functional import masked_mean\n",
    "from verl.utils.tracking import ValidationGenerationsLogger\n",
    "\n",
    "WorkerType = type[Worker]\n",
    "\n",
    "\n",
    "class Role(Enum):\n",
    "    \"\"\"\n",
    "    To create more roles dynamically, you can subclass Role and add new members\n",
    "    \"\"\"\n",
    "\n",
    "    Actor = 0\n",
    "    Rollout = 1\n",
    "    ActorRollout = 2\n",
    "    Critic = 3\n",
    "    RefPolicy = 4\n",
    "    RewardModel = 5\n",
    "    ActorRolloutRef = 6\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResourcePoolManager:\n",
    "    \"\"\"\n",
    "    Define a resource pool specification. Resource pool will be initialized first.\n",
    "    \"\"\"\n",
    "\n",
    "    resource_pool_spec: dict[str, list[int]]\n",
    "    mapping: dict[Role, str]\n",
    "    resource_pool_dict: dict[str, RayResourcePool] = field(default_factory=dict)\n",
    "\n",
    "    def create_resource_pool(self):\n",
    "        \"\"\"Create Ray resource pools for distributed training.\n",
    "\n",
    "        Initializes resource pools based on the resource pool specification,\n",
    "        with each pool managing GPU resources across multiple nodes.\n",
    "        For FSDP backend, uses max_colocate_count=1 to merge WorkerGroups.\n",
    "        For Megatron backend, uses max_colocate_count>1 for different models.\n",
    "        \"\"\"\n",
    "        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():\n",
    "            # max_colocate_count means the number of WorkerGroups (i.e. processes) in each RayResourcePool\n",
    "            # For FSDP backend, we recommend using max_colocate_count=1 that merge all WorkerGroups into one.\n",
    "            # For Megatron backend, we recommend using max_colocate_count>1\n",
    "            # that can utilize different WorkerGroup for differnt models\n",
    "            resource_pool = RayResourcePool(\n",
    "                process_on_nodes=process_on_nodes, use_gpu=True, max_colocate_count=1, name_prefix=resource_pool_name\n",
    "            )\n",
    "            self.resource_pool_dict[resource_pool_name] = resource_pool\n",
    "\n",
    "        self._check_resource_available()\n",
    "\n",
    "    def get_resource_pool(self, role: Role) -> RayResourcePool:\n",
    "        \"\"\"Get the resource pool of the worker_cls\"\"\"\n",
    "        return self.resource_pool_dict[self.mapping[role]]\n",
    "\n",
    "    def get_n_gpus(self) -> int:\n",
    "        \"\"\"Get the number of gpus in this cluster.\"\"\"\n",
    "        return sum([n_gpus for process_on_nodes in self.resource_pool_spec.values() for n_gpus in process_on_nodes])\n",
    "\n",
    "    def _check_resource_available(self):\n",
    "        \"\"\"Check if the resource pool can be satisfied in this ray cluster.\"\"\"\n",
    "        node_available_resources = ray.state.available_resources_per_node()\n",
    "        node_available_gpus = {\n",
    "            node: node_info.get(\"GPU\", 0) if \"GPU\" in node_info else node_info.get(\"NPU\", 0)\n",
    "            for node, node_info in node_available_resources.items()\n",
    "        }\n",
    "\n",
    "        # check total required gpus can be satisfied\n",
    "        total_available_gpus = sum(node_available_gpus.values())\n",
    "        total_required_gpus = sum(\n",
    "            [n_gpus for process_on_nodes in self.resource_pool_spec.values() for n_gpus in process_on_nodes]\n",
    "        )\n",
    "        if total_available_gpus < total_required_gpus:\n",
    "            raise ValueError(\n",
    "                f\"Total available GPUs {total_available_gpus} is less than total desired GPUs {total_required_gpus}\"\n",
    "            )\n",
    "\n",
    "        # check each resource pool can be satisfied, O(#resource_pools * #nodes)\n",
    "        for resource_pool_name, process_on_nodes in self.resource_pool_spec.items():\n",
    "            num_gpus, num_nodes = process_on_nodes[0], len(process_on_nodes)\n",
    "            for node, available_gpus in node_available_gpus.items():\n",
    "                if available_gpus >= num_gpus:\n",
    "                    node_available_gpus[node] -= num_gpus\n",
    "                    num_nodes -= 1\n",
    "                    if num_nodes == 0:\n",
    "                        break\n",
    "            if num_nodes > 0:\n",
    "                raise ValueError(\n",
    "                    f\"Resource pool {resource_pool_name}: {num_gpus}*{num_nodes}\"\n",
    "                    + \"cannot be satisfied in this ray cluster\"\n",
    "                )\n",
    "\n",
    "\n",
    "def apply_kl_penalty(data: DataProto, kl_ctrl: core_algos.AdaptiveKLController, kl_penalty=\"kl\"):\n",
    "    \"\"\"Apply KL penalty to the token-level rewards.\n",
    "\n",
    "    This function computes the KL divergence between the reference policy and current policy,\n",
    "    then applies a penalty to the token-level rewards based on this divergence.\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): The data containing batched model outputs and inputs.\n",
    "        kl_ctrl (core_algos.AdaptiveKLController): Controller for adaptive KL penalty.\n",
    "        kl_penalty (str, optional): Type of KL penalty to apply. Defaults to \"kl\".\n",
    "        multi_turn (bool, optional): Whether the data is from a multi-turn conversation. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - The updated data with token-level rewards adjusted by KL penalty\n",
    "            - A dictionary of metrics related to the KL penalty\n",
    "    \"\"\"\n",
    "    response_mask = data.batch[\"response_mask\"]\n",
    "    token_level_scores = data.batch[\"token_level_scores\"]\n",
    "    batch_size = data.batch.batch_size[0]\n",
    "\n",
    "    # compute kl between ref_policy and current policy\n",
    "    # When apply_kl_penalty, algorithm.use_kl_in_reward=True, so the reference model has been enabled.\n",
    "    kld = core_algos.kl_penalty(\n",
    "        data.batch[\"old_log_probs\"], data.batch[\"ref_log_prob\"], kl_penalty=kl_penalty\n",
    "    )  # (batch_size, response_length)\n",
    "    kld = kld * response_mask\n",
    "    beta = kl_ctrl.value\n",
    "\n",
    "    token_level_rewards = token_level_scores - beta * kld\n",
    "\n",
    "    current_kl = masked_mean(kld, mask=response_mask, axis=-1)  # average over sequence\n",
    "    current_kl = torch.mean(current_kl, dim=0).item()\n",
    "\n",
    "    # according to https://github.com/huggingface/trl/blob/951ca1841f29114b969b57b26c7d3e80a39f75a0/trl/trainer/ppo_trainer.py#L837\n",
    "    kl_ctrl.update(current_kl=current_kl, n_steps=batch_size)\n",
    "    data.batch[\"token_level_rewards\"] = token_level_rewards\n",
    "\n",
    "    metrics = {\"actor/reward_kl_penalty\": current_kl, \"actor/reward_kl_penalty_coeff\": beta}\n",
    "\n",
    "    return data, metrics\n",
    "\n",
    "\n",
    "def compute_response_mask(data: DataProto):\n",
    "    \"\"\"Compute the attention mask for the response part of the sequence.\n",
    "\n",
    "    This function extracts the portion of the attention mask that corresponds to the model's response,\n",
    "    which is used for masking computations that should only apply to response tokens.\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): The data containing batched model outputs and inputs.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The attention mask for the response tokens.\n",
    "    \"\"\"\n",
    "    responses = data.batch[\"responses\"]\n",
    "    response_length = responses.size(1)\n",
    "    attention_mask = data.batch[\"attention_mask\"]\n",
    "    return attention_mask[:, -response_length:]\n",
    "\n",
    "\n",
    "def compute_advantage(\n",
    "    data: DataProto,\n",
    "    adv_estimator: AdvantageEstimator,\n",
    "    gamma: float = 1.0,\n",
    "    lam: float = 1.0,\n",
    "    num_repeat: int = 1,\n",
    "    norm_adv_by_std_in_grpo: bool = True,\n",
    "    config: Optional[AlgoConfig] = None,\n",
    ") -> DataProto:\n",
    "    \"\"\"Compute advantage estimates for policy optimization.\n",
    "\n",
    "    This function computes advantage estimates using various estimators like GAE, GRPO, REINFORCE++, etc.\n",
    "    The advantage estimates are used to guide policy optimization in RL algorithms.\n",
    "\n",
    "    Args:\n",
    "        data (DataProto): The data containing batched model outputs and inputs.\n",
    "        adv_estimator (AdvantageEstimator): The advantage estimator to use (e.g., GAE, GRPO, REINFORCE++).\n",
    "        gamma (float, optional): Discount factor for future rewards. Defaults to 1.0.\n",
    "        lam (float, optional): Lambda parameter for GAE. Defaults to 1.0.\n",
    "        num_repeat (int, optional): Number of times to repeat the computation. Defaults to 1.\n",
    "        norm_adv_by_std_in_grpo (bool, optional): Whether to normalize advantages by standard deviation in\n",
    "            GRPO. Defaults to True.\n",
    "        config (dict, optional): Configuration dictionary for algorithm settings. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        DataProto: The updated data with computed advantages and returns.\n",
    "    \"\"\"\n",
    "    # Back-compatible with trainers that do not compute response mask in fit\n",
    "    if \"response_mask\" not in data.batch.keys():\n",
    "        data.batch[\"response_mask\"] = compute_response_mask(data)\n",
    "    # prepare response group\n",
    "    if adv_estimator == AdvantageEstimator.GAE:\n",
    "        # Compute advantages and returns using Generalized Advantage Estimation (GAE)\n",
    "        advantages, returns = core_algos.compute_gae_advantage_return(\n",
    "            token_level_rewards=data.batch[\"token_level_rewards\"],\n",
    "            values=data.batch[\"values\"],\n",
    "            response_mask=data.batch[\"response_mask\"],\n",
    "            gamma=gamma,\n",
    "            lam=lam,\n",
    "        )\n",
    "        data.batch[\"advantages\"] = advantages\n",
    "        data.batch[\"returns\"] = returns\n",
    "        if config.get(\"use_pf_ppo\", False):\n",
    "            data = core_algos.compute_pf_ppo_reweight_data(\n",
    "                data,\n",
    "                config.pf_ppo.reweight_method,\n",
    "                config.pf_ppo.weight_pow,\n",
    "            )\n",
    "    elif adv_estimator == AdvantageEstimator.GRPO:\n",
    "        # Initialize the mask for GRPO calculation\n",
    "        grpo_calculation_mask = data.batch[\"response_mask\"]\n",
    "        # Call compute_grpo_outcome_advantage with parameters matching its definition\n",
    "        advantages, returns = core_algos.compute_grpo_outcome_advantage(\n",
    "            token_level_rewards=data.batch[\"token_level_rewards\"],\n",
    "            response_mask=grpo_calculation_mask,\n",
    "            index=data.non_tensor_batch[\"uid\"],\n",
    "            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n",
    "        )\n",
    "        data.batch[\"advantages\"] = advantages\n",
    "        data.batch[\"returns\"] = returns\n",
    "    else:\n",
    "        # handle all other adv estimator type other than GAE and GRPO\n",
    "        adv_estimator_fn = core_algos.get_adv_estimator_fn(adv_estimator)\n",
    "        adv_kwargs = {\n",
    "            \"token_level_rewards\": data.batch[\"token_level_rewards\"],\n",
    "            \"response_mask\": data.batch[\"response_mask\"],\n",
    "            \"config\": config,\n",
    "        }\n",
    "        if \"uid\" in data.non_tensor_batch:  # optional\n",
    "            adv_kwargs[\"index\"] = data.non_tensor_batch[\"uid\"]\n",
    "        if \"reward_baselines\" in data.batch:  # optional\n",
    "            adv_kwargs[\"reward_baselines\"] = data.batch[\"reward_baselines\"]\n",
    "\n",
    "        # calculate advantage estimator\n",
    "        advantages, returns = adv_estimator_fn(**adv_kwargs)\n",
    "        data.batch[\"advantages\"] = advantages\n",
    "        data.batch[\"returns\"] = returns\n",
    "    return data\n",
    "\n",
    "\n",
    "class RayPPOTrainer:\n",
    "    \"\"\"Distributed PPO trainer using Ray for scalable reinforcement learning.\n",
    "\n",
    "    This trainer orchestrates distributed PPO training across multiple nodes and GPUs,\n",
    "    managing actor rollouts, critic training, and reward computation with Ray backend.\n",
    "    Supports various model architectures including FSDP, Megatron, and vLLM integration.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: support each role have individual ray_worker_group_cls,\n",
    "    # i.e., support different backend of different role\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        tokenizer,\n",
    "        role_worker_mapping: dict[Role, WorkerType],\n",
    "        resource_pool_manager: ResourcePoolManager,\n",
    "        ray_worker_group_cls: RayWorkerGroup = RayWorkerGroup,\n",
    "        processor=None,\n",
    "        reward_fn=None,\n",
    "        val_reward_fn=None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        val_dataset: Optional[Dataset] = None,\n",
    "        collate_fn=None,\n",
    "        train_sampler: Optional[Sampler] = None,\n",
    "        device_name=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize distributed PPO trainer with Ray backend.\n",
    "        Note that this trainer runs on the driver process on a single CPU/GPU node.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object containing training parameters.\n",
    "            tokenizer: Tokenizer used for encoding and decoding text.\n",
    "            role_worker_mapping (dict[Role, WorkerType]): Mapping from roles to worker classes.\n",
    "            resource_pool_manager (ResourcePoolManager): Manager for Ray resource pools.\n",
    "            ray_worker_group_cls (RayWorkerGroup, optional): Class for Ray worker groups. Defaults to RayWorkerGroup.\n",
    "            processor: Optional data processor, used for multimodal data\n",
    "            reward_fn: Function for computing rewards during training.\n",
    "            val_reward_fn: Function for computing rewards during validation.\n",
    "            train_dataset (Optional[Dataset], optional): Training dataset. Defaults to None.\n",
    "            val_dataset (Optional[Dataset], optional): Validation dataset. Defaults to None.\n",
    "            collate_fn: Function to collate data samples into batches.\n",
    "            train_sampler (Optional[Sampler], optional): Sampler for the training dataset. Defaults to None.\n",
    "            device_name (str, optional): Device name for training (e.g., \"cuda\", \"cpu\"). Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the tokenizer for text processing\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.config = config\n",
    "        self.reward_fn = reward_fn\n",
    "        self.val_reward_fn = val_reward_fn\n",
    "\n",
    "        self.hybrid_engine = config.actor_rollout_ref.hybrid_engine\n",
    "        assert self.hybrid_engine, \"Currently, only support hybrid engine\"\n",
    "\n",
    "        if self.hybrid_engine:\n",
    "            assert Role.ActorRollout in role_worker_mapping, f\"{role_worker_mapping.keys()=}\"\n",
    "\n",
    "        self.role_worker_mapping = role_worker_mapping\n",
    "        self.resource_pool_manager = resource_pool_manager\n",
    "        self.use_reference_policy = Role.RefPolicy in role_worker_mapping\n",
    "        self.use_rm = Role.RewardModel in role_worker_mapping\n",
    "        self.ray_worker_group_cls = ray_worker_group_cls\n",
    "        self.device_name = device_name if device_name else self.config.trainer.device\n",
    "        self.validation_generations_logger = ValidationGenerationsLogger(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "        )\n",
    "\n",
    "        # if ref_in_actor is True, the reference policy will be actor without lora applied\n",
    "        self.ref_in_actor = config.actor_rollout_ref.model.get(\"lora_rank\", 0) > 0\n",
    "\n",
    "        # define in-reward KL control\n",
    "        # kl loss control currently not suppoorted\n",
    "        if self.config.algorithm.use_kl_in_reward:\n",
    "            self.kl_ctrl_in_reward = core_algos.get_kl_controller(self.config.algorithm.kl_ctrl)\n",
    "\n",
    "        if self.config.algorithm.adv_estimator == AdvantageEstimator.GAE:\n",
    "            self.use_critic = True\n",
    "        elif self.config.algorithm.adv_estimator in [\n",
    "            AdvantageEstimator.GRPO,\n",
    "            AdvantageEstimator.GRPO_PASSK,\n",
    "            AdvantageEstimator.REINFORCE_PLUS_PLUS,\n",
    "            AdvantageEstimator.REMAX,\n",
    "            AdvantageEstimator.RLOO,\n",
    "            AdvantageEstimator.OPO,\n",
    "            AdvantageEstimator.REINFORCE_PLUS_PLUS_BASELINE,\n",
    "            AdvantageEstimator.GPG,\n",
    "        ]:\n",
    "            self.use_critic = False\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self._validate_config()\n",
    "        self._create_dataloader(train_dataset, val_dataset, collate_fn, train_sampler)\n",
    "\n",
    "    def _validate_config(self):\n",
    "        config = self.config\n",
    "        # number of GPUs total\n",
    "        n_gpus = config.trainer.n_gpus_per_node * config.trainer.nnodes\n",
    "        if config.actor_rollout_ref.actor.strategy == \"megatron\":\n",
    "            model_parallel_size = (\n",
    "                config.actor_rollout_ref.actor.megatron.tensor_model_parallel_size\n",
    "                * config.actor_rollout_ref.actor.megatron.pipeline_model_parallel_size\n",
    "            )\n",
    "            assert (\n",
    "                n_gpus % (model_parallel_size * config.actor_rollout_ref.actor.megatron.context_parallel_size) == 0\n",
    "            ), (\n",
    "                f\"n_gpus ({n_gpus}) must be divisible by model_parallel_size ({model_parallel_size}) times \"\n",
    "                f\"context_parallel_size ({config.actor_rollout_ref.actor.megatron.context_parallel_size})\"\n",
    "            )\n",
    "            megatron_dp = n_gpus // (\n",
    "                model_parallel_size * config.actor_rollout_ref.actor.megatron.context_parallel_size\n",
    "            )\n",
    "            minimal_bsz = megatron_dp * config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu\n",
    "        else:\n",
    "            minimal_bsz = n_gpus\n",
    "\n",
    "        # 1. Check total batch size for data correctness\n",
    "        real_train_batch_size = config.data.train_batch_size * config.actor_rollout_ref.rollout.n\n",
    "        assert real_train_batch_size % minimal_bsz == 0, (\n",
    "            f\"real_train_batch_size ({real_train_batch_size}) must be divisible by minimal possible batch size \"\n",
    "            f\"({minimal_bsz})\"\n",
    "        )\n",
    "\n",
    "        # A helper function to check \"micro_batch_size\" vs \"micro_batch_size_per_gpu\"\n",
    "        # We throw an error if the user sets both. The new convention is \"..._micro_batch_size_per_gpu\".\n",
    "        def check_mutually_exclusive(mbs, mbs_per_gpu, name: str):\n",
    "            \"\"\"Validate mutually exclusive micro batch size configuration options.\n",
    "\n",
    "            Ensures that users don't set both deprecated micro_batch_size and\n",
    "            the new micro_batch_size_per_gpu parameters simultaneously.\n",
    "\n",
    "            Args:\n",
    "                mbs: Deprecated micro batch size parameter value.\n",
    "                mbs_per_gpu: New micro batch size per GPU parameter value.\n",
    "                name (str): Configuration section name for error messages.\n",
    "\n",
    "            Raises:\n",
    "                ValueError: If both parameters are set or neither is set.\n",
    "            \"\"\"\n",
    "            settings = {\n",
    "                \"actor_rollout_ref.actor\": \"micro_batch_size\",\n",
    "                \"critic\": \"micro_batch_size\",\n",
    "                \"reward_model\": \"micro_batch_size\",\n",
    "                \"actor_rollout_ref.ref\": \"log_prob_micro_batch_size\",\n",
    "                \"actor_rollout_ref.rollout\": \"log_prob_micro_batch_size\",\n",
    "            }\n",
    "\n",
    "            if name in settings:\n",
    "                param = settings[name]\n",
    "                param_per_gpu = f\"{param}_per_gpu\"\n",
    "\n",
    "                if mbs is None and mbs_per_gpu is None:\n",
    "                    raise ValueError(\n",
    "                        f\"[{name}] Please set at least one of '{name}.{param}' or '{name}.{param_per_gpu}'.\"\n",
    "                    )\n",
    "\n",
    "                if mbs is not None and mbs_per_gpu is not None:\n",
    "                    raise ValueError(\n",
    "                        f\"[{name}] You have set both '{name}.{param}' AND '{name}.{param_per_gpu}'. Please remove \"\n",
    "                        f\"'{name}.{param}' because only '*_{param_per_gpu}' is supported (the former is deprecated).\"\n",
    "                    )\n",
    "\n",
    "        if not config.actor_rollout_ref.actor.use_dynamic_bsz:\n",
    "            # actor: ppo_micro_batch_size vs. ppo_micro_batch_size_per_gpu\n",
    "            check_mutually_exclusive(\n",
    "                config.actor_rollout_ref.actor.ppo_micro_batch_size,\n",
    "                config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu,\n",
    "                \"actor_rollout_ref.actor\",\n",
    "            )\n",
    "\n",
    "            if self.use_reference_policy:\n",
    "                # reference: log_prob_micro_batch_size vs. log_prob_micro_batch_size_per_gpu\n",
    "                check_mutually_exclusive(\n",
    "                    config.actor_rollout_ref.ref.log_prob_micro_batch_size,\n",
    "                    config.actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu,\n",
    "                    \"actor_rollout_ref.ref\",\n",
    "                )\n",
    "\n",
    "            #  The rollout section also has log_prob_micro_batch_size vs. log_prob_micro_batch_size_per_gpu\n",
    "            check_mutually_exclusive(\n",
    "                config.actor_rollout_ref.rollout.log_prob_micro_batch_size,\n",
    "                config.actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu,\n",
    "                \"actor_rollout_ref.rollout\",\n",
    "            )\n",
    "\n",
    "        if self.use_critic and not config.critic.use_dynamic_bsz:\n",
    "            # Check for critic micro-batch size conflicts\n",
    "            check_mutually_exclusive(\n",
    "                config.critic.ppo_micro_batch_size, config.critic.ppo_micro_batch_size_per_gpu, \"critic\"\n",
    "            )\n",
    "\n",
    "        # Check for reward model micro-batch size conflicts\n",
    "        if config.reward_model.enable and not config.reward_model.use_dynamic_bsz:\n",
    "            check_mutually_exclusive(\n",
    "                config.reward_model.micro_batch_size, config.reward_model.micro_batch_size_per_gpu, \"reward_model\"\n",
    "            )\n",
    "\n",
    "        # Actor\n",
    "        # check if train_batch_size is larger than ppo_mini_batch_size\n",
    "        # if NOT dynamic_bsz, we must ensure:\n",
    "        #    ppo_mini_batch_size is divisible by ppo_micro_batch_size\n",
    "        #    ppo_micro_batch_size * sequence_parallel_size >= n_gpus\n",
    "        if not config.actor_rollout_ref.actor.use_dynamic_bsz:\n",
    "            assert config.data.train_batch_size >= config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
    "            sp_size = config.actor_rollout_ref.actor.get(\"ulysses_sequence_parallel_size\", 1)\n",
    "            if config.actor_rollout_ref.actor.ppo_micro_batch_size is not None:\n",
    "                assert (\n",
    "                    config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
    "                    % config.actor_rollout_ref.actor.ppo_micro_batch_size\n",
    "                    == 0\n",
    "                )\n",
    "                assert config.actor_rollout_ref.actor.ppo_micro_batch_size * sp_size >= n_gpus\n",
    "\n",
    "        assert config.actor_rollout_ref.actor.loss_agg_mode in [\n",
    "            \"token-mean\",\n",
    "            \"seq-mean-token-sum\",\n",
    "            \"seq-mean-token-mean\",\n",
    "            \"seq-mean-token-sum-norm\",\n",
    "        ], f\"Invalid loss_agg_mode: {config.actor_rollout_ref.actor.loss_agg_mode}\"\n",
    "\n",
    "        if self.config.algorithm.use_kl_in_reward and config.actor_rollout_ref.actor.use_kl_loss:\n",
    "            print(\"NOTICE: You have both enabled in-reward kl and kl loss.\")\n",
    "\n",
    "        # critic\n",
    "        if self.use_critic and not config.critic.use_dynamic_bsz:\n",
    "            assert config.data.train_batch_size >= config.critic.ppo_mini_batch_size\n",
    "            sp_size = config.critic.get(\"ulysses_sequence_parallel_size\", 1)\n",
    "            if config.critic.ppo_micro_batch_size is not None:\n",
    "                assert config.critic.ppo_mini_batch_size % config.critic.ppo_micro_batch_size == 0\n",
    "                assert config.critic.ppo_micro_batch_size * sp_size >= n_gpus\n",
    "\n",
    "        # Check if use_remove_padding is enabled when using sequence parallelism for fsdp\n",
    "        if config.actor_rollout_ref.actor.strategy in {\"fsdp\", \"fsdp2\"} and (\n",
    "            config.actor_rollout_ref.actor.get(\"ulysses_sequence_parallel_size\", 1) > 1\n",
    "            or config.actor_rollout_ref.ref.get(\"ulysses_sequence_parallel_size\", 1) > 1\n",
    "        ):\n",
    "            assert config.actor_rollout_ref.model.use_remove_padding, (\n",
    "                \"When using sequence parallelism for actor/ref policy, you must enable `use_remove_padding`.\"\n",
    "            )\n",
    "\n",
    "        if self.use_critic and config.critic.strategy in {\"fsdp\", \"fsdp2\"}:\n",
    "            if config.critic.get(\"ulysses_sequence_parallel_size\", 1) > 1:\n",
    "                assert config.critic.model.use_remove_padding, (\n",
    "                    \"When using sequence parallelism for critic, you must enable `use_remove_padding`.\"\n",
    "                )\n",
    "\n",
    "        if config.data.get(\"val_batch_size\", None) is not None:\n",
    "            print(\n",
    "                \"WARNING: val_batch_size is deprecated.\"\n",
    "                + \" Validation datasets are sent to inference engines as a whole batch,\"\n",
    "                + \" which will schedule the memory themselves.\"\n",
    "            )\n",
    "\n",
    "        # check eval config\n",
    "        if config.actor_rollout_ref.rollout.val_kwargs.do_sample:\n",
    "            assert config.actor_rollout_ref.rollout.temperature > 0, (\n",
    "                \"validation gen temperature should be greater than 0 when enabling do_sample\"\n",
    "            )\n",
    "\n",
    "        print(\"[validate_config] All configuration checks passed successfully!\")\n",
    "\n",
    "    def _create_dataloader(self, train_dataset, val_dataset, collate_fn, train_sampler: Optional[Sampler]):\n",
    "        \"\"\"\n",
    "        Creates the train and validation dataloaders.\n",
    "        \"\"\"\n",
    "        # TODO: we have to make sure the batch size is divisible by the dp size\n",
    "        from verl.trainer.main_ppo import create_rl_dataset, create_rl_sampler\n",
    "\n",
    "        if train_dataset is None:\n",
    "            train_dataset = create_rl_dataset(\n",
    "                self.config.data.train_files, self.config.data, self.tokenizer, self.processor\n",
    "            )\n",
    "        if val_dataset is None:\n",
    "            val_dataset = create_rl_dataset(\n",
    "                self.config.data.val_files, self.config.data, self.tokenizer, self.processor\n",
    "            )\n",
    "        self.train_dataset, self.val_dataset = train_dataset, val_dataset\n",
    "\n",
    "        if train_sampler is None:\n",
    "            train_sampler = create_rl_sampler(self.config.data, self.train_dataset)\n",
    "        if collate_fn is None:\n",
    "            from verl.utils.dataset.rl_dataset import collate_fn as default_collate_fn\n",
    "\n",
    "            collate_fn = default_collate_fn\n",
    "\n",
    "        num_workers = self.config.data[\"dataloader_num_workers\"]\n",
    "\n",
    "        self.train_dataloader = StatefulDataLoader(\n",
    "            dataset=self.train_dataset,\n",
    "            batch_size=self.config.data.get(\"gen_batch_size\", self.config.data.train_batch_size),\n",
    "            num_workers=num_workers,\n",
    "            drop_last=True,\n",
    "            collate_fn=collate_fn,\n",
    "            sampler=train_sampler,\n",
    "        )\n",
    "\n",
    "        val_batch_size = self.config.data.val_batch_size  # Prefer config value if set\n",
    "        if val_batch_size is None:\n",
    "            val_batch_size = len(self.val_dataset)\n",
    "\n",
    "        self.val_dataloader = StatefulDataLoader(\n",
    "            dataset=self.val_dataset,\n",
    "            batch_size=val_batch_size,\n",
    "            num_workers=num_workers,\n",
    "            shuffle=self.config.data.get(\"validation_shuffle\", True),\n",
    "            drop_last=False,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "        assert len(self.train_dataloader) >= 1, \"Train dataloader is empty!\"\n",
    "        assert len(self.val_dataloader) >= 1, \"Validation dataloader is empty!\"\n",
    "\n",
    "        print(\n",
    "            f\"Size of train dataloader: {len(self.train_dataloader)}, Size of val dataloader: \"\n",
    "            f\"{len(self.val_dataloader)}\"\n",
    "        )\n",
    "\n",
    "        total_training_steps = len(self.train_dataloader) * self.config.trainer.total_epochs\n",
    "\n",
    "        if self.config.trainer.total_training_steps is not None:\n",
    "            total_training_steps = self.config.trainer.total_training_steps\n",
    "\n",
    "        self.total_training_steps = total_training_steps\n",
    "        print(f\"Total training steps: {self.total_training_steps}\")\n",
    "\n",
    "        try:\n",
    "            OmegaConf.set_struct(self.config, True)\n",
    "            with open_dict(self.config):\n",
    "                if OmegaConf.select(self.config, \"actor_rollout_ref.actor.optim\"):\n",
    "                    self.config.actor_rollout_ref.actor.optim.total_training_steps = total_training_steps\n",
    "                if OmegaConf.select(self.config, \"critic.optim\"):\n",
    "                    self.config.critic.optim.total_training_steps = total_training_steps\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set total_training_steps in config. Structure missing? Error: {e}\")\n",
    "\n",
    "    def _dump_generations(self, inputs, outputs, scores, reward_extra_infos_dict, dump_path):\n",
    "        \"\"\"Dump rollout/validation samples as JSONL.\"\"\"\n",
    "        os.makedirs(dump_path, exist_ok=True)\n",
    "        filename = os.path.join(dump_path, f\"{self.global_steps}.jsonl\")\n",
    "\n",
    "        n = len(inputs)\n",
    "        base_data = {\n",
    "            \"input\": inputs,\n",
    "            \"output\": outputs,\n",
    "            \"score\": scores,\n",
    "            \"step\": [self.global_steps] * n,\n",
    "        }\n",
    "\n",
    "        for k, v in reward_extra_infos_dict.items():\n",
    "            if len(v) == n:\n",
    "                base_data[k] = v\n",
    "\n",
    "        lines = []\n",
    "        for i in range(n):\n",
    "            entry = {k: v[i] for k, v in base_data.items()}\n",
    "            lines.append(json.dumps(entry, ensure_ascii=False))\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines) + \"\\n\")\n",
    "\n",
    "        print(f\"Dumped generations to {filename}\")\n",
    "\n",
    "    def _maybe_log_val_generations(self, inputs, outputs, scores):\n",
    "        \"\"\"Log a table of validation samples to the configured logger (wandb or swanlab)\"\"\"\n",
    "\n",
    "        generations_to_log = self.config.trainer.log_val_generations\n",
    "\n",
    "        if generations_to_log == 0:\n",
    "            return\n",
    "\n",
    "        import numpy as np\n",
    "\n",
    "        # Create tuples of (input, output, score) and sort by input text\n",
    "        samples = list(zip(inputs, outputs, scores, strict=True))\n",
    "        samples.sort(key=lambda x: x[0])  # Sort by input text\n",
    "\n",
    "        # Use fixed random seed for deterministic shuffling\n",
    "        rng = np.random.RandomState(42)\n",
    "        rng.shuffle(samples)\n",
    "\n",
    "        # Take first N samples after shuffling\n",
    "        samples = samples[:generations_to_log]\n",
    "\n",
    "        # Log to each configured logger\n",
    "        self.validation_generations_logger.log(self.config.trainer.logger, samples, self.global_steps)\n",
    "\n",
    "    def _validate(self):\n",
    "        data_source_lst = []\n",
    "        reward_extra_infos_dict: dict[str, list] = defaultdict(list)\n",
    "\n",
    "        # Lists to collect samples for the table\n",
    "        sample_inputs = []\n",
    "        sample_outputs = []\n",
    "        sample_scores = []\n",
    "        sample_turns = []\n",
    "\n",
    "        for test_data in self.val_dataloader:\n",
    "            test_batch = DataProto.from_single_dict(test_data)\n",
    "\n",
    "            # repeat test batch\n",
    "            test_batch = test_batch.repeat(\n",
    "                repeat_times=self.config.actor_rollout_ref.rollout.val_kwargs.n, interleave=True\n",
    "            )\n",
    "\n",
    "            # we only do validation on rule-based rm\n",
    "            if self.config.reward_model.enable and test_batch[0].non_tensor_batch[\"reward_model\"][\"style\"] == \"model\":\n",
    "                return {}\n",
    "\n",
    "            # Store original inputs\n",
    "            input_ids = test_batch.batch[\"input_ids\"]\n",
    "            # TODO: Can we keep special tokens except for padding tokens?\n",
    "            input_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
    "            sample_inputs.extend(input_texts)\n",
    "\n",
    "            batch_keys_to_pop = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "            non_tensor_batch_keys_to_pop = [\"raw_prompt_ids\"]\n",
    "            if \"multi_modal_data\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"multi_modal_data\")\n",
    "            if \"raw_prompt\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"raw_prompt\")\n",
    "            if \"tools_kwargs\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"tools_kwargs\")\n",
    "            if \"interaction_kwargs\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"interaction_kwargs\")\n",
    "            if \"agent_name\" in test_batch.non_tensor_batch:\n",
    "                non_tensor_batch_keys_to_pop.append(\"agent_name\")\n",
    "            test_gen_batch = test_batch.pop(\n",
    "                batch_keys=batch_keys_to_pop,\n",
    "                non_tensor_batch_keys=non_tensor_batch_keys_to_pop,\n",
    "            )\n",
    "\n",
    "            test_gen_batch.meta_info = {\n",
    "                \"eos_token_id\": self.tokenizer.eos_token_id,\n",
    "                \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "                \"recompute_log_prob\": False,\n",
    "                \"do_sample\": self.config.actor_rollout_ref.rollout.val_kwargs.do_sample,\n",
    "                \"validate\": True,\n",
    "                \"global_steps\": self.global_steps,\n",
    "            }\n",
    "            print(f\"test_gen_batch meta info: {test_gen_batch.meta_info}\")\n",
    "\n",
    "            # pad to be divisible by dp_size\n",
    "            size_divisor = (\n",
    "                self.actor_rollout_wg.world_size\n",
    "                if not self.async_rollout_mode\n",
    "                else self.config.actor_rollout_ref.rollout.agent.num_workers\n",
    "            )\n",
    "            test_gen_batch_padded, pad_size = pad_dataproto_to_divisor(test_gen_batch, size_divisor)\n",
    "            if not self.async_rollout_mode:\n",
    "                test_output_gen_batch_padded = self.actor_rollout_wg.generate_sequences(test_gen_batch_padded)\n",
    "            else:\n",
    "                test_output_gen_batch_padded = self.async_rollout_manager.generate_sequences(test_gen_batch_padded)\n",
    "\n",
    "            # unpad\n",
    "            test_output_gen_batch = unpad_dataproto(test_output_gen_batch_padded, pad_size=pad_size)\n",
    "\n",
    "            print(\"validation generation end\")\n",
    "\n",
    "            # Store generated outputs\n",
    "            output_ids = test_output_gen_batch.batch[\"responses\"]\n",
    "            output_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "            sample_outputs.extend(output_texts)\n",
    "\n",
    "            test_batch = test_batch.union(test_output_gen_batch)\n",
    "            test_batch.meta_info[\"validate\"] = True\n",
    "\n",
    "            # evaluate using reward_function\n",
    "            result = self.val_reward_fn(test_batch, return_dict=True)\n",
    "            reward_tensor = result[\"reward_tensor\"]\n",
    "            scores = reward_tensor.sum(-1).cpu().tolist()\n",
    "            sample_scores.extend(scores)\n",
    "\n",
    "            reward_extra_infos_dict[\"reward\"].extend(scores)\n",
    "            print(f\"len reward_extra_infos_dict['reward']: {len(reward_extra_infos_dict['reward'])}\")\n",
    "            if \"reward_extra_info\" in result:\n",
    "                for key, lst in result[\"reward_extra_info\"].items():\n",
    "                    reward_extra_infos_dict[key].extend(lst)\n",
    "                    print(f\"len reward_extra_infos_dict['{key}']: {len(reward_extra_infos_dict[key])}\")\n",
    "\n",
    "            # collect num_turns of each prompt\n",
    "            if \"__num_turns__\" in test_batch.non_tensor_batch:\n",
    "                sample_turns.append(test_batch.non_tensor_batch[\"__num_turns__\"])\n",
    "\n",
    "            data_source_lst.append(test_batch.non_tensor_batch.get(\"data_source\", [\"unknown\"] * reward_tensor.shape[0]))\n",
    "\n",
    "        self._maybe_log_val_generations(inputs=sample_inputs, outputs=sample_outputs, scores=sample_scores)\n",
    "\n",
    "        # dump generations\n",
    "        val_data_dir = self.config.trainer.get(\"validation_data_dir\", None)\n",
    "        if val_data_dir:\n",
    "            self._dump_generations(\n",
    "                inputs=sample_inputs,\n",
    "                outputs=sample_outputs,\n",
    "                scores=sample_scores,\n",
    "                reward_extra_infos_dict=reward_extra_infos_dict,\n",
    "                dump_path=val_data_dir,\n",
    "            )\n",
    "\n",
    "        for key_info, lst in reward_extra_infos_dict.items():\n",
    "            assert len(lst) == 0 or len(lst) == len(sample_scores), f\"{key_info}: {len(lst)=}, {len(sample_scores)=}\"\n",
    "\n",
    "        data_sources = np.concatenate(data_source_lst, axis=0)\n",
    "\n",
    "        data_src2var2metric2val = process_validation_metrics(data_sources, sample_inputs, reward_extra_infos_dict)\n",
    "        metric_dict = {}\n",
    "        for data_source, var2metric2val in data_src2var2metric2val.items():\n",
    "            core_var = \"acc\" if \"acc\" in var2metric2val else \"reward\"\n",
    "            for var_name, metric2val in var2metric2val.items():\n",
    "                n_max = max([int(name.split(\"@\")[-1].split(\"/\")[0]) for name in metric2val.keys()])\n",
    "                for metric_name, metric_val in metric2val.items():\n",
    "                    if (\n",
    "                        (var_name == core_var)\n",
    "                        and any(metric_name.startswith(pfx) for pfx in [\"mean\", \"maj\", \"best\"])\n",
    "                        and (f\"@{n_max}\" in metric_name)\n",
    "                    ):\n",
    "                        metric_sec = \"val-core\"\n",
    "                    else:\n",
    "                        metric_sec = \"val-aux\"\n",
    "                    pfx = f\"{metric_sec}/{data_source}/{var_name}/{metric_name}\"\n",
    "                    metric_dict[pfx] = metric_val\n",
    "\n",
    "        if len(sample_turns) > 0:\n",
    "            sample_turns = np.concatenate(sample_turns)\n",
    "            metric_dict[\"val-aux/num_turns/min\"] = sample_turns.min()\n",
    "            metric_dict[\"val-aux/num_turns/max\"] = sample_turns.max()\n",
    "            metric_dict[\"val-aux/num_turns/mean\"] = sample_turns.mean()\n",
    "\n",
    "        return metric_dict\n",
    "\n",
    "    def init_workers(self):\n",
    "        \"\"\"Initialize distributed training workers using Ray backend.\n",
    "\n",
    "        Creates:\n",
    "        1. Ray resource pools from configuration\n",
    "        2. Worker groups for each role (actor, critic, etc.)\n",
    "        \"\"\"\n",
    "        self.resource_pool_manager.create_resource_pool()\n",
    "\n",
    "        self.resource_pool_to_cls = {pool: {} for pool in self.resource_pool_manager.resource_pool_dict.values()}\n",
    "\n",
    "        # create actor and rollout\n",
    "        if self.hybrid_engine:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.ActorRollout)\n",
    "            actor_rollout_cls = RayClassWithInitArgs(\n",
    "                cls=self.role_worker_mapping[Role.ActorRollout],\n",
    "                config=self.config.actor_rollout_ref,\n",
    "                role=\"actor_rollout\",\n",
    "                profile_option=self.config.trainer.npu_profile.options,\n",
    "            )\n",
    "            self.resource_pool_to_cls[resource_pool][\"actor_rollout\"] = actor_rollout_cls\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # create critic\n",
    "        if self.use_critic:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.Critic)\n",
    "            critic_cls = RayClassWithInitArgs(cls=self.role_worker_mapping[Role.Critic], config=self.config.critic)\n",
    "            self.resource_pool_to_cls[resource_pool][\"critic\"] = critic_cls\n",
    "\n",
    "        # create reference policy if needed\n",
    "        if self.use_reference_policy:\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RefPolicy)\n",
    "            ref_policy_cls = RayClassWithInitArgs(\n",
    "                self.role_worker_mapping[Role.RefPolicy],\n",
    "                config=self.config.actor_rollout_ref,\n",
    "                role=\"ref\",\n",
    "                profile_option=self.config.trainer.npu_profile.options,\n",
    "            )\n",
    "            self.resource_pool_to_cls[resource_pool][\"ref\"] = ref_policy_cls\n",
    "\n",
    "        # create a reward model if reward_fn is None\n",
    "        if self.use_rm:\n",
    "            # we create a RM here\n",
    "            resource_pool = self.resource_pool_manager.get_resource_pool(Role.RewardModel)\n",
    "            rm_cls = RayClassWithInitArgs(self.role_worker_mapping[Role.RewardModel], config=self.config.reward_model)\n",
    "            self.resource_pool_to_cls[resource_pool][\"rm\"] = rm_cls\n",
    "\n",
    "        # initialize WorkerGroup\n",
    "        # NOTE: if you want to use a different resource pool for each role, which can support different parallel size,\n",
    "        # you should not use `create_colocated_worker_cls`.\n",
    "        # Instead, directly pass different resource pool to different worker groups.\n",
    "        # See https://github.com/volcengine/verl/blob/master/examples/ray/tutorial.ipynb for more information.\n",
    "        all_wg = {}\n",
    "        wg_kwargs = {}  # Setting up kwargs for RayWorkerGroup\n",
    "        if OmegaConf.select(self.config.trainer, \"ray_wait_register_center_timeout\") is not None:\n",
    "            wg_kwargs[\"ray_wait_register_center_timeout\"] = self.config.trainer.ray_wait_register_center_timeout\n",
    "        if OmegaConf.select(self.config.trainer, \"profile_steps\") is not None:\n",
    "            wg_kwargs[\"profile_steps\"] = OmegaConf.select(self.config.trainer, \"profile_steps\")\n",
    "            assert OmegaConf.select(self.config.trainer, \"worker_nsight_options\") is not None, (\n",
    "                \"worker_nsight_options must be set when profile_steps is set\"\n",
    "            )\n",
    "            wg_kwargs[\"worker_nsight_options\"] = OmegaConf.to_container(\n",
    "                OmegaConf.select(self.config.trainer, \"worker_nsight_options\")\n",
    "            )\n",
    "        wg_kwargs[\"device_name\"] = self.device_name\n",
    "\n",
    "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
    "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
    "            wg_dict = self.ray_worker_group_cls(\n",
    "                resource_pool=resource_pool,\n",
    "                ray_cls_with_init=worker_dict_cls,\n",
    "                **wg_kwargs,\n",
    "            )\n",
    "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
    "            all_wg.update(spawn_wg)\n",
    "\n",
    "        if self.use_critic:\n",
    "            self.critic_wg = all_wg[\"critic\"]\n",
    "            self.critic_wg.init_model()\n",
    "\n",
    "        if self.use_reference_policy and not self.ref_in_actor:\n",
    "            self.ref_policy_wg = all_wg[\"ref\"]\n",
    "            self.ref_policy_wg.init_model()\n",
    "\n",
    "        if self.use_rm:\n",
    "            self.rm_wg = all_wg[\"rm\"]\n",
    "            self.rm_wg.init_model()\n",
    "\n",
    "        # we should create rollout at the end so that vllm can have a better estimation of kv cache memory\n",
    "        self.actor_rollout_wg = all_wg[\"actor_rollout\"]\n",
    "        self.actor_rollout_wg.init_model()\n",
    "\n",
    "        # create async rollout manager and request scheduler\n",
    "        self.async_rollout_mode = False\n",
    "        if self.config.actor_rollout_ref.rollout.mode == \"async\":\n",
    "            from verl.experimental.agent_loop import AgentLoopManager\n",
    "\n",
    "            self.async_rollout_mode = True\n",
    "            self.async_rollout_manager = AgentLoopManager(\n",
    "                config=self.config,\n",
    "                worker_group=self.actor_rollout_wg,\n",
    "            )\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        from verl.utils.fs import local_mkdir_safe\n",
    "\n",
    "        # path: given_path + `/global_step_{global_steps}` + `/actor`\n",
    "        local_global_step_folder = os.path.join(\n",
    "            self.config.trainer.default_local_dir, f\"global_step_{self.global_steps}\"\n",
    "        )\n",
    "\n",
    "        print(f\"local_global_step_folder: {local_global_step_folder}\")\n",
    "        actor_local_path = os.path.join(local_global_step_folder, \"actor\")\n",
    "\n",
    "        actor_remote_path = (\n",
    "            None\n",
    "            if self.config.trainer.default_hdfs_dir is None\n",
    "            else os.path.join(self.config.trainer.default_hdfs_dir, f\"global_step_{self.global_steps}\", \"actor\")\n",
    "        )\n",
    "\n",
    "        remove_previous_ckpt_in_save = self.config.trainer.get(\"remove_previous_ckpt_in_save\", False)\n",
    "        if remove_previous_ckpt_in_save:\n",
    "            print(\n",
    "                \"Warning: remove_previous_ckpt_in_save is deprecated,\"\n",
    "                + \" set max_actor_ckpt_to_keep=1 and max_critic_ckpt_to_keep=1 instead\"\n",
    "            )\n",
    "        max_actor_ckpt_to_keep = (\n",
    "            self.config.trainer.get(\"max_actor_ckpt_to_keep\", None) if not remove_previous_ckpt_in_save else 1\n",
    "        )\n",
    "        max_critic_ckpt_to_keep = (\n",
    "            self.config.trainer.get(\"max_critic_ckpt_to_keep\", None) if not remove_previous_ckpt_in_save else 1\n",
    "        )\n",
    "\n",
    "        self.actor_rollout_wg.save_checkpoint(\n",
    "            actor_local_path, actor_remote_path, self.global_steps, max_ckpt_to_keep=max_actor_ckpt_to_keep\n",
    "        )\n",
    "\n",
    "        if self.use_critic:\n",
    "            critic_local_path = os.path.join(local_global_step_folder, \"critic\")\n",
    "            critic_remote_path = (\n",
    "                None\n",
    "                if self.config.trainer.default_hdfs_dir is None\n",
    "                else os.path.join(self.config.trainer.default_hdfs_dir, f\"global_step_{self.global_steps}\", \"critic\")\n",
    "            )\n",
    "            self.critic_wg.save_checkpoint(\n",
    "                critic_local_path, critic_remote_path, self.global_steps, max_ckpt_to_keep=max_critic_ckpt_to_keep\n",
    "            )\n",
    "\n",
    "        # save dataloader\n",
    "        local_mkdir_safe(local_global_step_folder)\n",
    "        dataloader_local_path = os.path.join(local_global_step_folder, \"data.pt\")\n",
    "        dataloader_state_dict = self.train_dataloader.state_dict()\n",
    "        torch.save(dataloader_state_dict, dataloader_local_path)\n",
    "\n",
    "        # latest checkpointed iteration tracker (for atomic usage)\n",
    "        local_latest_checkpointed_iteration = os.path.join(\n",
    "            self.config.trainer.default_local_dir, \"latest_checkpointed_iteration.txt\"\n",
    "        )\n",
    "        with open(local_latest_checkpointed_iteration, \"w\") as f:\n",
    "            f.write(str(self.global_steps))\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        if self.config.trainer.resume_mode == \"disable\":\n",
    "            return 0\n",
    "\n",
    "        # load from hdfs\n",
    "        if self.config.trainer.default_hdfs_dir is not None:\n",
    "            raise NotImplementedError(\"load from hdfs is not implemented yet\")\n",
    "        else:\n",
    "            checkpoint_folder = self.config.trainer.default_local_dir  # TODO: check path\n",
    "            if not os.path.isabs(checkpoint_folder):\n",
    "                working_dir = os.getcwd()\n",
    "                checkpoint_folder = os.path.join(working_dir, checkpoint_folder)\n",
    "            global_step_folder = find_latest_ckpt_path(checkpoint_folder)  # None if no latest\n",
    "\n",
    "        # find global_step_folder\n",
    "        if self.config.trainer.resume_mode == \"auto\":\n",
    "            if global_step_folder is None:\n",
    "                print(\"Training from scratch\")\n",
    "                return 0\n",
    "        else:\n",
    "            if self.config.trainer.resume_mode == \"resume_path\":\n",
    "                assert isinstance(self.config.trainer.resume_from_path, str), \"resume ckpt must be str type\"\n",
    "                assert \"global_step_\" in self.config.trainer.resume_from_path, (\n",
    "                    \"resume ckpt must specify the global_steps\"\n",
    "                )\n",
    "                global_step_folder = self.config.trainer.resume_from_path\n",
    "                if not os.path.isabs(global_step_folder):\n",
    "                    working_dir = os.getcwd()\n",
    "                    global_step_folder = os.path.join(working_dir, global_step_folder)\n",
    "        print(f\"Load from checkpoint folder: {global_step_folder}\")\n",
    "        # set global step\n",
    "        self.global_steps = int(global_step_folder.split(\"global_step_\")[-1])\n",
    "\n",
    "        print(f\"Setting global step to {self.global_steps}\")\n",
    "        print(f\"Resuming from {global_step_folder}\")\n",
    "\n",
    "        actor_path = os.path.join(global_step_folder, \"actor\")\n",
    "        critic_path = os.path.join(global_step_folder, \"critic\")\n",
    "        # load actor\n",
    "        self.actor_rollout_wg.load_checkpoint(\n",
    "            actor_path, del_local_after_load=self.config.trainer.del_local_ckpt_after_load\n",
    "        )\n",
    "        # load critic\n",
    "        if self.use_critic:\n",
    "            self.critic_wg.load_checkpoint(\n",
    "                critic_path, del_local_after_load=self.config.trainer.del_local_ckpt_after_load\n",
    "            )\n",
    "\n",
    "        # load dataloader,\n",
    "        # TODO: from remote not implemented yet\n",
    "        dataloader_local_path = os.path.join(global_step_folder, \"data.pt\")\n",
    "        if os.path.exists(dataloader_local_path):\n",
    "            dataloader_state_dict = torch.load(dataloader_local_path, weights_only=False)\n",
    "            self.train_dataloader.load_state_dict(dataloader_state_dict)\n",
    "        else:\n",
    "            print(f\"Warning: No dataloader state found at {dataloader_local_path}, will start from scratch\")\n",
    "\n",
    "    def _start_profiling(self, do_profile: bool) -> None:\n",
    "        \"\"\"Start profiling for all worker groups if profiling is enabled.\"\"\"\n",
    "        if do_profile:\n",
    "            self.actor_rollout_wg.start_profile(role=\"e2e\", profile_step=self.global_steps)\n",
    "            if self.use_reference_policy:\n",
    "                self.ref_policy_wg.start_profile()\n",
    "            if self.use_critic:\n",
    "                self.critic_wg.start_profile()\n",
    "            if self.use_rm:\n",
    "                self.rm_wg.start_profile()\n",
    "\n",
    "    def _stop_profiling(self, do_profile: bool) -> None:\n",
    "        \"\"\"Stop profiling for all worker groups if profiling is enabled.\"\"\"\n",
    "        if do_profile:\n",
    "            self.actor_rollout_wg.stop_profile()\n",
    "            if self.use_reference_policy:\n",
    "                self.ref_policy_wg.stop_profile()\n",
    "            if self.use_critic:\n",
    "                self.critic_wg.stop_profile()\n",
    "            if self.use_rm:\n",
    "                self.rm_wg.stop_profile()\n",
    "\n",
    "    def _balance_batch(self, batch: DataProto, metrics, logging_prefix=\"global_seqlen\"):\n",
    "        \"\"\"Reorder the data on single controller such that each dp rank gets similar total tokens\"\"\"\n",
    "        attention_mask = batch.batch[\"attention_mask\"]\n",
    "        batch_size = attention_mask.shape[0]\n",
    "        global_seqlen_lst = batch.batch[\"attention_mask\"].view(batch_size, -1).sum(-1).tolist()  # (train_batch_size,)\n",
    "        world_size = self.actor_rollout_wg.world_size\n",
    "        global_partition_lst = get_seqlen_balanced_partitions(\n",
    "            global_seqlen_lst, k_partitions=world_size, equal_size=True\n",
    "        )\n",
    "        # reorder based on index. The data will be automatically equally partitioned by dispatch function\n",
    "        global_idx = torch.tensor([j for partition in global_partition_lst for j in partition])\n",
    "        batch.reorder(global_idx)\n",
    "        global_balance_stats = log_seqlen_unbalance(\n",
    "            seqlen_list=global_seqlen_lst, partitions=global_partition_lst, prefix=logging_prefix\n",
    "        )\n",
    "        metrics.update(global_balance_stats)\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        The training loop of PPO.\n",
    "        The driver process only need to call the compute functions of the worker group through RPC\n",
    "        to construct the PPO dataflow.\n",
    "        The light-weight advantage computation is done on the driver process.\n",
    "        \"\"\"\n",
    "        from omegaconf import OmegaConf\n",
    "\n",
    "        from verl.utils.tracking import Tracking\n",
    "\n",
    "        logger = Tracking(\n",
    "            project_name=self.config.trainer.project_name,\n",
    "            experiment_name=self.config.trainer.experiment_name,\n",
    "            default_backend=self.config.trainer.logger,\n",
    "            config=OmegaConf.to_container(self.config, resolve=True),\n",
    "        )\n",
    "\n",
    "        self.global_steps = 0\n",
    "\n",
    "        # load checkpoint before doing anything\n",
    "        self._load_checkpoint()\n",
    "\n",
    "        # perform validation before training\n",
    "        # currently, we only support validation using the reward_function.\n",
    "        if self.val_reward_fn is not None and self.config.trainer.get(\"val_before_train\", True):\n",
    "            val_metrics = self._validate()\n",
    "            assert val_metrics, f\"{val_metrics=}\"\n",
    "            pprint(f\"Initial validation metrics: {val_metrics}\")\n",
    "            logger.log(data=val_metrics, step=self.global_steps)\n",
    "            if self.config.trainer.get(\"val_only\", False):\n",
    "                return\n",
    "\n",
    "        # add tqdm\n",
    "        progress_bar = tqdm(total=self.total_training_steps, initial=self.global_steps, desc=\"Training Progress\")\n",
    "\n",
    "        # we start from step 1\n",
    "        self.global_steps += 1\n",
    "        last_val_metrics = None\n",
    "        self.max_steps_duration = 0\n",
    "\n",
    "        for epoch in range(self.config.trainer.total_epochs):\n",
    "            for batch_dict in self.train_dataloader:\n",
    "                metrics = {}\n",
    "                timing_raw = {}\n",
    "\n",
    "                do_profile = (\n",
    "                    self.global_steps in self.config.trainer.profile_steps\n",
    "                    if self.config.trainer.profile_steps is not None\n",
    "                    else False\n",
    "                )\n",
    "                with marked_timer(\"start_profile\", timing_raw):\n",
    "                    self._start_profiling(do_profile)\n",
    "\n",
    "                batch: DataProto = DataProto.from_single_dict(batch_dict)\n",
    "\n",
    "                # pop those keys for generation\n",
    "                batch_keys_to_pop = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "                non_tensor_batch_keys_to_pop = [\"raw_prompt_ids\"]\n",
    "                if \"multi_modal_data\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"multi_modal_data\")\n",
    "                if \"raw_prompt\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"raw_prompt\")\n",
    "                if \"tools_kwargs\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"tools_kwargs\")\n",
    "                if \"interaction_kwargs\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"interaction_kwargs\")\n",
    "                if \"index\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"index\")\n",
    "                if \"agent_name\" in batch.non_tensor_batch:\n",
    "                    non_tensor_batch_keys_to_pop.append(\"agent_name\")\n",
    "\n",
    "                gen_batch = batch.pop(\n",
    "                    batch_keys=batch_keys_to_pop,\n",
    "                    non_tensor_batch_keys=non_tensor_batch_keys_to_pop,\n",
    "                )\n",
    "\n",
    "                # pass global_steps to trace\n",
    "                gen_batch.meta_info[\"global_steps\"] = self.global_steps\n",
    "                gen_batch = gen_batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)\n",
    "\n",
    "                is_last_step = self.global_steps >= self.total_training_steps\n",
    "\n",
    "                with marked_timer(\"step\", timing_raw):\n",
    "                    # generate a batch\n",
    "                    with marked_timer(\"gen\", timing_raw, color=\"red\"):\n",
    "                        if not self.async_rollout_mode:\n",
    "                            gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
    "                        else:\n",
    "                            gen_batch_output = self.async_rollout_manager.generate_sequences(gen_batch)\n",
    "                        timing_raw.update(gen_batch_output.meta_info[\"timing\"])\n",
    "                        gen_batch_output.meta_info.pop(\"timing\", None)\n",
    "\n",
    "                    if self.config.algorithm.adv_estimator == AdvantageEstimator.REMAX:\n",
    "                        with marked_timer(\"gen_max\", timing_raw, color=\"purple\"):\n",
    "                            gen_baseline_batch = deepcopy(gen_batch)\n",
    "                            gen_baseline_batch.meta_info[\"do_sample\"] = False\n",
    "                            if not self.async_rollout_mode:\n",
    "                                gen_baseline_output = self.actor_rollout_wg.generate_sequences(gen_baseline_batch)\n",
    "                            else:\n",
    "                                gen_baseline_output = self.async_rollout_manager.generate_sequences(gen_baseline_batch)\n",
    "                            batch = batch.union(gen_baseline_output)\n",
    "                            reward_baseline_tensor = self.reward_fn(batch)\n",
    "                            reward_baseline_tensor = reward_baseline_tensor.sum(dim=-1)\n",
    "\n",
    "                            batch.pop(batch_keys=list(gen_baseline_output.batch.keys()))\n",
    "\n",
    "                            batch.batch[\"reward_baselines\"] = reward_baseline_tensor\n",
    "\n",
    "                            del gen_baseline_batch, gen_baseline_output\n",
    "\n",
    "                    batch.non_tensor_batch[\"uid\"] = np.array(\n",
    "                        [str(uuid.uuid4()) for _ in range(len(batch.batch))], dtype=object\n",
    "                    )\n",
    "                    # repeat to align with repeated responses in rollout\n",
    "                    batch = batch.repeat(repeat_times=self.config.actor_rollout_ref.rollout.n, interleave=True)\n",
    "                    batch = batch.union(gen_batch_output)\n",
    "\n",
    "                    if \"response_mask\" not in batch.batch.keys():\n",
    "                        batch.batch[\"response_mask\"] = compute_response_mask(batch)\n",
    "                    # Balance the number of valid tokens across DP ranks.\n",
    "                    # NOTE: This usually changes the order of data in the `batch`,\n",
    "                    # which won't affect the advantage calculation (since it's based on uid),\n",
    "                    # but might affect the loss calculation (due to the change of mini-batching).\n",
    "                    # TODO: Decouple the DP balancing and mini-batching.\n",
    "                    if self.config.trainer.balance_batch:\n",
    "                        self._balance_batch(batch, metrics=metrics)\n",
    "\n",
    "                    # compute global_valid tokens\n",
    "                    batch.meta_info[\"global_token_num\"] = torch.sum(batch.batch[\"attention_mask\"], dim=-1).tolist()\n",
    "\n",
    "                    with marked_timer(\"reward\", timing_raw, color=\"yellow\"):\n",
    "                        # compute reward model score\n",
    "                        if self.use_rm:\n",
    "                            reward_tensor = self.rm_wg.compute_rm_score(batch)\n",
    "                            batch = batch.union(reward_tensor)\n",
    "\n",
    "                        if self.config.reward_model.launch_reward_fn_async:\n",
    "                            future_reward = compute_reward_async.remote(data=batch, reward_fn=self.reward_fn)\n",
    "                        else:\n",
    "                            reward_tensor, reward_extra_infos_dict = compute_reward(batch, self.reward_fn)\n",
    "\n",
    "                    # recompute old_log_probs\n",
    "                    with marked_timer(\"old_log_prob\", timing_raw, color=\"blue\"):\n",
    "                        old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)\n",
    "                        entropys = old_log_prob.batch[\"entropys\"]\n",
    "                        response_masks = batch.batch[\"response_mask\"]\n",
    "                        loss_agg_mode = self.config.actor_rollout_ref.actor.loss_agg_mode\n",
    "                        entropy_agg = agg_loss(loss_mat=entropys, loss_mask=response_masks, loss_agg_mode=loss_agg_mode)\n",
    "                        old_log_prob_metrics = {\"actor/entropy\": entropy_agg.detach().item()}\n",
    "                        metrics.update(old_log_prob_metrics)\n",
    "                        old_log_prob.batch.pop(\"entropys\")\n",
    "                        batch = batch.union(old_log_prob)\n",
    "\n",
    "                        if \"rollout_log_probs\" in batch.batch.keys():\n",
    "                            # TODO: we may want to add diff of probs too.\n",
    "                            rollout_old_log_probs = batch.batch[\"rollout_log_probs\"]\n",
    "                            actor_old_log_probs = batch.batch[\"old_log_probs\"]\n",
    "                            attention_mask = batch.batch[\"attention_mask\"]\n",
    "                            responses = batch.batch[\"responses\"]\n",
    "                            response_length = responses.size(1)\n",
    "                            response_mask = attention_mask[:, -response_length:]\n",
    "\n",
    "                            rollout_probs = torch.exp(rollout_old_log_probs)\n",
    "                            actor_probs = torch.exp(actor_old_log_probs)\n",
    "                            rollout_probs_diff = torch.abs(rollout_probs - actor_probs)\n",
    "                            rollout_probs_diff = torch.masked_select(rollout_probs_diff, response_mask.bool())\n",
    "                            rollout_probs_diff_max = torch.max(rollout_probs_diff)\n",
    "                            rollout_probs_diff_mean = torch.mean(rollout_probs_diff)\n",
    "                            rollout_probs_diff_std = torch.std(rollout_probs_diff)\n",
    "                            metrics.update(\n",
    "                                {\n",
    "                                    \"training/rollout_probs_diff_max\": rollout_probs_diff_max.detach().item(),\n",
    "                                    \"training/rollout_probs_diff_mean\": rollout_probs_diff_mean.detach().item(),\n",
    "                                    \"training/rollout_probs_diff_std\": rollout_probs_diff_std.detach().item(),\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                    if self.use_reference_policy:\n",
    "                        # compute reference log_prob\n",
    "                        with marked_timer(\"ref\", timing_raw, color=\"olive\"):\n",
    "                            if not self.ref_in_actor:\n",
    "                                ref_log_prob = self.ref_policy_wg.compute_ref_log_prob(batch)\n",
    "                            else:\n",
    "                                ref_log_prob = self.actor_rollout_wg.compute_ref_log_prob(batch)\n",
    "                            batch = batch.union(ref_log_prob)\n",
    "\n",
    "                    # compute values\n",
    "                    if self.use_critic:\n",
    "                        with marked_timer(\"values\", timing_raw, color=\"cyan\"):\n",
    "                            values = self.critic_wg.compute_values(batch)\n",
    "                            batch = batch.union(values)\n",
    "\n",
    "                    with marked_timer(\"adv\", timing_raw, color=\"brown\"):\n",
    "                        # we combine with rule-based rm\n",
    "                        reward_extra_infos_dict: dict[str, list]\n",
    "                        if self.config.reward_model.launch_reward_fn_async:\n",
    "                            reward_tensor, reward_extra_infos_dict = ray.get(future_reward)\n",
    "                        batch.batch[\"token_level_scores\"] = reward_tensor\n",
    "\n",
    "                        if reward_extra_infos_dict:\n",
    "                            batch.non_tensor_batch.update({k: np.array(v) for k, v in reward_extra_infos_dict.items()})\n",
    "\n",
    "                        # compute rewards. apply_kl_penalty if available\n",
    "                        if self.config.algorithm.use_kl_in_reward:\n",
    "                            batch, kl_metrics = apply_kl_penalty(\n",
    "                                batch, kl_ctrl=self.kl_ctrl_in_reward, kl_penalty=self.config.algorithm.kl_penalty\n",
    "                            )\n",
    "                            metrics.update(kl_metrics)\n",
    "                        else:\n",
    "                            batch.batch[\"token_level_rewards\"] = batch.batch[\"token_level_scores\"]\n",
    "\n",
    "                        # compute advantages, executed on the driver process\n",
    "\n",
    "                        norm_adv_by_std_in_grpo = self.config.algorithm.get(\n",
    "                            \"norm_adv_by_std_in_grpo\", True\n",
    "                        )  # GRPO adv normalization factor\n",
    "\n",
    "                        batch = compute_advantage(\n",
    "                            batch,\n",
    "                            adv_estimator=self.config.algorithm.adv_estimator,\n",
    "                            gamma=self.config.algorithm.gamma,\n",
    "                            lam=self.config.algorithm.lam,\n",
    "                            num_repeat=self.config.actor_rollout_ref.rollout.n,\n",
    "                            norm_adv_by_std_in_grpo=norm_adv_by_std_in_grpo,\n",
    "                            config=self.config.algorithm,\n",
    "                        )\n",
    "\n",
    "                    # update critic\n",
    "                    if self.use_critic:\n",
    "                        with marked_timer(\"update_critic\", timing_raw, color=\"pink\"):\n",
    "                            critic_output = self.critic_wg.update_critic(batch)\n",
    "                        critic_output_metrics = reduce_metrics(critic_output.meta_info[\"metrics\"])\n",
    "                        metrics.update(critic_output_metrics)\n",
    "\n",
    "                    # implement critic warmup\n",
    "                    if self.config.trainer.critic_warmup <= self.global_steps:\n",
    "                        # update actor\n",
    "                        with marked_timer(\"update_actor\", timing_raw, color=\"red\"):\n",
    "                            batch.meta_info[\"multi_turn\"] = self.config.actor_rollout_ref.rollout.multi_turn.enable\n",
    "                            actor_output = self.actor_rollout_wg.update_actor(batch)\n",
    "                        actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n",
    "                        metrics.update(actor_output_metrics)\n",
    "\n",
    "                    # Log rollout generations if enabled\n",
    "                    rollout_data_dir = self.config.trainer.get(\"rollout_data_dir\", None)\n",
    "                    if rollout_data_dir:\n",
    "                        with marked_timer(\"dump_rollout_generations\", timing_raw, color=\"green\"):\n",
    "                            inputs = self.tokenizer.batch_decode(batch.batch[\"prompts\"], skip_special_tokens=True)\n",
    "                            outputs = self.tokenizer.batch_decode(batch.batch[\"responses\"], skip_special_tokens=True)\n",
    "                            scores = batch.batch[\"token_level_scores\"].sum(-1).cpu().tolist()\n",
    "                            if \"request_id\" in batch.non_tensor_batch:\n",
    "                                reward_extra_infos_dict.setdefault(\n",
    "                                    \"request_id\",\n",
    "                                    batch.non_tensor_batch[\"request_id\"].tolist(),\n",
    "                                )\n",
    "                            self._dump_generations(\n",
    "                                inputs=inputs,\n",
    "                                outputs=outputs,\n",
    "                                scores=scores,\n",
    "                                reward_extra_infos_dict=reward_extra_infos_dict,\n",
    "                                dump_path=rollout_data_dir,\n",
    "                            )\n",
    "\n",
    "                    # validate\n",
    "                    if (\n",
    "                        self.val_reward_fn is not None\n",
    "                        and self.config.trainer.test_freq > 0\n",
    "                        and (is_last_step or self.global_steps % self.config.trainer.test_freq == 0)\n",
    "                    ):\n",
    "                        with marked_timer(\"testing\", timing_raw, color=\"green\"):\n",
    "                            val_metrics: dict = self._validate()\n",
    "                            if is_last_step:\n",
    "                                last_val_metrics = val_metrics\n",
    "                        metrics.update(val_metrics)\n",
    "\n",
    "                    # Check if the ESI (Elastic Server Instance)/training plan is close to expiration.\n",
    "                    esi_close_to_expiration = should_save_ckpt_esi(\n",
    "                        max_steps_duration=self.max_steps_duration,\n",
    "                        redundant_time=self.config.trainer.esi_redundant_time,\n",
    "                    )\n",
    "                    # Check if the conditions for saving a checkpoint are met.\n",
    "                    # The conditions include a mandatory condition (1) and\n",
    "                    # one of the following optional conditions (2/3/4):\n",
    "                    # 1. The save frequency is set to a positive value.\n",
    "                    # 2. It's the last training step.\n",
    "                    # 3. The current step number is a multiple of the save frequency.\n",
    "                    # 4. The ESI(Elastic Server Instance)/training plan is close to expiration.\n",
    "                    if self.config.trainer.save_freq > 0 and (\n",
    "                        is_last_step\n",
    "                        or self.global_steps % self.config.trainer.save_freq == 0\n",
    "                        or esi_close_to_expiration\n",
    "                    ):\n",
    "                        if esi_close_to_expiration:\n",
    "                            print(\"Force saving checkpoint: ESI instance expiration approaching.\")\n",
    "                        with marked_timer(\"save_checkpoint\", timing_raw, color=\"green\"):\n",
    "                            self._save_checkpoint()\n",
    "\n",
    "                with marked_timer(\"stop_profile\", timing_raw):\n",
    "                    self._stop_profiling(do_profile)\n",
    "\n",
    "                steps_duration = timing_raw[\"step\"]\n",
    "                self.max_steps_duration = max(self.max_steps_duration, steps_duration)\n",
    "\n",
    "                # training metrics\n",
    "                metrics.update(\n",
    "                    {\n",
    "                        \"training/global_step\": self.global_steps,\n",
    "                        \"training/epoch\": epoch,\n",
    "                    }\n",
    "                )\n",
    "                # collect metrics\n",
    "                metrics.update(compute_data_metrics(batch=batch, use_critic=self.use_critic))\n",
    "                metrics.update(compute_timing_metrics(batch=batch, timing_raw=timing_raw))\n",
    "                # TODO: implement actual tflpo and theoretical tflpo\n",
    "                n_gpus = self.resource_pool_manager.get_n_gpus()\n",
    "                metrics.update(compute_throughout_metrics(batch=batch, timing_raw=timing_raw, n_gpus=n_gpus))\n",
    "\n",
    "                # this is experimental and may be changed/removed in the future in favor of a general-purpose one\n",
    "                if isinstance(self.train_dataloader.sampler, AbstractCurriculumSampler):\n",
    "                    self.train_dataloader.sampler.update(batch=batch)\n",
    "\n",
    "                # TODO: make a canonical logger that supports various backend\n",
    "                logger.log(data=metrics, step=self.global_steps)\n",
    "\n",
    "                progress_bar.update(1)\n",
    "                self.global_steps += 1\n",
    "\n",
    "                if is_last_step:\n",
    "                    pprint(f\"Final validation metrics: {last_val_metrics}\")\n",
    "                    progress_bar.close()\n",
    "                    return\n",
    "\n",
    "                # this is experimental and may be changed/removed in the future\n",
    "                # in favor of a general-purpose data buffer pool\n",
    "                if hasattr(self.train_dataset, \"on_batch_end\"):\n",
    "                    # The dataset may be changed after each training batch\n",
    "                    self.train_dataset.on_batch_end(batch=batch)\n",
    "\"\"\"\n",
    "main_self_reflective.py代码如下\n",
    "\"\"\"\n",
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Note that we don't combine the main with ray_trainer as ray_trainer is used by other main.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import socket\n",
    "\n",
    "import hydra\n",
    "import ray\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from verl.experimental.dataset.sampler import AbstractSampler\n",
    "from verl.trainer.constants_ppo import get_ppo_ray_runtime_env\n",
    "from verl.trainer.ppo.self_reflective_trainer import SelfReflectiveGRPOTrainer\n",
    "from verl.trainer.ppo.teacher_student_reflective_trainer import TeacherStudentReflectiveTrainer\n",
    "from verl.trainer.ppo.reward import load_reward_manager\n",
    "from verl.utils.device import is_cuda_available\n",
    "from verl.utils.import_utils import load_extern_type\n",
    "\n",
    "\n",
    "@hydra.main(config_path=\"config\", config_name=\"ppo_trainer\", version_base=None)\n",
    "def main(config):\n",
    "    \"\"\"Main entry point for PPO training with Hydra configuration management.\n",
    "\n",
    "    Args:\n",
    "        config_dict: Hydra configuration dictionary containing training parameters.\n",
    "    \"\"\"\n",
    "    run_ppo(config)\n",
    "\n",
    "\n",
    "# Define a function to run the PPO-like training process\n",
    "def run_ppo(config) -> None:\n",
    "    \"\"\"Initialize Ray cluster and run distributed PPO training process.\n",
    "\n",
    "    Args:\n",
    "        config: Training configuration object containing all necessary parameters\n",
    "                for distributed PPO training including Ray initialization settings,\n",
    "                model paths, and training hyperparameters.\n",
    "    \"\"\"\n",
    "    # Check if Ray is not initialized\n",
    "    if not ray.is_initialized():\n",
    "        # Initialize Ray with a local cluster configuration\n",
    "        # Set environment variables in the runtime environment to control tokenizer parallelism,\n",
    "        # NCCL debug level, VLLM logging level, and allow runtime LoRA updating\n",
    "        # `num_cpus` specifies the number of CPU cores Ray can use, obtained from the configuration\n",
    "        ray.init(\n",
    "            runtime_env=get_ppo_ray_runtime_env(),\n",
    "            num_cpus=config.ray_init.num_cpus,\n",
    "        )\n",
    "\n",
    "    # Create a remote instance of the TaskRunner class, and\n",
    "    # Execute the `run` method of the TaskRunner instance remotely and wait for it to complete\n",
    "    if (\n",
    "        is_cuda_available\n",
    "        and config.trainer.get(\"profile_steps\") is not None\n",
    "        and len(config.trainer.get(\"profile_steps\", [])) > 0\n",
    "    ):\n",
    "        nsight_options = OmegaConf.to_container(config.trainer.controller_nsight_options)\n",
    "        runner = TaskRunner.options(runtime_env={\"nsight\": nsight_options}).remote()\n",
    "    else:\n",
    "        runner = TaskRunner.remote()\n",
    "    ray.get(runner.run.remote(config))\n",
    "\n",
    "    # [Optional] get the path of the timeline trace file from the configuration, default to None\n",
    "    # This file is used for performance analysis\n",
    "    timeline_json_file = config.ray_init.get(\"timeline_json_file\", None)\n",
    "    if timeline_json_file:\n",
    "        ray.timeline(filename=timeline_json_file)\n",
    "\n",
    "\n",
    "@ray.remote(num_cpus=1)  # please make sure main_task is not scheduled on head\n",
    "class TaskRunner:\n",
    "    \"\"\"Ray remote class for executing distributed PPO training tasks.\n",
    "\n",
    "    This class encapsulates the main training logic and runs as a Ray remote actor\n",
    "    to enable distributed execution across multiple nodes and GPUs.\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, config):\n",
    "        \"\"\"Execute the main PPO training workflow.\n",
    "\n",
    "        This method sets up the distributed training environment, initializes\n",
    "        workers, datasets, and reward functions, then starts the training process.\n",
    "\n",
    "        Args:\n",
    "            config: Training configuration object containing all parameters needed\n",
    "                   for setting up and running the PPO training process.\n",
    "        \"\"\"\n",
    "        # Print the initial configuration. `resolve=True` will evaluate symbolic values.\n",
    "        from pprint import pprint\n",
    "\n",
    "        from omegaconf import OmegaConf\n",
    "\n",
    "        from verl.utils.fs import copy_to_local\n",
    "\n",
    "        print(f\"TaskRunner hostname: {socket.gethostname()}, PID: {os.getpid()}\")\n",
    "        pprint(OmegaConf.to_container(config, resolve=True))\n",
    "        OmegaConf.resolve(config)\n",
    "\n",
    "        # Download the checkpoint from HDFS to the local machine.\n",
    "        # `use_shm` determines whether to use shared memory, which could lead to faster model loading if turned on\n",
    "        local_path = copy_to_local(\n",
    "            config.actor_rollout_ref.model.path, use_shm=config.actor_rollout_ref.model.get(\"use_shm\", False)\n",
    "        )\n",
    "\n",
    "        # Instantiate the tokenizer and processor.\n",
    "        from verl.utils import hf_processor, hf_tokenizer\n",
    "\n",
    "        trust_remote_code = config.data.get(\"trust_remote_code\", False)\n",
    "        tokenizer = hf_tokenizer(local_path, trust_remote_code=trust_remote_code)\n",
    "        # Used for multimodal LLM, could be None\n",
    "        processor = hf_processor(local_path, trust_remote_code=trust_remote_code, use_fast=True)\n",
    "\n",
    "        # Define worker classes based on the actor strategy.\n",
    "        if config.actor_rollout_ref.actor.strategy in {\"fsdp\", \"fsdp2\"}:\n",
    "            assert config.critic.strategy in {\"fsdp\", \"fsdp2\"}\n",
    "            from verl.single_controller.ray import RayWorkerGroup\n",
    "            from verl.workers.fsdp_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker\n",
    "\n",
    "            use_legacy_worker_impl = config.trainer.get(\"use_legacy_worker_impl\", \"auto\")\n",
    "            if use_legacy_worker_impl in [\"auto\", \"enable\"]:\n",
    "                # import warnings\n",
    "                # warnings.warn(f\"Legacy worker impl is going to be deprecated, will be removed in the future. \\\n",
    "                #   Please set trainer.use_legacy_worker_impl = false to switch to the new worker implementation.\")\n",
    "                from verl.workers.fsdp_workers import CriticWorker\n",
    "            elif use_legacy_worker_impl == \"disable\":\n",
    "                from verl.workers.roles import CriticWorker\n",
    "\n",
    "                print(\"Using new worker implementation\")\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid use_legacy_worker_impl: {use_legacy_worker_impl}\")\n",
    "\n",
    "            actor_rollout_cls = (\n",
    "                AsyncActorRolloutRefWorker\n",
    "                if config.actor_rollout_ref.rollout.mode == \"async\"\n",
    "                else ActorRolloutRefWorker\n",
    "            )\n",
    "            ray_worker_group_cls = RayWorkerGroup\n",
    "\n",
    "        elif config.actor_rollout_ref.actor.strategy == \"megatron\":\n",
    "            assert config.actor_rollout_ref.actor.strategy == config.critic.strategy\n",
    "            from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup\n",
    "            from verl.workers.megatron_workers import ActorRolloutRefWorker, AsyncActorRolloutRefWorker, CriticWorker\n",
    "\n",
    "            actor_rollout_cls = (\n",
    "                AsyncActorRolloutRefWorker\n",
    "                if config.actor_rollout_ref.rollout.mode == \"async\"\n",
    "                else ActorRolloutRefWorker\n",
    "            )\n",
    "            ray_worker_group_cls = NVMegatronRayWorkerGroup\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        from verl.trainer.ppo.ray_trainer import ResourcePoolManager, Role\n",
    "\n",
    "        # Map roles to their corresponding remote worker classes.\n",
    "        role_worker_mapping = {\n",
    "            Role.ActorRollout: ray.remote(actor_rollout_cls),\n",
    "            Role.Critic: ray.remote(CriticWorker),\n",
    "        }\n",
    "\n",
    "        # Define the resource pool specification.\n",
    "        # Map roles to the resource pool.\n",
    "        global_pool_id = \"global_pool\"\n",
    "        resource_pool_spec = {\n",
    "            global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,\n",
    "        }\n",
    "        mapping = {\n",
    "            Role.ActorRollout: global_pool_id,\n",
    "            Role.Critic: global_pool_id,\n",
    "        }\n",
    "\n",
    "        # We should adopt a multi-source reward function here:\n",
    "        # - for rule-based rm, we directly call a reward score\n",
    "        # - for model-based rm, we call a model\n",
    "        # - for code related prompt, we send to a sandbox if there are test cases\n",
    "        # finally, we combine all the rewards together\n",
    "        # The reward type depends on the tag of the data\n",
    "        if config.reward_model.enable:\n",
    "            if config.reward_model.strategy in {\"fsdp\", \"fsdp2\"}:\n",
    "                from verl.workers.fsdp_workers import RewardModelWorker\n",
    "            elif config.reward_model.strategy == \"megatron\":\n",
    "                from verl.workers.megatron_workers import RewardModelWorker\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)\n",
    "            mapping[Role.RewardModel] = global_pool_id\n",
    "\n",
    "        # Add a reference policy worker if KL loss or KL reward is used.\n",
    "        # For TeacherStudentReflectiveTrainer, we ALWAYS need a RefPolicy to act as the Teacher.\n",
    "        # So we remove the conditional check or ensure it's always added.\n",
    "        \n",
    "        # 强制添加 RefPolicy\n",
    "        role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker)\n",
    "        mapping[Role.RefPolicy] = global_pool_id\n",
    "\n",
    "        # Load the reward manager for training and validation.\n",
    "        reward_fn = load_reward_manager(\n",
    "            config, tokenizer, num_examine=0, **config.reward_model.get(\"reward_kwargs\", {})\n",
    "        )\n",
    "        val_reward_fn = load_reward_manager(\n",
    "            config, tokenizer, num_examine=1, **config.reward_model.get(\"reward_kwargs\", {})\n",
    "        )\n",
    "        resource_pool_manager = ResourcePoolManager(resource_pool_spec=resource_pool_spec, mapping=mapping)\n",
    "\n",
    "        from verl.utils.dataset.rl_dataset import collate_fn\n",
    "\n",
    "        # Create training and validation datasets.\n",
    "        train_dataset = create_rl_dataset(config.data.train_files, config.data, tokenizer, processor, is_train=True)\n",
    "        val_dataset = create_rl_dataset(config.data.val_files, config.data, tokenizer, processor, is_train=False)\n",
    "        train_sampler = create_rl_sampler(config.data, train_dataset)\n",
    "\n",
    "        # Initialize the PPO trainer.\n",
    "        trainer = TeacherStudentReflectiveTrainer(\n",
    "            config=config,\n",
    "            tokenizer=tokenizer,\n",
    "            processor=processor,\n",
    "            role_worker_mapping=role_worker_mapping,\n",
    "            resource_pool_manager=resource_pool_manager,\n",
    "            ray_worker_group_cls=ray_worker_group_cls,\n",
    "            reward_fn=reward_fn,\n",
    "            val_reward_fn=val_reward_fn,\n",
    "            train_dataset=train_dataset,\n",
    "            val_dataset=val_dataset,\n",
    "            collate_fn=collate_fn,\n",
    "            train_sampler=train_sampler,\n",
    "        )\n",
    "        # Initialize the workers of the trainer.\n",
    "        trainer.init_workers()\n",
    "        # Start the training process.\n",
    "        trainer.fit()\n",
    "\n",
    "\n",
    "def create_rl_dataset(data_paths, data_config, tokenizer, processor, is_train=True):\n",
    "    \"\"\"Create a dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_paths: List of paths to data files.\n",
    "        data_config: The data config.\n",
    "        tokenizer (Tokenizer): The tokenizer.\n",
    "        processor (Processor): The processor.\n",
    "\n",
    "    Returns:\n",
    "        dataset (Dataset): The dataset.\n",
    "    \"\"\"\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    from verl.utils.dataset.rl_dataset import RLHFDataset\n",
    "\n",
    "    # Check if a custom dataset class is specified in the data configuration\n",
    "    # and if the path to the custom class is provided\n",
    "    if \"custom_cls\" in data_config and data_config.custom_cls.get(\"path\", None) is not None:\n",
    "        # Dynamically load the custom dataset class\n",
    "        dataset_cls = load_extern_type(data_config.custom_cls.path, data_config.custom_cls.name)\n",
    "        # Verify that the custom dataset class inherits from torch.utils.data.Dataset\n",
    "        if not issubclass(dataset_cls, Dataset):\n",
    "            raise TypeError(\n",
    "                f\"The custom dataset class '{data_config.custom_cls.name}' from \"\n",
    "                f\"'{data_config.custom_cls.path}' must inherit from torch.utils.data.Dataset\"\n",
    "            )\n",
    "    elif \"datagen\" in data_config and data_config.datagen.get(\"path\", None) is not None and is_train:\n",
    "        # If a data generation strategy is specified, use the DynamicGenDataset class\n",
    "        from verl.utils.dataset.dynamicgen_dataset import DynamicGenDataset\n",
    "\n",
    "        dataset_cls = DynamicGenDataset\n",
    "        print(\"Using DynamicGenDataset for data generation.\")\n",
    "\n",
    "    else:\n",
    "        # Use the default RLHFDataset class if no custom class is specified\n",
    "        dataset_cls = RLHFDataset\n",
    "    print(f\"Using dataset class: {dataset_cls.__name__}\")\n",
    "\n",
    "    # Instantiate the dataset using the determined dataset class\n",
    "    dataset = dataset_cls(\n",
    "        data_files=data_paths,\n",
    "        tokenizer=tokenizer,\n",
    "        processor=processor,\n",
    "        config=data_config,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_rl_sampler(data_config, dataset):\n",
    "    \"\"\"Create a sampler for the dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_config: The data config.\n",
    "        dataset (Dataset): The dataset.\n",
    "\n",
    "    Returns:\n",
    "        sampler (Sampler): The sampler.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import RandomSampler, SequentialSampler\n",
    "\n",
    "    if data_config.sampler is not None and data_config.sampler.get(\"class_path\", None) is not None:\n",
    "        curriculum_class = load_extern_type(\n",
    "            data_config.sampler.class_path,\n",
    "            data_config.sampler.class_name,\n",
    "        )\n",
    "        sampler = curriculum_class(\n",
    "            data_source=dataset,\n",
    "            data_config=data_config,\n",
    "        )\n",
    "        assert isinstance(sampler, AbstractSampler)\n",
    "        assert data_config.get(\"dataloader_num_workers\", 8) == 0, (\n",
    "            \"If using curriculum, num_workers must be 0 to prevent data caching. \"\n",
    "            \"If the dataloader caches data before the batch is done the \"\n",
    "            \"curriculum sampler won't have the opportunity to reorder it. \"\n",
    "        )\n",
    "\n",
    "    # Use a sampler to facilitate checkpoint resumption.\n",
    "    # If shuffling is enabled in the data configuration, create a random sampler.\n",
    "    elif data_config.shuffle:\n",
    "        train_dataloader_generator = torch.Generator()\n",
    "        train_dataloader_generator.manual_seed(data_config.get(\"seed\", 1))\n",
    "        sampler = RandomSampler(data_source=dataset, generator=train_dataloader_generator)\n",
    "    else:\n",
    "        # If shuffling is disabled, use a sequential sampler to iterate through the dataset in order.\n",
    "        sampler = SequentialSampler(data_source=dataset)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "dp_actor.py修改代码如下\n",
    "\"\"\"\n",
    "# Copyright 2024 Bytedance Ltd. and/or its affiliates\n",
    "# Copyright 2023-2024 SGLang Team\n",
    "# Copyright 2025 ModelBest Inc. and/or its affiliates\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Single Process Actor\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "import verl.utils.torch_functional as verl_F\n",
    "from verl import DataProto\n",
    "from verl.trainer.ppo.core_algos import agg_loss, compute_policy_loss, get_policy_loss_fn, kl_penalty\n",
    "from verl.utils.device import get_device_name, is_cuda_available, is_npu_available\n",
    "from verl.utils.fsdp_utils import FSDPModule, fsdp2_clip_grad_norm_\n",
    "from verl.utils.profiler import GPUMemoryLogger\n",
    "from verl.utils.py_functional import append_to_dict\n",
    "from verl.utils.seqlen_balancing import prepare_dynamic_batch, restore_dynamic_batch\n",
    "from verl.utils.torch_functional import logprobs_from_logits\n",
    "from verl.utils.ulysses import gather_outputs_and_unpad, ulysses_pad, ulysses_pad_and_slice_inputs\n",
    "from verl.workers.actor import BasePPOActor\n",
    "\n",
    "if is_cuda_available:\n",
    "    from flash_attn.bert_padding import index_first_axis, pad_input, rearrange, unpad_input\n",
    "elif is_npu_available:\n",
    "    from transformers.integrations.npu_flash_attention import index_first_axis, pad_input, rearrange, unpad_input\n",
    "\n",
    "\n",
    "__all__ = [\"DataParallelPPOActor\"]\n",
    "\n",
    "logger = logging.getLogger(__file__)\n",
    "logger.setLevel(os.getenv(\"VERL_LOGGING_LEVEL\", \"WARN\"))\n",
    "\n",
    "\n",
    "class DataParallelPPOActor(BasePPOActor):\n",
    "    def __init__(self, config, actor_module: nn.Module, actor_optimizer: torch.optim.Optimizer = None):\n",
    "        \"\"\"When optimizer is None, it is Reference Policy\"\"\"\n",
    "        super().__init__(config)\n",
    "        self.actor_module = actor_module\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "\n",
    "        self.use_remove_padding = self.config.get(\"use_remove_padding\", False)\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print(f\"Actor use_remove_padding={self.use_remove_padding}\")\n",
    "        self.use_fused_kernels = self.config.get(\"use_fused_kernels\", False)\n",
    "        if torch.distributed.get_rank() == 0:\n",
    "            print(f\"Actor use_fused_kernels={self.use_fused_kernels}\")\n",
    "\n",
    "        self.ulysses_sequence_parallel_size = self.config.ulysses_sequence_parallel_size\n",
    "        self.use_ulysses_sp = self.ulysses_sequence_parallel_size > 1\n",
    "\n",
    "        if self.config.entropy_from_logits_with_chunking:\n",
    "            entropy_from_logits = verl_F.entropy_from_logits_with_chunking\n",
    "        else:\n",
    "            entropy_from_logits = verl_F.entropy_from_logits\n",
    "\n",
    "        self.compute_entropy_from_logits = (\n",
    "            torch.compile(entropy_from_logits, dynamic=True)\n",
    "            if self.config.get(\"use_torch_compile\", True)  #  use torch compile by default\n",
    "            else entropy_from_logits\n",
    "        )\n",
    "        self.device_name = get_device_name()\n",
    "\n",
    "    def _forward_micro_batch(\n",
    "        self, micro_batch, temperature, calculate_entropy=False\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            entropy: # (bs, response_len)\n",
    "            log_probs: # (bs, response_len)\n",
    "        \"\"\"\n",
    "        response_length = micro_batch[\"responses\"].size(-1)\n",
    "        multi_modal_inputs = {}\n",
    "        if \"multi_modal_inputs\" in micro_batch.keys():\n",
    "            if \"image_bound\" in micro_batch[\"multi_modal_inputs\"][0]:  # minicpm-o logic\n",
    "                for key in micro_batch[\"multi_modal_inputs\"][0].keys():\n",
    "                    multi_modal_inputs[key] = [inputs[key] for inputs in micro_batch[\"multi_modal_inputs\"]]\n",
    "            else:\n",
    "                for key in micro_batch[\"multi_modal_inputs\"][0].keys():\n",
    "                    multi_modal_inputs[key] = torch.cat(\n",
    "                        [inputs[key] for inputs in micro_batch[\"multi_modal_inputs\"]], dim=0\n",
    "                    )\n",
    "\n",
    "        with torch.autocast(device_type=self.device_name, dtype=torch.bfloat16):\n",
    "            input_ids = micro_batch[\"input_ids\"]\n",
    "            batch_size, seqlen = input_ids.shape\n",
    "            attention_mask = micro_batch[\"attention_mask\"]\n",
    "            position_ids = micro_batch[\"position_ids\"]\n",
    "            entropy = None\n",
    "            if position_ids.dim() == 3:  # qwen2vl mrope\n",
    "                position_ids = position_ids.transpose(0, 1)  # (bsz, 3, seqlen) -> (3, bsz, seqlen)\n",
    "\n",
    "            if self.use_remove_padding:\n",
    "                input_ids_rmpad, indices, cu_seqlens, *_ = unpad_input(\n",
    "                    input_ids.unsqueeze(-1), attention_mask\n",
    "                )  # input_ids_rmpad (total_nnz, ...)\n",
    "                input_ids_rmpad = input_ids_rmpad.transpose(0, 1)  # (1, total_nnz)\n",
    "\n",
    "                # unpad the position_ids to align the rotary\n",
    "                if position_ids.dim() == 3:\n",
    "                    position_ids_rmpad = (\n",
    "                        index_first_axis(rearrange(position_ids, \"c b s ... -> (b s) c ...\"), indices)\n",
    "                        .transpose(0, 1)\n",
    "                        .unsqueeze(1)\n",
    "                    )  # (3, bsz, seqlen) -> (3, 1, bsz * seqlen)\n",
    "                else:\n",
    "                    position_ids_rmpad = index_first_axis(\n",
    "                        rearrange(position_ids.unsqueeze(-1), \"b s ... -> (b s) ...\"), indices\n",
    "                    ).transpose(0, 1)\n",
    "\n",
    "                if \"image_bound\" in multi_modal_inputs:\n",
    "                    from verl.utils.dataset.vision_utils import process_multi_modal_inputs_for_minicpmo\n",
    "\n",
    "                    multi_modal_inputs = process_multi_modal_inputs_for_minicpmo(\n",
    "                        input_ids, attention_mask, position_ids, cu_seqlens, multi_modal_inputs\n",
    "                    )\n",
    "\n",
    "                # for compute the log_prob\n",
    "                input_ids_rmpad_rolled = torch.roll(input_ids_rmpad, shifts=-1, dims=1)  # (1, total_nnz)\n",
    "\n",
    "                # pad and slice the inputs if sp > 1\n",
    "                if self.use_ulysses_sp:\n",
    "                    is_vlm_model = \"multi_modal_inputs\" in micro_batch.keys()\n",
    "                    if is_vlm_model:\n",
    "                        # vlm model's inputs will be sliced after embedding\n",
    "                        input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad(\n",
    "                            input_ids_rmpad,\n",
    "                            position_ids_rmpad=position_ids_rmpad,\n",
    "                            sp_size=self.ulysses_sequence_parallel_size,\n",
    "                        )\n",
    "                    else:\n",
    "                        input_ids_rmpad, position_ids_rmpad, pad_size = ulysses_pad_and_slice_inputs(\n",
    "                            input_ids_rmpad,\n",
    "                            position_ids_rmpad=position_ids_rmpad,\n",
    "                            sp_size=self.ulysses_sequence_parallel_size,\n",
    "                        )\n",
    "                    input_ids_rmpad_rolled, _, _ = ulysses_pad_and_slice_inputs(\n",
    "                        input_ids_rmpad_rolled,\n",
    "                        position_ids_rmpad=None,\n",
    "                        sp_size=self.ulysses_sequence_parallel_size,\n",
    "                    )\n",
    "\n",
    "                input_ids_rmpad_rolled = input_ids_rmpad_rolled.squeeze(0)  # ((total_nnz / sp) + pad)\n",
    "\n",
    "                # only pass input_ids and position_ids to enable flash_attn_varlen\n",
    "                extra_args = {}\n",
    "                if self.use_fused_kernels:\n",
    "                    extra_args[\"temperature\"] = temperature\n",
    "                    extra_args[\"return_dict\"] = True\n",
    "\n",
    "                output = self.actor_module(\n",
    "                    input_ids=input_ids_rmpad,\n",
    "                    attention_mask=None,\n",
    "                    position_ids=position_ids_rmpad,\n",
    "                    **multi_modal_inputs,\n",
    "                    use_cache=False,\n",
    "                    **extra_args,\n",
    "                )  # prevent model thinks we are generating\n",
    "\n",
    "                if self.use_fused_kernels:\n",
    "                    log_probs = output.log_probs.squeeze(0)  # (total_nnz,)\n",
    "                    entropy_rmpad = output.entropy.squeeze(0)  # (total_nnz,)\n",
    "\n",
    "                else:\n",
    "                    logits_rmpad = output.logits.squeeze(0)  # (total_nnz, vocab_size)\n",
    "                    logits_rmpad.div_(temperature)\n",
    "\n",
    "                    # if use_sp: ((total_nnz / sp) + pad) ; if not use_sp: (batch, seqlen)\n",
    "                    inplace_backward = True\n",
    "                    if calculate_entropy:\n",
    "                        inplace_backward = False\n",
    "                    log_probs = logprobs_from_logits(\n",
    "                        logits=logits_rmpad,\n",
    "                        labels=input_ids_rmpad_rolled,\n",
    "                        inplace_backward=inplace_backward,\n",
    "                    )\n",
    "\n",
    "                    # compute entropy\n",
    "                    if calculate_entropy:\n",
    "                        if not self.config.entropy_checkpointing:\n",
    "                            entropy_rmpad = self.compute_entropy_from_logits(logits_rmpad)  # ((total_nnz / sp) + pad)\n",
    "                        else:\n",
    "                            entropy_rmpad = torch.utils.checkpoint.checkpoint(\n",
    "                                self.compute_entropy_from_logits, logits_rmpad\n",
    "                            )\n",
    "\n",
    "                # gather log_prob if sp > 1\n",
    "                if self.use_ulysses_sp:\n",
    "                    # gather and unpad for the ulysses sp\n",
    "                    log_probs = gather_outputs_and_unpad(\n",
    "                        log_probs,\n",
    "                        gather_dim=0,\n",
    "                        unpad_dim=0,\n",
    "                        padding_size=pad_size,\n",
    "                    )\n",
    "                    if calculate_entropy:\n",
    "                        entropy_rmpad = gather_outputs_and_unpad(\n",
    "                            entropy_rmpad,\n",
    "                            gather_dim=0,\n",
    "                            unpad_dim=0,\n",
    "                            padding_size=pad_size,\n",
    "                        )\n",
    "                # pad back to (bsz, seqlen)\n",
    "                if calculate_entropy:\n",
    "                    full_entropy = pad_input(\n",
    "                        hidden_states=entropy_rmpad.unsqueeze(-1),\n",
    "                        indices=indices,\n",
    "                        batch=batch_size,\n",
    "                        seqlen=seqlen,\n",
    "                    )\n",
    "                full_log_probs = pad_input(\n",
    "                    hidden_states=log_probs.unsqueeze(-1),\n",
    "                    indices=indices,\n",
    "                    batch=batch_size,\n",
    "                    seqlen=seqlen,\n",
    "                )\n",
    "\n",
    "                # only return response part:\n",
    "                if calculate_entropy:\n",
    "                    entropy = full_entropy.squeeze(-1)[:, -response_length - 1 : -1]  # (bsz, response_length)\n",
    "                log_probs = full_log_probs.squeeze(-1)[:, -response_length - 1 : -1]  # (bsz, response_length)\n",
    "\n",
    "            else:  # not using rmpad and no ulysses sp\n",
    "                extra_args = {}\n",
    "                if self.use_fused_kernels:\n",
    "                    extra_args[\"temperature\"] = temperature\n",
    "                    extra_args[\"return_dict\"] = True\n",
    "\n",
    "                output = self.actor_module(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    **multi_modal_inputs,\n",
    "                    use_cache=False,\n",
    "                    **extra_args,\n",
    "                )  # prevent model thinks we are generating\n",
    "\n",
    "                if self.use_fused_kernels:\n",
    "                    log_probs = output.log_probs[:, -response_length - 1 : -1]\n",
    "                    entropy = output.entropy[:, -response_length - 1 : -1]  # (bsz, response_length)\n",
    "\n",
    "                else:\n",
    "                    logits = output.logits\n",
    "\n",
    "                    logits.div_(temperature)\n",
    "                    logits = logits[:, -response_length - 1 : -1, :]  # (bsz, response_length, vocab_size)\n",
    "                    log_probs = logprobs_from_logits(logits, micro_batch[\"responses\"])\n",
    "                    if calculate_entropy:\n",
    "                        if not self.config.entropy_checkpointing:\n",
    "                            entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)\n",
    "                        else:\n",
    "                            entropy = torch.utils.checkpoint.checkpoint(verl_F.entropy_from_logits, logits)\n",
    "\n",
    "            return entropy, log_probs\n",
    "\n",
    "    def _optimizer_step(self):\n",
    "        assert self.config.grad_clip is not None\n",
    "\n",
    "        if isinstance(self.actor_module, FSDP):\n",
    "            grad_norm = self.actor_module.clip_grad_norm_(max_norm=self.config.grad_clip)\n",
    "        elif isinstance(self.actor_module, FSDPModule):\n",
    "            grad_norm = fsdp2_clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.grad_clip)\n",
    "        else:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(self.actor_module.parameters(), max_norm=self.config.grad_clip)\n",
    "\n",
    "        # if grad_norm is not finite, skip the update\n",
    "        if not torch.isfinite(grad_norm):\n",
    "            print(f\"WARN: rank {torch.distributed.get_rank()} grad_norm is not finite: {grad_norm}\")\n",
    "            self.actor_optimizer.zero_grad()\n",
    "        else:\n",
    "            self.actor_optimizer.step()\n",
    "        return grad_norm\n",
    "\n",
    "    @GPUMemoryLogger(role=\"dp actor\", logger=logger)\n",
    "    def compute_log_prob(self, data: DataProto, calculate_entropy=False) -> torch.Tensor:\n",
    "        \"\"\"Compute the log probability of the responses given input_ids, attention_mask and position_ids\"\"\"\n",
    "        # set to eval\n",
    "        self.actor_module.eval()\n",
    "\n",
    "        micro_batch_size = data.meta_info[\"micro_batch_size\"]\n",
    "        temperature = data.meta_info[\"temperature\"]\n",
    "        use_dynamic_bsz = data.meta_info[\"use_dynamic_bsz\"]\n",
    "        has_multi_modal_inputs = \"multi_modal_inputs\" in data.non_tensor_batch.keys()\n",
    "        select_keys = [\"responses\", \"input_ids\", \"attention_mask\", \"position_ids\"]\n",
    "        non_tensor_select_keys = [\"multi_modal_inputs\"] if has_multi_modal_inputs else []\n",
    "\n",
    "        data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)\n",
    "\n",
    "        if use_dynamic_bsz:\n",
    "            max_token_len = data.meta_info[\"max_token_len\"] * self.ulysses_sequence_parallel_size\n",
    "            micro_batches, batch_idx_list = prepare_dynamic_batch(data, max_token_len=max_token_len)\n",
    "        else:\n",
    "            micro_batches = data.split(micro_batch_size)\n",
    "\n",
    "        log_probs_lst = []\n",
    "        entropy_lst = []\n",
    "        for micro_batch in micro_batches:\n",
    "            model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\n",
    "            with torch.no_grad():\n",
    "                entropy, log_probs = self._forward_micro_batch(\n",
    "                    model_inputs, temperature=temperature, calculate_entropy=calculate_entropy\n",
    "                )\n",
    "            log_probs_lst.append(log_probs)\n",
    "            if calculate_entropy:\n",
    "                entropy_lst.append(entropy)\n",
    "\n",
    "        log_probs = torch.concat(log_probs_lst, dim=0)\n",
    "        entropys = None\n",
    "        if calculate_entropy:\n",
    "            entropys = torch.concat(entropy_lst, dim=0)\n",
    "\n",
    "        if use_dynamic_bsz:\n",
    "            log_probs = restore_dynamic_batch(log_probs, batch_idx_list)\n",
    "            if calculate_entropy:\n",
    "                entropys = restore_dynamic_batch(entropys, batch_idx_list)\n",
    "\n",
    "        return log_probs, entropys\n",
    "\n",
    "    @GPUMemoryLogger(role=\"dp actor\", logger=logger)\n",
    "    def update_policy(self, data: DataProto):\n",
    "        # make sure we are in training mode\n",
    "        self.actor_module.train()\n",
    "\n",
    "        temperature = data.meta_info[\"temperature\"]\n",
    "\n",
    "        select_keys = [\n",
    "            \"responses\",\n",
    "            \"response_mask\",\n",
    "            \"input_ids\",\n",
    "            \"attention_mask\",\n",
    "            \"position_ids\",\n",
    "            \"old_log_probs\",\n",
    "            \"advantages\",\n",
    "        ]\n",
    "        if self.config.use_kl_loss:\n",
    "            select_keys.append(\"ref_log_prob\")\n",
    "\n",
    "        has_multi_modal_inputs = \"multi_modal_inputs\" in data.non_tensor_batch.keys()\n",
    "        non_tensor_select_keys = [\"multi_modal_inputs\"] if has_multi_modal_inputs else []\n",
    "\n",
    "        data = data.select(batch_keys=select_keys, non_tensor_batch_keys=non_tensor_select_keys)\n",
    "\n",
    "        # Split to make minibatch iterator\n",
    "        mini_batches = data.split(self.config.ppo_mini_batch_size)\n",
    "\n",
    "        metrics = {}\n",
    "        for _ in range(self.config.ppo_epochs):\n",
    "            for batch_idx, mini_batch in enumerate(mini_batches):\n",
    "                if self.config.use_dynamic_bsz:\n",
    "                    max_token_len = self.config.ppo_max_token_len_per_gpu * self.ulysses_sequence_parallel_size\n",
    "                    micro_batches, _ = prepare_dynamic_batch(mini_batch, max_token_len=max_token_len)\n",
    "                else:\n",
    "                    self.gradient_accumulation = (\n",
    "                        self.config.ppo_mini_batch_size // self.config.ppo_micro_batch_size_per_gpu\n",
    "                    )\n",
    "                    micro_batches = mini_batch.split(self.config.ppo_micro_batch_size_per_gpu)\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "\n",
    "                for micro_batch in micro_batches:\n",
    "                    micro_batch_metrics = {}\n",
    "                    model_inputs = {**micro_batch.batch, **micro_batch.non_tensor_batch}\n",
    "                    response_mask = model_inputs[\"response_mask\"]\n",
    "                    old_log_prob = model_inputs[\"old_log_probs\"]\n",
    "                    advantages = model_inputs[\"advantages\"]\n",
    "\n",
    "                    clip_ratio = self.config.clip_ratio\n",
    "                    clip_ratio_low = (\n",
    "                        self.config.clip_ratio_low if self.config.clip_ratio_low is not None else clip_ratio\n",
    "                    )\n",
    "                    clip_ratio_high = (\n",
    "                        self.config.clip_ratio_high if self.config.clip_ratio_high is not None else clip_ratio\n",
    "                    )\n",
    "                    clip_ratio_c = self.config.get(\"clip_ratio_c\", 3.0)\n",
    "                    entropy_coeff = self.config.entropy_coeff\n",
    "                    loss_agg_mode = self.config.loss_agg_mode\n",
    "\n",
    "                    calculate_entropy = False\n",
    "                    if entropy_coeff != 0:\n",
    "                        calculate_entropy = True\n",
    "                    entropy, log_prob = self._forward_micro_batch(\n",
    "                        model_inputs, temperature=temperature, calculate_entropy=calculate_entropy\n",
    "                    )\n",
    "\n",
    "                    # =========================================================================\n",
    "                    # 修复: 健壮地读取 loss_mode\n",
    "                    # =========================================================================\n",
    "                    if isinstance(self.config.policy_loss, str):\n",
    "                        loss_mode = self.config.policy_loss\n",
    "                    else:\n",
    "                        loss_mode = self.config.policy_loss.get(\"loss_mode\", \"vanilla\")\n",
    "\n",
    "                    # =========================================================================\n",
    "                    # 新增: distillation 模式 (Pure Policy Gradient / No Clipping)\n",
    "                    # =========================================================================\n",
    "                    if loss_mode == \"distillation\":\n",
    "                        # 直接最大化 (advantages * log_prob)\n",
    "                        # 我们假设 advantages 已经包含了 (log P_T - log P_S) 的信号\n",
    "                        # 这是一个无偏的 KL 散度梯度估计\n",
    "                        pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
    "                        \n",
    "                        # 占位符，避免日志报错\n",
    "                        pg_clipfrac = torch.tensor(0.0, device=pg_loss.device)\n",
    "                        ppo_kl = torch.tensor(0.0, device=pg_loss.device)\n",
    "                        pg_clipfrac_lower = torch.tensor(0.0, device=pg_loss.device)\n",
    "\n",
    "                    elif loss_mode == \"vanilla\":\n",
    "                        pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = compute_policy_loss(\n",
    "                            old_log_prob=old_log_prob,\n",
    "                            log_prob=log_prob,\n",
    "                            advantages=advantages,\n",
    "                            response_mask=response_mask,\n",
    "                            cliprange=clip_ratio,\n",
    "                            cliprange_low=clip_ratio_low,\n",
    "                            cliprange_high=clip_ratio_high,\n",
    "                            clip_ratio_c=clip_ratio_c,\n",
    "                            loss_agg_mode=loss_agg_mode,\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        policy_loss_fn = get_policy_loss_fn(loss_mode)\n",
    "                        pg_loss, pg_clipfrac, ppo_kl, pg_clipfrac_lower = policy_loss_fn(\n",
    "                            old_log_prob=old_log_prob,\n",
    "                            log_prob=log_prob,\n",
    "                            advantages=advantages,\n",
    "                            response_mask=response_mask,\n",
    "                            loss_agg_mode=loss_agg_mode,\n",
    "                            config=self.config,\n",
    "                        )\n",
    "\n",
    "                    if entropy_coeff != 0:\n",
    "                        entropy_loss = agg_loss(loss_mat=entropy, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)\n",
    "                        policy_loss = pg_loss - entropy_loss * entropy_coeff\n",
    "                    else:\n",
    "                        policy_loss = pg_loss\n",
    "\n",
    "                    if self.config.use_kl_loss:\n",
    "                        ref_log_prob = model_inputs[\"ref_log_prob\"]\n",
    "                        kld = kl_penalty(\n",
    "                            logprob=log_prob, ref_logprob=ref_log_prob, kl_penalty=self.config.kl_loss_type\n",
    "                        )\n",
    "                        kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)\n",
    "\n",
    "                        policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef\n",
    "                        micro_batch_metrics[\"actor/kl_loss\"] = kl_loss.detach().item()\n",
    "                        micro_batch_metrics[\"actor/kl_coef\"] = self.config.kl_loss_coef\n",
    "\n",
    "                    if self.config.use_dynamic_bsz:\n",
    "                        loss = policy_loss * (response_mask.shape[0] / self.config.ppo_mini_batch_size)\n",
    "                    else:\n",
    "                        loss = policy_loss / self.gradient_accumulation\n",
    "                    loss.backward()\n",
    "\n",
    "                    micro_batch_metrics.update(\n",
    "                        {\n",
    "                            \"actor/pg_loss\": pg_loss.detach().item(),\n",
    "                            \"actor/pg_clipfrac\": pg_clipfrac.detach().item(),\n",
    "                            \"actor/ppo_kl\": ppo_kl.detach().item(),\n",
    "                            \"actor/pg_clipfrac_lower\": pg_clipfrac_lower.detach().item(),\n",
    "                        }\n",
    "                    )\n",
    "                    append_to_dict(metrics, micro_batch_metrics)\n",
    "\n",
    "                grad_norm = self._optimizer_step()\n",
    "                mini_batch_metrics = {\"actor/grad_norm\": grad_norm.detach().item()}\n",
    "                append_to_dict(metrics, mini_batch_metrics)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        return metrics\n",
    "\"\"\"\n",
    "启动脚本代码如下\n",
    "\"\"\"\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "export DATA_DIR='/data/kcl/lpy/data'\n",
    "WAND_PROJECT=\"self_reflect\"\n",
    "export SWANLAB_API_KEY=\"DDMUFxYQU6Om1ZuHWYoxm\"           # 设置在线跟踪模式API\n",
    "export BASE_MODEL='/data/kcl/lpy/models/qwen3_4b_instruct'\n",
    "export EXPERIMENT_NAME=sr_dl_teacher_student_gsm8k_qwen3_4b_instruct\n",
    "N_NODES=1\n",
    "n_gpus_per_node=8\n",
    "export BASE_DIR=$(pwd)\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "export VLLM_USE_V1=1\n",
    "PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_self_reflective \\\n",
    " data.train_files=$DATA_DIR/gsm8k/train.parquet \\\n",
    " data.val_files=$DATA_DIR/gsm8k/test.parquet \\\n",
    " data.train_batch_size=256 \\\n",
    " data.max_prompt_length=512 \\\n",
    " data.max_response_length=512 \\\n",
    " actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    " actor_rollout_ref.actor.optim.lr=5e-7 \\\n",
    " actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n",
    " actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n",
    " actor_rollout_ref.actor.policy_loss=distillation \\\n",
    " actor_rollout_ref.rollout.name=vllm \\\n",
    " actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n",
    " actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
    " actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
    " actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
    " critic.optim.lr=1e-5 \\\n",
    " critic.model.path=$BASE_MODEL \\\n",
    " critic.ppo_micro_batch_size_per_gpu=4 \\\n",
    " algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    " trainer.logger=console \\\n",
    " trainer.val_before_train=False \\\n",
    " trainer.n_gpus_per_node=$n_gpus_per_node \\\n",
    " trainer.nnodes=$N_NODES \\\n",
    " trainer.save_freq=60 \\\n",
    " trainer.test_freq=60 \\\n",
    " trainer.logger=\"['swanlab', 'console']\" \\\n",
    " trainer.project_name=$WAND_PROJECT \\\n",
    " trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    " trainer.default_local_dir=${BASE_DIR}/verl_checkpoints/$EXPERIMENT_NAME \\\n",
    " trainer.total_epochs=15 2>&1 | tee verl_log/$EXPERIMENT_NAME.log\n",
    "\"\"\"\n",
    "现在运行我发现问题，日志输出如下\n",
    "\"\"\"\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ==================== Teacher-Student Reflection Debug (Step 1) ====================\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- [0] Summary Generation Input ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>system\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m You are a helpful math assistant.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>user\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Question: Tom receives a $12 allowance per month. In the first week, he spends a third of it; in the second week, he spends a quarter of what he has left. How much money does he have left to finish the month? Let's think step by step and output the final answer after \"####\".\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Standard Answer: 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Student Answer: Let's solve this step by step.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Tom receives a $12 allowance per month.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **First week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a third of it.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A third of $12 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{3} \\times 12 = 4\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after the first week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 12 - 4 = 8 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **Second week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a quarter of what he has left.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A quarter of $8 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{4} \\times 8 = 2\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after spending in the second week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 8 - 2 = 6 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Task: Verify the Student Answer step-by-step. Is it correct?\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Answer concisely.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>assistant\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- [1] Summary Output ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Yes, the student answer is correct.  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Step-by-step:  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Initial allowance: $12  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - First week: spends $1/3 of $12 = $4 → remaining: $12 - $4 = $8  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Second week: spends $1/4 of $8 = $2 → remaining: $8 - $2 = $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Final amount left: $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- [2] Teacher LogProb Input (Full Context) ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>system\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m You are a helpful math assistant.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>user\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Problem: Tom receives a $12 allowance per month. In the first week, he spends a third of it; in the second week, he spends a quarter of what he has left. How much money does he have left to finish the month? Let's think step by step and output the final answer after \"####\".\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Analysis/Hint: Yes, the student answer is correct.  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Step-by-step:  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Initial allowance: $12  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - First week: spends $1/3 of $12 = $4 → remaining: $12 - $4 = $8  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m - Second week: spends $1/4 of $8 = $2 → remaining: $8 - $2 = $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Final amount left: $6  \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Based on the analysis above, please provide the correct, step-by-step solution.<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m <|im_start|>assistant\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Let's solve this step by step.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Tom receives a $12 allowance per month.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **First week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a third of it.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A third of $12 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{3} \\times 12 = 4\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after the first week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 12 - 4 = 8 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **Second week:**\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He spends a quarter of what he has left.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A quarter of $8 is:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\frac{1}{4} \\times 8 = 2\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m So, after spending in the second week, he has:\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 8 - 2 = 6 \\text{ dollars left}\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m #### 6<|im_end|>\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ============================================================\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ==================== Token-Level KL Analysis (Step 1) ====================\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- Sequence Alignment Check ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m [OK] Token IDs match perfectly.\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m --- Token-wise KL Breakdown (First 50 tokens) ---\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Token           | ID     | S_LogP   | T_LogP   | Diff    \n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ---------------------------------------------------------------------------\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Let             | 10061  | -0.00   | -12.61   | -12.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 's              | 594    | -0.00   | -15.50   | -15.50\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  solve          | 11625  | -0.04   | -14.15   | -14.11\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  this           | 419    | -0.01   | -18.05   | -18.04\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  step           | 3019   | -0.00   | -17.83   | -17.83\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  by             | 553    | -0.00   | -17.40   | -17.40\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  step           | 3019   | -0.00   | -17.83   | -17.83\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m .\\n\\n           | 382    | -0.31   | -13.00   | -12.69\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m Tom             | 24732  | -0.25   | -9.66   | -9.40\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  receives       | 21189  | -0.00   | -14.84   | -14.83\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  a              | 264    | -0.00   | -14.08   | -14.08\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  $              | 400    | -0.01   | -14.63   | -14.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 1               | 16     | -0.00   | -11.31   | -11.31\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 2               | 17     | -0.00   | -12.61   | -12.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  allowance      | 40358  | -0.00   | -18.77   | -18.77\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  per            | 817    | -0.00   | -19.28   | -19.28\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  month          | 2254   | -0.00   | -17.45   | -17.45\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m .\\n\\n           | 382    | -0.00   | -13.00   | -13.00\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **              | 334    | -0.00   | -13.63   | -13.63\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m First           | 5338   | -0.85   | -14.77   | -13.93\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  week           | 2003   | -0.00   | -22.22   | -22.22\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m :               | 25     | -0.25   | -16.95   | -16.70\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m **\\n            | 1019   | -0.00   | -14.02   | -14.02\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m He              | 1519   | -0.00   | -13.55   | -13.55\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  spends         | 37102  | -0.00   | -14.45   | -14.45\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  a              | 264    | -0.00   | -14.08   | -14.08\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  third          | 4843   | -0.00   | -18.66   | -18.66\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  of             | 315    | -0.00   | -15.58   | -15.58\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  it             | 432    | -0.01   | -16.15   | -16.14\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m .\\n\\n           | 382    | -0.00   | -13.00   | -13.00\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m A               | 32     | -0.29   | -11.69   | -11.40\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  third          | 4843   | -0.00   | -18.66   | -18.66\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  of             | 315    | -0.00   | -15.58   | -15.58\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  $              | 400    | -0.00   | -14.63   | -14.63\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 1               | 16     | -0.00   | -11.31   | -11.31\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 2               | 17     | -0.00   | -12.61   | -12.61\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  is             | 374    | -0.00   | -14.92   | -14.92\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m :\\n             | 510    | -0.00   | -15.54   | -15.54\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m $$              | 14085  | -0.02   | -9.88   | -9.86\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\n              | 198    | -0.00   | -13.74   | -13.74\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m \\               | 59     | -0.00   | -11.16   | -11.16\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m frac            | 37018  | -0.00   | -10.91   | -10.91\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m {               | 90     | -0.00   | -12.13   | -12.13\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 1               | 16     | -0.00   | -11.31   | -11.31\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m }{              | 15170  | -0.00   | -9.66   | -9.66\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m 3               | 18     | -0.00   | -13.41   | -13.41\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m }               | 92     | -0.00   | -8.97   | -8.97\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m  \\              | 1124   | -0.00   | -12.17   | -12.17\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m times           | 15136  | -0.00   | -11.63   | -11.63\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m                 | 220    | -0.00   | -16.90   | -16.90\n",
    "\u001b[36m(TaskRunner pid=1613436)\u001b[0m ============================================================\n",
    "\"\"\"\n",
    "Token-wise KL Breakdown的日志里显示S_LogP都很低，接近0，T_LogP都很大，你分析下原因，是不是哪里出问题了。\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96a394a2-8b8f-4de8-95b8-939d5f1013a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常深刻且关键的问题。\n",
      "\n",
      "简短的回答是：**不会破坏，恰恰相反，设置为 1.0 是实现正确的 On-Policy Distillation（以及 PPO/KL 散度计算）的必要条件。**\n",
      "\n",
      "如果不设置为 1.0（特别是你现在的日志显示 Student 似乎用了极低的温度），你的 Distillation 实际上是**失效**的。\n",
      "\n",
      "以下是详细的数学和逻辑解释：\n",
      "\n",
      "### 1. 区分“生成温度”与“计算温度”\n",
      "\n",
      "你需要区分两个阶段的 Temperature：\n",
      "\n",
      "1.  **Rollout / Generation 阶段（生成时）**：\n",
      "    *   这里 Temperature 控制**探索性**。\n",
      "    *   如果你设置 $T=0.1$，Student 会生成非常确定性的回答。\n",
      "    *   如果你设置 $T=1.0$，Student 会生成多样性的回答。\n",
      "    *   **On-Policy 的含义**：指数据必须是由当前 Student 策略生成的。只要数据是 Student 产生的，无论生成时用了什么温度，它都是 On-Policy 数据。\n",
      "\n",
      "2.  **Forward / Compute LogProb 阶段（计算概率时）**：\n",
      "    *   这里 Temperature 决定了**概率分布的形状**。\n",
      "    *   Distillation（蒸馏）的核心数学目标是最小化两个分布的距离（通常是 KL 散度）。\n",
      "    *   模型的**真实分布**是由原始 Logits 定义的（即 $T=1.0$）。\n",
      "\n",
      "### 2. 为什么计算概率时必须是 T=1.0？\n",
      "\n",
      "假设 Student 输出的 Logits 为 $z_S$，Teacher 输出的 Logits 为 $z_T$。\n",
      "\n",
      "#### 情况 A：如果你使用 T=1.0 (标准做法)\n",
      "概率 $P(x) = \\text{Softmax}(z)$。\n",
      "LogProb $\\log P(x) \\approx z_x - \\text{Const}$。\n",
      "你的 Reward (Reverse KL) 近似于：\n",
      "$$ R \\approx \\log P_T(x) - \\log P_S(x) \\approx z_T - z_S $$\n",
      "这意味着：**你要拉近 Student 的 Logits 和 Teacher 的 Logits**。这是蒸馏的本质——让 Student 学习 Teacher 的“软标签”（Soft Targets）。\n",
      "\n",
      "#### 情况 B：如果你使用极低的 T (例如你日志中的情况)\n",
      "你的日志显示 `S_LogP = -0.00`。这说明在计算概率时，系统使用了极低的温度（比如 $T=0.01$），导致 Softmax 变成了 Hardmax。\n",
      "$$ P_S(x) \\approx 1.0 \\implies \\log P_S(x) \\approx 0 $$\n",
      "\n",
      "此时你的 Reward 变成了：\n",
      "$$ R \\approx \\log P_T(x) - 0 $$\n",
      "**后果**：\n",
      "1.  **Student 的梯度消失/截断**：因为 $\\log P_S(x)$ 已经是 0（最大值），Student 认为自己已经“完美”确信了这个 Token，无法再优化自己的分布。\n",
      "2.  **数值不稳定性**：只要 Student 稍微偏离一点点，在低温下 LogProb 就会从 0 瞬间变成 $-\\infty$。\n",
      "3.  **失去了“蒸馏”的意义**：你不再是学习 Teacher 的分布，而是单纯地在用 Teacher 的 LogProb 给 Student 的 Sample 打分，但 Student 自身的分布信息被低温抹去了。\n",
      "\n",
      "### 3. 你的日志暴露的问题\n",
      "\n",
      "看看你的日志：\n",
      "```text\n",
      "Token | S_LogP | T_LogP | Diff\n",
      "Let   | -0.00  | -12.61 | -12.61\n",
      "```\n",
      "这意味着：\n",
      "*   Student 说：“我在 $T \\approx 0$ 的温度下，有 100% 的信心输出 'Let'。”\n",
      "*   Teacher 说：“我在 $T \\approx 1$ 的温度下（推测），觉得 'Let' 的概率很低。”\n",
      "\n",
      "由于 `S_LogP` 被人为地“锐化”到了 0，计算出的 `Diff` 完全由 `T_LogP` 主导。这实际上把 Distillation 变成了一个 **Reinforcement Learning with a Fixed Reward Model**（Teacher 变成了单纯的打分器），而不是让两个模型的分布互相靠近。\n",
      "\n",
      "### 4. 正确的 Distillation 流程\n",
      "\n",
      "为了实现真正的 On-Policy Distillation，流程应该是：\n",
      "\n",
      "1.  **Student 生成 (Rollout)**：\n",
      "    *   可以使用 $T=0.7$ 或 $1.0$（甚至 $T=0.1$ 如果你希望它探索得少一点）。这是为了产生样本 $y$。\n",
      "    *   *注：你的代码里似乎生成时就用了很低的温度，或者 meta_info 传导导致计算时用了低迷温度。*\n",
      "\n",
      "2.  **计算概率 (Forward)**：\n",
      "    *   拿着样本 $y$，计算 **Teacher** 对 $y$ 的 Logits（$T=1.0$）。\n",
      "    *   拿着样本 $y$，计算 **Student** 对 $y$ 的 Logits（$T=1.0$）。\n",
      "    *   **核心理由**：我们要比较的是两个神经网络在“原始状态”下对这个 Token 的看法差异。\n",
      "\n",
      "### 结论\n",
      "\n",
      "**将 `compute_log_prob` 时的 `temperature` 强制设为 1.0 是绝对正确的，也是必须的。**\n",
      "\n",
      "这不会破坏 On-Policy，因为“On-Policy”指的是**样本 $y$ 是由 Student 生成的**。至于我们如何计算 $y$ 的概率来更新梯度，必须使用原始 Logits ($T=1.0$) 才能保证梯度的数学正确性。\n",
      "\n",
      "**建议操作：**\n",
      "请务必按照我上一条回答，在 `fit` 函数中，在调用 `compute_log_prob` 之前，强制将 `meta_info['temperature']` 设置为 `1.0`。\n",
      "\n",
      "```python\n",
      "# 确保 Teacher 和 Student 都在同一把“尺子”（T=1.0）下衡量概率\n",
      "teacher_batch.meta_info['temperature'] = 1.0\n",
      "batch.meta_info['temperature'] = 1.0 \n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "如果设置为1.0会破坏我想要训练on policy distillation的目的吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38f74224-2fa7-480e-8bc2-cda82a07be9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常奇怪的现象。文本看起来只有几百个 Token，远未达到 1024 的限制，却突然截断了。\n",
      "\n",
      "这通常有三种可能：\n",
      "1.  **配置未生效**：代码读取 `max_response_length` 失败，回退到了某个默认值（比如 256）。\n",
      "2.  **EOS 误判**：模型生成了一个特殊的 Token，被 Tokenizer 判定为结束，或者被 `skip_special_tokens=True` 隐去了，导致看起来像截断。\n",
      "3.  **vLLM 内部限制**：vLLM 启动时可能锁定了最大生成长度。\n",
      "\n",
      "为了彻底解决这个问题，请在 `_prepare_summary_generation_batch` 中做以下 **3 点修改**：\n",
      "\n",
      "1.  **强制硬编码一个大数值**（例如 4096），排除配置读取错误的可能性。\n",
      "2.  **打印调试信息**：输出 Summary 的 Prompt 长度和设置的 `max_new_tokens`，确信参数传进去了。\n",
      "3.  **增加 `ignore_eos=False`**：显式告诉 vLLM 不要忽略结束符（虽然默认就是 False，但强调一下）。\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py`：\n",
      "\n",
      "```python\n",
      "    def _prepare_summary_generation_batch(self, batch: DataProto, raw_prompts: List[str], ground_truths: List[str]) -> DataProto:\n",
      "        # ... (前文不变) ...\n",
      "\n",
      "        # === FIX: 增强 Summary 生成的鲁棒性 ===\n",
      "        \n",
      "        # 1. 尝试读取配置，如果读不到，直接给 4096\n",
      "        # 注意：OmegaConf 有时需要用 .key 访问，有时用 get\n",
      "        try:\n",
      "            max_tokens = self.config.data.max_response_length\n",
      "        except:\n",
      "            max_tokens = 4096\n",
      "            \n",
      "        # 强制保底，防止配置里只有 256 之类的\n",
      "        if max_tokens < 1024:\n",
      "            max_tokens = 2048\n",
      "            \n",
      "        # 2. 打印调试信息 (只打印一次)\n",
      "        if self.global_steps == 1:\n",
      "            print(f\">>> [Summary Debug] Prompt Count: {len(input_ids_list)}\")\n",
      "            print(f\">>> [Summary Debug] Avg Prompt Len: {np.mean([len(t) for t in input_ids_list]):.1f}\")\n",
      "            print(f\">>> [Summary Debug] Setting max_new_tokens = {max_tokens}\")\n",
      "\n",
      "        summary_batch.meta_info = {\n",
      "            \"do_sample\": True,\n",
      "            \"temperature\": 0.1,  # 低温有助于逻辑连贯\n",
      "            \"max_new_tokens\": max_tokens, # <--- 确保这里是大数值\n",
      "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
      "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
      "            \"ignore_eos\": False, # 允许模型自己决定何时结束\n",
      "        }\n",
      "        \n",
      "        return summary_batch\n",
      "```\n",
      "\n",
      "**同时，请检查你的启动脚本**：\n",
      "确认 `data.max_response_length` 确实被设置为了 1024 或更高。\n",
      "\n",
      "如果修改后依然截断，那只有一种可能：**模型自己输出了 EOS**。\n",
      "也就是说，模型写到 `$70` 的时候，觉得自己写完了（或者写错了想重来），输出了 `<|im_end|>`。\n",
      "如果是这种情况，说明 Teacher 模型在 Summary 任务上有点“不知道该怎么结尾”。\n",
      "**解决方法**：在 `summary_batch.meta_info` 中尝试把 `temperature` 稍微调高一点（例如 `0.5`），或者修改 Summary 的 Prompt 指令，加上一句 `\"Ensure your analysis is complete.\"`。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "修改之后日志如下\n",
    "\"\"\"\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m --- [1] Summary Output ---\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m Let's verify the student's reasoning step by step:\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 1. **Trade-in value**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    80% of $150 = $120 → correct.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 2. **Net cash paid**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    $80 - $10 = $70 → correct.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 3. **Game value**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    $30 game received for free → this is value Tom receives.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 4. **Total value Tom receives**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Trade-in ($120) + game ($30) = $150 → correct.\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 5. **Net cost to Tom**:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Cash paid: $70  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Value received: $30 (game)  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    So net outflow = $70 - $30 = $40\n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m 6. **What is the price of the NES?**  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The NES was purchased with:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    - A $120 trade-in credit (store gives it to him)  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    - $70 cash paid  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    - He gets $30 in value (the game)  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The trade-in credit offsets part of the price.  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The price of the NES must equal the amount Tom effectively pays, which is:  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Net cash paid ($70) minus value of free item ($30) = $40  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    But the store gives $120 in trade-in — this is money *received* by Tom, not paid.  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    So the total value of the transaction is $150 (trade-in + game).  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    The only outflow is $70 in cash.  \n",
    "\u001b[36m(TaskRunner pid=344141)\u001b[0m    Therefore, the NES must have been priced at $150 — because that’s the total value of what Tom received (in value), and the means of payment were $70\n",
    "\"\"\"\n",
    "Summary Output看起来还是截断了，我已经设置了token限制是1024，但看起来没有生效？ 日志里的输出远不到1024就截断了\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db50b697-57a7-4643-8a36-a11c31b9a3f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个指标的计算逻辑分为两步：**单题打分** 和 **汇总平均**。\n",
      "\n",
      "具体的代码位置和逻辑如下：\n",
      "\n",
      "### 1. 汇总平均的逻辑 (Mean Calculation)\n",
      "**位置**：`verl/trainer/ppo/metric_utils.py`\n",
      "**函数**：`process_validation_metrics` 和 `compute_score_metrics`\n",
      "\n",
      "这是生成 `mean@1` 这个数值的地方。逻辑非常简单，就是对所有样本的分数取平均值。\n",
      "\n",
      "```python\n",
      "# verl/trainer/ppo/metric_utils.py\n",
      "\n",
      "def compute_score_metrics(scores, n=1):\n",
      "    \"\"\"\n",
      "    scores: 一个列表，包含所有验证集样本的得分 (0.0 或 1.0)\n",
      "    n: 每个 Prompt 采样的回复数量 (验证集通常 n=1)\n",
      "    \"\"\"\n",
      "    total = len(scores)\n",
      "    # 核心逻辑：直接用 numpy 求平均\n",
      "    # 这就是 mean@1 的来源\n",
      "    mean = np.mean(scores) \n",
      "    \n",
      "    return {\n",
      "        f\"mean@{n}\": mean,  # -> \"mean@1\"\n",
      "        # ... 其他如 min, max ...\n",
      "    }\n",
      "\n",
      "def process_validation_metrics(data_sources, ...):\n",
      "    # ...\n",
      "    # 根据 data_source (例如 \"openai/gsm8k\") 对分数进行分组\n",
      "    # 然后调用上面的 compute_score_metrics\n",
      "    # ...\n",
      "```\n",
      "\n",
      "### 2. 单题打分的逻辑 (0/1 Logic)\n",
      "**位置**：`verl/utils/reward_score/gsm8k.py` (通常情况下)\n",
      "**函数**：`compute_score`\n",
      "\n",
      "这是决定每个样本是得 1 分还是 0 分的地方。逻辑是**正则匹配**。\n",
      "\n",
      "```python\n",
      "# verl/utils/reward_score/gsm8k.py\n",
      "\n",
      "def compute_score(solution_str, ground_truth, ...):\n",
      "    # 1. 提取 Ground Truth (#### 后面的数字)\n",
      "    target = extract_answer(ground_truth)\n",
      "    \n",
      "    # 2. 提取 Student 回答 (#### 后面的数字)\n",
      "    # 如果没有 ####，尝试找最后一个数字\n",
      "    prediction = extract_answer(solution_str)\n",
      "    \n",
      "    # 3. 字符串比对\n",
      "    if prediction == target:\n",
      "        return 1.0\n",
      "    else:\n",
      "        return 0.0\n",
      "```\n",
      "\n",
      "### 3. 指标名称的拼接逻辑\n",
      "**位置**：`ray_trainer.py` 中的 `_validate` 方法\n",
      "\n",
      "你看到的字符串 `val-core/openai/gsm8k/reward/mean@1` 是这样拼出来的：\n",
      "\n",
      "```python\n",
      "# ray_trainer.py\n",
      "\n",
      "# 1. metric_sec: \"val-core\" \n",
      "# (逻辑：如果是 reward 或 acc，且是 mean/maj/best，归类为核心指标)\n",
      "metric_sec = \"val-core\"\n",
      "\n",
      "# 2. data_source: \"openai/gsm8k\"\n",
      "# (来自 parquet 数据文件中的 data_source 列，或者默认值)\n",
      "\n",
      "# 3. var_name: \"reward\"\n",
      "# (因为你计算的是 reward 分数)\n",
      "\n",
      "# 4. metric_name: \"mean@1\"\n",
      "# (来自 metric_utils.py 计算的结果)\n",
      "\n",
      "# 最终拼接：\n",
      "pfx = f\"{metric_sec}/{data_source}/{var_name}/{metric_name}\"\n",
      "```\n",
      "\n",
      "### 总结\n",
      "*   **计算公式**：$\\frac{\\text{回答正确的题目数}}{\\text{总题目数}}$\n",
      "*   **代码核心**：`np.mean(scores)`\n",
      "*   **文件路径**：`verl/trainer/ppo/metric_utils.py`\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我的意思是metric里val-core/openai/gsm8k/reward/mean@1这个具体计算逻辑是什么样的，代码是在哪里写的？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ef4fa61-7c00-4046-b604-a50cd71cca0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个很常见的问题。原因很简单：你在重写 `TeacherStudentReflectiveTrainer` 的 `fit` 函数时，**把父类 `RayPPOTrainer` 中关于 Validation (测试) 的代码块给漏掉了**。\n",
      "\n",
      "虽然你在启动脚本里传了参数，且 `main` 函数里也初始化了 `val_reward_fn`，但因为 `fit` 循环里没有调用它，所以永远不会执行测试。\n",
      "\n",
      "请在 `verl/trainer/ppo/teacher_student_reflective_trainer.py` 的 `fit` 函数中补上这段逻辑。\n",
      "\n",
      "### 修改位置\n",
      "\n",
      "在 `fit` 函数的末尾，**在 `logger.log` 之前**，插入验证逻辑。\n",
      "\n",
      "```python\n",
      "    def fit(self):\n",
      "        # ... (前面的代码保持不变) ...\n",
      "\n",
      "                with marked_timer(\"update_actor\", timing_raw):\n",
      "                    actor_output = self.actor_rollout_wg.update_actor(batch)\n",
      "                    actor_output_metrics = reduce_metrics(actor_output.meta_info[\"metrics\"])\n",
      "                    metrics.update(actor_output_metrics)\n",
      "\n",
      "                # =================================================================\n",
      "                # === FIX: 添加 Validation/Testing 逻辑 ===\n",
      "                # =================================================================\n",
      "                # 检查是否满足测试条件：\n",
      "                # 1. 存在验证集 Reward Function\n",
      "                # 2. test_freq 设置大于 0\n",
      "                # 3. 步数整除 OR 是最后一步\n",
      "                \n",
      "                is_last_step = self.global_steps >= self.total_training_steps\n",
      "\n",
      "                if (\n",
      "                    self.val_reward_fn is not None\n",
      "                    and self.config.trainer.test_freq > 0\n",
      "                    and (is_last_step or self.global_steps % self.config.trainer.test_freq == 0)\n",
      "                ):\n",
      "                    print(f\"Start Testing at step {self.global_steps}...\")\n",
      "                    with marked_timer(\"testing\", timing_raw, color=\"green\"):\n",
      "                        # 调用父类 RayPPOTrainer 的 _validate 方法\n",
      "                        # 它会使用 Student (Actor) 在验证集上生成，并计算 Ground Truth Accuracy\n",
      "                        val_metrics: dict = self._validate() \n",
      "                    \n",
      "                    # 将测试指标添加到 metrics 中，以便 logger 记录\n",
      "                    metrics.update(val_metrics)\n",
      "                    print(f\"Testing finished. Metrics: {val_metrics}\")\n",
      "                # =================================================================\n",
      "\n",
      "                metrics.update({\n",
      "                    \"training/global_step\": self.global_steps,\n",
      "                    \"training/epoch\": epoch\n",
      "                })\n",
      "                \n",
      "                logger.log(data=metrics, step=self.global_steps)\n",
      "                progress_bar.update(1)\n",
      "                self.global_steps += 1\n",
      "                \n",
      "                if self.config.trainer.save_freq > 0 and self.global_steps % self.config.trainer.save_freq == 0:\n",
      "                    self._save_checkpoint()\n",
      "\n",
      "                if self.global_steps >= self.total_training_steps:\n",
      "                    progress_bar.close()\n",
      "                    return\n",
      "```\n",
      "\n",
      "### 为什么直接调用 `self._validate()` 就可以？\n",
      "\n",
      "因为你的 `TeacherStudentReflectiveTrainer` 继承自 `RayPPOTrainer`，而 `RayPPOTrainer` 中已经实现了 `_validate` 方法（在你提供的 `ray_trainer.py` 代码中可以看到）。\n",
      "\n",
      "`_validate` 方法的逻辑是：\n",
      "1.  从 `self.val_dataloader` 取数据。\n",
      "2.  让 `self.actor_rollout_wg` (也就是 Student) 生成回答。\n",
      "3.  调用 `self.val_reward_fn` (通常是 GSM8K 的准确率计算) 打分。\n",
      "4.  返回 `val-core/gsm8k/acc` 等指标。\n",
      "\n",
      "这正是你想要的：**在训练过程中，定期评估 Student 模型在真实测试集上的解题准确率**，看它是否因为 Teacher 的蒸馏而变强了。\n",
      "\n",
      "### 补充：确保 Validation Dataset 正确加载\n",
      "\n",
      "在你的启动脚本中：\n",
      "```bash\n",
      "data.val_files=$DATA_DIR/gsm8k/test.parquet\n",
      "```\n",
      "这看起来是正确的。只要 `main_self_reflective.py` 中正确创建了 `val_dataset` 和 `val_reward_fn`（代码里看是有的），加上上面的修改后，每 60 步你就会在日志（和 SwanLab）中看到类似 `val-core/gsm8k/acc` 的曲线了。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我在启动脚本里传了 trainer.test_freq=60, 但是日志里没有测试指标，\n",
    "能在metric里加入测试集的评测信息吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2babea7d-d265-473c-b802-f2bb025bd24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：在 LLM 的 On-Policy Distillation（或 PPO）训练初期，Actor Entropy 维持在 4.5 左右是完全正常的，甚至是一个健康的信号。**\n",
      "\n",
      "你感觉“偏高”，可能是因为习惯了较小的动作空间（如 Atari 游戏动作空间只有几百，Entropy 通常在 1.0-2.0）或者传统的 NLP 分类任务。\n",
      "\n",
      "以下是详细的分析，告诉你为什么 4.5 是合理的，以及如何通过它判断训练状态。\n",
      "\n",
      "### 1. 数学直觉：4.5 意味着什么？\n",
      "\n",
      "熵（Entropy）衡量的是**不确定性**。公式是 $H(x) = - \\sum p(x) \\ln p(x)$。\n",
      "\n",
      "*   **最大熵（完全随机）**：Qwen 的词表大小约为 152,000。\n",
      "    $$ H_{max} = \\ln(152,000) \\approx 11.9 $$\n",
      "    如果 Entropy 接近 11-12，说明模型坏了，在输出均匀噪声。\n",
      "*   **最小熵（One-Hot/完全确定）**：\n",
      "    $$ H_{min} = 0 $$\n",
      "    如果 Entropy 接近 0，说明模型发生了**模式坍塌（Mode Collapse）**，只会输出固定的几句话，失去了多样性。\n",
      "\n",
      "*   **当前值 4.5**：\n",
      "    $$ e^{4.5} \\approx 90 $$\n",
      "    这大致意味着模型在每一步生成时，平均在考虑约 **90 个合理的候选 Token**。\n",
      "    考虑到 Qwen 的词表很大，且自然语言（尤其是思维链 CoT）本身就有多种表达方式（比如 \"First,\" vs \"To start,\" vs \"Step 1:\"），保留一定的“困惑度”是正常的。\n",
      "\n",
      "### 2. 为什么比“传统 RL”高？\n",
      "\n",
      "在你的场景（On-Policy Distillation）中，Entropy 较高的原因主要有两点：\n",
      "\n",
      "#### A. 蒸馏损失 (Distillation Loss) 的特性\n",
      "你使用的 Loss 本质上是 **KL Divergence**（或者 Reverse KL）。\n",
      "*   **纯 PPO (Sparse Reward)**：模型只要找到**一条**能拿满分的路径，就会疯狂强化这条路径，导致概率分布迅速尖锐化，Entropy 骤降（可能降到 1.0 以下）。这往往导致过拟合或模式坍塌。\n",
      "*   **蒸馏 (Teacher-Student)**：Student 被要求**模仿 Teacher 的概率分布**。\n",
      "    *   Teacher 是一个 SFT 模型，它本身就保留了语言的多样性（High Entropy）。\n",
      "    *   如果 Teacher 对某个词只有 60% 的信心，Student 就**被迫**学会保持 60% 的信心，不能变成 100%。\n",
      "    *   因此，**Distillation 天然具有维持 Entropy、防止模式坍塌的作用**。\n",
      "\n",
      "#### B. 此时是 Step 1\n",
      "你看到的日志是 Step 1 或者是训练早期。\n",
      "*   基座模型（Instruct Model）本身的 Entropy 通常就在 3.0 - 5.0 之间。\n",
      "*   训练初期，Student 还没有学到足以改变其分布的强烈信号，所以它保持了基座模型的 Entropy 水平。\n",
      "\n",
      "### 3. 如何通过 Entropy 监控训练？\n",
      "\n",
      "虽然 4.5 是正常的，但你需要关注它的**变化趋势**：\n",
      "\n",
      "1.  **健康曲线**：\n",
      "    *   **缓慢下降**：随着训练进行，Student 对解题路径越来越确信，Entropy 会缓慢下降（例如从 4.5 -> 3.5 -> 3.0）。\n",
      "    *   **平稳波动**：在 Distillation 中，如果 Teacher 本身就很“纠结”，Student 的 Entropy 可能会一直维持在 4.0 左右，这也是可以接受的。\n",
      "\n",
      "2.  **危险信号**：\n",
      "    *   **暴涨 (Rising -> 8.0+)**：模型“脑以此了”（Catastrophic Forgetting）。它忘记了如何说话，开始随机输出乱码。通常是 LR 太大或 KL 惩罚太重。\n",
      "    *   **骤降 (Dropping -> < 1.0)**：模式坍塌。模型学会了一句“万能废话”或者只会复读题目，虽然 Reward 可能不低（如果 Reward 设计有漏洞），但模型废了。\n",
      "\n",
      "### 4. 总结\n",
      "\n",
      "*   **数值 4.5**：对于 150k 词表的 LLM 来说，非常健康。\n",
      "*   **波动**：小幅度波动是正常的 Sample 差异。\n",
      "*   **对比**：不要和 AlphaGo 或 CartPole 的 Entropy 比，那个动作空间太小。也不要和训练后期的 PPO 比。\n",
      "\n",
      "**建议**：继续训练，观察 Loss 和 Validation Accuracy。只要 Accuracy 在上升，Entropy 在 4.5 左右晃荡完全不是问题，甚至说明你的模型保留了很好的探索能力。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "actor/entropy一直在4.5附近波动，这正常吗？ 感觉做强化学习的时候没有那么大\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "861d03b3-8267-426b-ad2f-8fce37b26c3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：非常正常，而且这个曲线揭示了你的蒸馏过程正在按预期工作。**\n",
      "\n",
      "这不仅是正常的，而且从数学和原理上解释，这正是 **Teacher-Student Distillation（尤其是 Student 和 Teacher 源自同一基座模型时）** 应有的表现。\n",
      "\n",
      "以下是详细的分析：\n",
      "\n",
      "### 1. 为什么是负数？ (-0.05)\n",
      "你的 Reward 计算公式是：\n",
      "$$ R = \\log P_{\\text{Teacher}}(y|x, \\text{Hint}) - \\log P_{\\text{Student}}(y|x) $$\n",
      "\n",
      "*   **数据来源**：Token $y$ 是由 **Student** 生成的（On-Policy）。\n",
      "*   **自洽性偏差**：对于一个语言模型来说，它通常对自己生成的 Token 赋予最高的概率（或者接近最高）。\n",
      "*   **Teacher 的视角**：Teacher 虽然有 Hint，但它面对的是 Student 生成的 Token。除非 Student 的生成完美符合 Teacher 结合 Hint 后的预期，否则 Teacher 对该 Token 的概率通常会略低于 Student 自己的自信度。\n",
      "    *   即 $P_S \\ge P_T \\Rightarrow \\log P_T - \\log P_S \\le 0$。\n",
      "*   **因此**：Reward 略微为负（-0.05）是符合直觉的，表示 Student 对自己的生成比 Teacher 更“自信”一点点，或者两者分布非常接近。\n",
      "\n",
      "### 2. 为什么会上升？ (-0.05 -> -0.03)\n",
      "*   **优化目标**：你的训练目标是最大化 Reward，也就是最小化 $P_S$ 和 $P_T$ 之间的距离（Reverse KL）。\n",
      "*   **上升的含义**：从 -0.05 上升到 -0.03，意味着差距变小了。Student 正在调整自己的参数，使其概率分布更接近 Teacher（拥有上帝视角）的分布。\n",
      "*   **训练有效**：这证明梯度正在正确传导，Student 正在学习。\n",
      "\n",
      "### 3. 为什么在 -0.03 处波动/停滞？ (Information Gap)\n",
      "这是最有趣的一点。为什么它不变成 0 甚至正数？\n",
      "\n",
      "*   **信息不对称（Information Gap）**：\n",
      "    *   **Teacher**：看到了 Hint（正确答案/解题思路）。\n",
      "    *   **Student**：**没看到** Hint，只能靠推理。\n",
      "*   **理论极限**：Student 无论怎么学，都无法**完全**复刻 Teacher 的分布，因为它缺少关键输入（Hint）。\n",
      "    *   Teacher 看着答案说：“这一步肯定是 6。” ($P_T \\approx 1.0$)\n",
      "    *   Student 没看答案，推导说：“这一步大概率是 6，但也可能是别的。” ($P_S \\approx 0.9$)\n",
      "    *   这种由“未知信息”导致的不确定性差异，就是那剩下的 -0.03。\n",
      "*   **结论**：-0.03 可能就是当前模型能力下，Student 能逼近 Teacher 的**理论极限**（Aleatoric Uncertainty）。\n",
      "\n",
      "### 4. 数值大小分析\n",
      "*   **-0.03 意味着什么？**\n",
      "    $$ \\Delta = -0.03 \\implies \\frac{P_T}{P_S} = e^{-0.03} \\approx 0.97 $$\n",
      "    这说明 Teacher 和 Student 的概率分布重合度高达 **97%**。\n",
      "*   **评价**：这说明你的 Student 模型本身底子很好（毕竟是 Qwen-Instruct），而且经过微调后，已经非常接近“拥有答案的 Teacher”的表现了。\n",
      "\n",
      "### 5. 接下来该关注什么？\n",
      "\n",
      "既然 KL 已经趋于稳定，单纯看 `reward/reflection_kl` 已经看不出太多变化了。你需要关注以下指标来判断模型是否真的变强：\n",
      "\n",
      "1.  **Validation Accuracy (`val-core/gsm8k/acc`)**：这是金标准。即使 KL 只有微小的优化，是否转化为了解题率的提升？\n",
      "2.  **Response Length**：观察 Student 的回答是否变长了？（Teacher 可能会鼓励更详细的推理）。\n",
      "3.  **Bad Case 分析**：如果 Acc 没有提升，去看看那些 Student 做错、Teacher 却给了高分（或者 Student 做对、Teacher 给了低分）的 Case。\n",
      "\n",
      "**总结：你的训练处于非常健康的状态。-0.03 的波动代表 Student 已经尽力在模仿那位“开了天眼”的老师了。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "学习的过程中reward/reflection_kl最初step 1在-0.05，然后快速上升，step 12上升到-0.03，然后一直在0.03附近波动，这个正常吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cc91db7-3bdd-42fa-a1b6-01e37426c6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：完全没有影响，也不用担心。这只是变量命名的复用。**\n",
      "\n",
      "在你的代码逻辑中，`actor/pg_loss` **实际上就是你的 Distillation Loss**。\n",
      "\n",
      "### 1. 为什么会出现这个日志？\n",
      "\n",
      "请回看你修改过的 `dp_actor.py` 代码：\n",
      "\n",
      "```python\n",
      "                    # ... (你的修改) ...\n",
      "                    if loss_mode == \"distillation\":\n",
      "                        # 直接最大化 (advantages * log_prob)\n",
      "                        # 这里 pg_loss 被赋值为你定义的蒸馏损失\n",
      "                        pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
      "                        \n",
      "                        # 占位符\n",
      "                        pg_clipfrac = torch.tensor(0.0, device=pg_loss.device)\n",
      "                        # ...\n",
      "\n",
      "                    elif loss_mode == \"vanilla\":\n",
      "                        # ... (PPO 的逻辑) ...\n",
      "                        pg_loss, ... = compute_policy_loss(...)\n",
      "\n",
      "                    # ... (后续代码) ...\n",
      "                    \n",
      "                    # 这里的 metrics 字典 Key 是写死的字符串 \"actor/pg_loss\"\n",
      "                    micro_batch_metrics.update(\n",
      "                        {\n",
      "                            \"actor/pg_loss\": pg_loss.detach().item(), # <--- 这里记录了它\n",
      "                            \"actor/pg_clipfrac\": pg_clipfrac.detach().item(),\n",
      "                            # ...\n",
      "                        }\n",
      "                    )\n",
      "```\n",
      "\n",
      "**解释：**\n",
      "*   Verl 框架默认使用 `actor/pg_loss` (Policy Gradient Loss) 这个 Key 来记录策略优化的主损失。\n",
      "*   你在代码中拦截了逻辑，计算了 **Distillation Loss**，但为了保持代码兼容性，你把计算结果赋给了变量 `pg_loss`。\n",
      "*   因此，日志里的 `actor/pg_loss` **不再是 PPO 的 Clipped Loss，而是你的 On-Policy Distillation Loss**。\n",
      "\n",
      "### 2. 这个数值代表什么？\n",
      "\n",
      "在你的设置中：\n",
      "*   `advantages` $\\approx$ `token_level_rewards` $\\approx \\log P_T - \\log P_S$ (通常是负数，如 -0.03)。\n",
      "*   `pg_loss` $= - \\text{mean}(\\text{advantages} \\times \\log P_S)$。\n",
      "\n",
      "由于这是一个 **Loss**（损失），优化器会尝试**最小化**它。\n",
      "最小化 `pg_loss` 等同于 **最大化** `advantages * log_prob`，也就是让模型更有可能生成那些 Teacher 认为概率更高的 Token。这正是你想要的目标。\n",
      "\n",
      "### 3. 如何确认代码跑对了？\n",
      "\n",
      "检查日志中的 **`actor/pg_clipfrac`**。\n",
      "\n",
      "*   **如果是 PPO**：这个值通常在 0.0 到 0.2 之间波动（表示有多少比例的数据被 Clip 了）。\n",
      "*   **如果是你的 Distillation**：你在代码里写了：\n",
      "    ```python\n",
      "    pg_clipfrac = torch.tensor(0.0, device=pg_loss.device)\n",
      "    ```\n",
      "    所以，只要你在日志里看到 `actor/pg_clipfrac` **恒定为 0**，就说明代码正确进入了 `loss_mode == \"distillation\"` 的分支，那个 `pg_loss` 就是你想要的蒸馏损失。\n",
      "\n",
      "**总结：一切正常，忽略名字，关注数值即可。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "日志中还输出了actor/pg_loss，这个有影响吗？我应该没用这个？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27772b77-2a3f-4cc1-a7e8-6d42c24b4e52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：完全符合。你的写法是标准的“基于 Policy Gradient 的 Reverse KL 最小化”。**\n",
      "\n",
      "虽然你没有直接对 Logits 进行均方误差（MSE）回归（那是 Offline Distillation 的做法），但在 On-Policy（强化学习）的框架下，你的写法是在数学上等价于让 Student 的概率分布去逼近 Teacher。\n",
      "\n",
      "以下是详细的数学推导和直观解释，证明你的写法是对的。\n",
      "\n",
      "### 1. 数学证明\n",
      "\n",
      "你的目标是最小化 Student ($S$) 和 Teacher ($T$) 之间的 **Reverse KL Divergence**：\n",
      "$$ J(\\theta) = D_{KL}(P_S || P_T) = \\sum_y P_S(y|x) \\log \\frac{P_S(y|x)}{P_T(y|x)} $$\n",
      "$$ J(\\theta) = \\mathbb{E}_{y \\sim P_S} [ \\log P_S(y|x) - \\log P_T(y|x) ] $$\n",
      "\n",
      "我们需要计算关于 Student 参数 $\\theta$ 的梯度 $\\nabla_\\theta J(\\theta)$。利用 Log-Derivative Trick (REINFORCE 技巧)：\n",
      "\n",
      "$$ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{y \\sim P_S} \\left[ \\nabla_\\theta \\log P_S(y|x) \\cdot \\underbrace{(\\log P_S(y|x) - \\log P_T(y|x))}_{\\text{KL Difference term}} \\right] $$\n",
      "\n",
      "现在看你的代码：\n",
      "*   **Loss**: `pg_loss = - mean(advantages * log_prob)`\n",
      "*   **Gradient**: $\\nabla L = - \\mathbb{E} [ \\text{advantages} \\cdot \\nabla \\log P_S ]$\n",
      "*   **Advantages**: 在你的 `fit` 函数中，`advantages` 被赋值为 `token_level_rewards`。\n",
      "*   **Rewards**: 计算方式为 `kl_diff = log P_T - log P_S`。\n",
      "\n",
      "将 `advantages` 代入梯度公式：\n",
      "$$ \\nabla L = - \\mathbb{E} [ (\\log P_T - \\log P_S) \\cdot \\nabla \\log P_S ] $$\n",
      "$$ \\nabla L = \\mathbb{E} [ (\\log P_S - \\log P_T) \\cdot \\nabla \\log P_S ] $$\n",
      "\n",
      "**完美匹配！**\n",
      "你的代码计算出的梯度方向，正是 **Reverse KL Divergence 的梯度方向**。\n",
      "\n",
      "### 2. 直观解释\n",
      "\n",
      "这个 Loss 的物理含义非常直观：\n",
      "\n",
      "$$ \\text{Loss} = - (\\log P_{\\text{Teacher}} - \\log P_{\\text{Student}}) \\times \\log P_{\\text{Student}} $$\n",
      "\n",
      "*   **情形 A：Teacher 比 Student 更确信 ($P_T > P_S$)**\n",
      "    *   Teacher 觉得这个词好，Student 觉得一般。\n",
      "    *   `Advantage` ($P_T - P_S$) 是 **正数**。\n",
      "    *   Loss 变为负数（因为前面有负号），优化器想要最小化 Loss，就会 **增大** $\\log P_{\\text{Student}}$。\n",
      "    *   **效果**：Student 被推着去提高这个词的概率 -> **靠近 Teacher**。\n",
      "\n",
      "*   **情形 B：Teacher 觉得不行 ($P_T < P_S$)**\n",
      "    *   Student 盲目自信，Teacher 觉得这个词不对。\n",
      "    *   `Advantage` ($P_T - P_S$) 是 **负数**。\n",
      "    *   Loss 变为正数。为了最小化 Loss，梯度会 **减小** $\\log P_{\\text{Student}}$。\n",
      "    *   **效果**：Student 被打压，降低这个词的概率 -> **靠近 Teacher**。\n",
      "\n",
      "### 3. 与 \"MSE Logits Loss\" 的区别\n",
      "\n",
      "你可能会疑惑：“为什么不直接算 `MSE(Logits_S, Logits_T)`？”\n",
      "\n",
      "*   **Offline Distillation (MSE)**：通常用于预训练或 SFT 阶段。需要计算所有词表（150k 个词）的 Logits，计算量巨大，且通常假设数据是固定的（Off-policy）。\n",
      "*   **On-Policy Distillation (你的做法)**：\n",
      "    *   因为是 On-Policy，你只能拿到 Student 采样出来的这一条路径 ($y$)。\n",
      "    *   你无法知道 Teacher 对其他 149,999 个词的看法。\n",
      "    *   所以你利用 **Policy Gradient**，仅根据当前采样的这个词，通过 Reward (KL Diff) 来调整 Student 的分布。\n",
      "    *   **这是在 RL 框架下做蒸馏的标准做法**（也被称为 Generalized Knowledge Distillation）。\n",
      "\n",
      "### 4. 一个小提示\n",
      "\n",
      "在 `dp_actor.py` 中：\n",
      "```python\n",
      "pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
      "```\n",
      "这里的 `advantages` 必须是 **Detached** (不需要梯度的)。\n",
      "在你的 `TeacherStudentReflectiveTrainer` 中，`token_level_rewards` 是计算出来的 Tensor，如果没有显式 `detach()`，可能会导致计算图错误。\n",
      "\n",
      "不过，在 `verl` 的 `compute_advantage` 或数据传递过程中，通常会自动 detach。只要确认 `batch.batch['advantages']` 没有梯度即可（通常从 DataProto 取出来用来做 PPO Update 的时候都是当作常数的）。\n",
      "\n",
      "**总结：你的逻辑是完全正确的，这就是 On-Policy Distillation 的核心实现。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "pg_loss = -verl_F.masked_mean(advantages * log_prob, response_mask)\n",
    "这个改动符合我的onpolicy distillation的逻辑吗？ 我是想让student去学teacher的logit？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15324352-04bc-4004-b60a-8e7030e64346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常实用的需求。监控 Logits 的极值和差异分布，能帮你快速判断训练是否异常（比如梯度爆炸、Teacher 过于强势、或者 Student 彻底躺平）。\n",
      "\n",
      "请在 `verl/trainer/ppo/teacher_student_reflective_trainer.py` 的 `fit` 函数中，在计算完 `kl_diff` 之后，插入以下统计逻辑。\n",
      "\n",
      "### 修改代码\n",
      "\n",
      "找到 `fit` 函数中计算 `kl_diff` 的地方，插入标记为 `=== FIX: Add Debug Metrics ===` 的代码块。\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.6 Reward Calculation) ...\n",
      "                    token_level_rewards = torch.zeros_like(student_full_log_probs)\n",
      "                    s_probs = student_full_log_probs\n",
      "                    t_probs = teacher_full_log_probs_aligned # 确保这里用的是对齐后的！\n",
      "                    \n",
      "                    min_len = min(s_probs.shape[1], t_probs.shape[1])\n",
      "                    s_part = s_probs[:, :min_len]\n",
      "                    t_part = t_probs[:, :min_len]\n",
      "                    \n",
      "                    kl_diff = t_part - s_part\n",
      "                    \n",
      "                    # =================================================================\n",
      "                    # === FIX: Add Debug Metrics (Logits Statistics) ===\n",
      "                    # =================================================================\n",
      "                    with torch.no_grad():\n",
      "                        # 1. 获取有效 Mask (切片以匹配 min_len)\n",
      "                        # 必须过滤掉 Padding，否则 Min LogP 会显示 -14.82 (Padding 的 LogP)\n",
      "                        valid_mask = batch.batch['response_mask'][:, :min_len].bool()\n",
      "                        \n",
      "                        if valid_mask.any():\n",
      "                            # 2. 提取有效数据 (Flatten)\n",
      "                            s_valid = s_part[valid_mask]\n",
      "                            t_valid = t_part[valid_mask]\n",
      "                            diff_valid = kl_diff[valid_mask]\n",
      "                            \n",
      "                            # 3. Student Stats\n",
      "                            metrics['debug/student_logp_max'] = s_valid.max().item()\n",
      "                            metrics['debug/student_logp_min'] = s_valid.min().item()\n",
      "                            metrics['debug/student_logp_mean'] = s_valid.mean().item()\n",
      "                            \n",
      "                            # 4. Teacher Stats\n",
      "                            metrics['debug/teacher_logp_max'] = t_valid.max().item()\n",
      "                            metrics['debug/teacher_logp_min'] = t_valid.min().item()\n",
      "                            metrics['debug/teacher_logp_mean'] = t_valid.mean().item()\n",
      "                            \n",
      "                            # 5. Diff Stats (Reward Raw)\n",
      "                            metrics['debug/diff_logp_max'] = diff_valid.max().item()\n",
      "                            metrics['debug/diff_logp_min'] = diff_valid.min().item()\n",
      "                            metrics['debug/diff_logp_mean'] = diff_valid.mean().item()\n",
      "                            \n",
      "                            # 6. (可选) 监控 Teacher 是否过于自信\n",
      "                            # 如果 Teacher LogP 经常接近 0 (Max ~ 0)，说明 Teacher 非常确信\n",
      "                            # 如果 Diff Max 很大 (e.g. > 5.0)，说明 Teacher 觉得 Student 错得离谱\n",
      "                    # =================================================================\n",
      "\n",
      "                    # ... (后续代码: Clamp, assign token_level_rewards) ...\n",
      "                    kl_diff = torch.clamp(kl_diff, min=-1.0, max=1.0)\n",
      "                    token_level_rewards[:, :min_len] = kl_diff\n",
      "```\n",
      "\n",
      "### 如何解读这些指标？\n",
      "\n",
      "1.  **`debug/student_logp_mean` vs `debug/teacher_logp_mean`**\n",
      "    *   **正常**：两者应该比较接近，通常在 -0.5 到 -2.0 之间。\n",
      "    *   **Teacher 强**：如果 Teacher Mean (-0.2) 显著高于 Student Mean (-1.5)，说明 Teacher 更加自信（这很好，蒸馏有动力）。\n",
      "    *   **异常**：如果 Teacher Mean 很低 (-5.0)，说明 Teacher 对 Student 的输出感到困惑（Prompt 问题或 Student 胡说八道）。\n",
      "\n",
      "2.  **`debug/diff_logp_max` (最大奖励)**\n",
      "    *   代表 Teacher 觉得 Student 说得“太棒了”（或者 Teacher 比 Student 更确信这个词）。\n",
      "    *   如果 > 2.0，说明这是一个很强的正向信号。\n",
      "\n",
      "3.  **`debug/diff_logp_min` (最大惩罚)**\n",
      "    *   代表 Teacher 觉得 Student 说得“太烂了”。\n",
      "    *   如果 < -5.0，说明 Student 输出了 Teacher 认为概率极低的词。\n",
      "    *   **注意**：你后续有 `clamp(min=-1.0)`，所以 Reward 实际上被截断了，但这里的 Metric 能让你看到**原始的惩罚力度**。如果 Min 经常非常小（如 -10），说明你的 Student 经常“偏离正轨”。\n",
      "\n",
      "这些指标会直接显示在 SwanLab/WandB 面板中，非常利于监控训练动态。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我还想在metric中加入几个关键调试信息\n",
    "\"\"\"\n",
    "1. teacher中token最大/最小的logit\n",
    "2. student中token最大/最小的logit\n",
    "3. teacher和student logit 最大/最小的diff\n",
    "\"\"\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c341486-6f59-49e4-9427-cfde51902a0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 问题 1：为什么 `debug/student_logp_min` 会有 -14？\n",
      "\n",
      "你可能会觉得：“如果一个词的 LogProb 是 -14 ($P \\approx 8 \\times 10^{-7}$)，采样时怎么可能选中它？”\n",
      "\n",
      "**原因有三点：**\n",
      "\n",
      "1.  **Top-P (Nucleus) Sampling 的长尾效应**：\n",
      "    *   虽然 -14 很小，但如果 `top_p` 设置为 0.9 或 1.0（默认通常是 1.0），且分布非常平坦（Entropy 高），那么长尾中的低概率词也是有机会被选中的。\n",
      "    *   更常见的是：**采样时的 Temperature $\\neq$ 计算 LogProb 时的 Temperature**。\n",
      "        *   **生成时**：假设你用了 `temperature=0.8`。原本 -14 的 Logits 会被放大，概率分布会变，虽然还是很低，但如果上下文合适，有可能被选。\n",
      "        *   **计算时**：我们强制用了 `temperature=1.0`。在原始 Logits 下，这个词可能就是这么低分。\n",
      "\n",
      "2.  **Teacher 和 Student 的分布差异 (最可能的原因)**：\n",
      "    *   注意你的 Metric 是 `debug/student_logp_min`。\n",
      "    *   这是 Student 模型对 **自己生成的 Token** 的打分。\n",
      "    *   **Wait...** 如果是 Student 自己生成的，它通常应该比较自信（比如 -0.5）。为什么它会对自己生成的词打 -14 分？\n",
      "    *   这通常发生在 **\"被迫\"生成** 的情况下。\n",
      "        *   例如：Prompt 强制要求某种格式，或者之前的 Token 已经把路走窄了，导致下一个 Token 即使是“最优解”，其绝对概率也很低（因为模型很困惑）。\n",
      "        *   或者：**EOS Token**。在长文本生成被迫截断或者逻辑结束时，模型可能对何时输出 `<|im_end|>` 感到非常纠结，导致该 Token 概率极低。\n",
      "\n",
      "3.  **计算对齐的边缘效应**：\n",
      "    *   虽然我们对齐了 Padding，但如果 `response_mask` 在边界处理上差了一个 Token（比如把最后的 EOS 算进去了，或者把第一个 Prompt Token 算进去了），那个边缘 Token 的 LogProb 可能会很异常。\n",
      "    *   不过只要 Mean 正常，偶尔出现的 Min -14 不影响大局。\n",
      "\n",
      "### 问题 2：Sequence Length 增大后 Entropy 从 4.5 升到 6.5，正常吗？\n",
      "\n",
      "**非常正常，且符合预期。**\n",
      "\n",
      "1.  **长文本的不确定性累积**：\n",
      "    *   当 Sequence Length 变长（比如从 512 -> 1024），模型需要进行更复杂的推理。\n",
      "    *   在推理的中间步骤（CoT），可选的路径（措辞、步骤顺序）非常多。\n",
      "    *   相比于短文本（往往是简单的回答），长文本的“自由度”更高，因此平均 Entropy 会升高。\n",
      "\n",
      "2.  **Qwen 的特性**：\n",
      "    *   Qwen 等模型在处理长 CoT 时，往往会进入一种“发散”状态，直到收敛到答案。\n",
      "    *   Entropy 6.5 ($e^{6.5} \\approx 665$) 意味着每一步有几百个词可选。对于 150k 词表的模型，这依然属于“有意义的生成”，而不是随机噪声（11.9）。\n",
      "\n",
      "3.  **验证方法**：\n",
      "    *   只要 `train/batch_acc` 或 `val/acc` 没有崩盘，Entropy 升高说明模型正在**探索更长的思维链**。\n",
      "    *   如果 Acc 暴跌，说明模型在长文本中迷失了（幻觉），这时候才需要担心 Entropy 过高的问题。\n",
      "\n",
      "**总结：目前的指标变化都在合理范围内，说明模型正在适应更长的上下文。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在运行过程中metric发现debug/student_logp_min有-14，这么小的logit是如何采样出来的呢？\n",
    "然后我发现我把sequence length增大了之后 entropy从4.5提升到了6.5，这正常吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95b0740a-667a-414a-9d1d-2eae683cc1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 截断通常是因为 `generate_sequences` 时传入的 `max_new_tokens` 限制太紧。\n",
      "\n",
      "在你的代码 `teacher_student_reflective_trainer.py` 的 `_prepare_summary_generation_batch` 函数中，你硬编码了一个限制：\n",
      "\n",
      "```python\n",
      "        summary_batch.meta_info = {\n",
      "            \"do_sample\": True,\n",
      "            \"temperature\": 0.1, \n",
      "            \"max_new_tokens\": 512, # <--- 罪魁祸首\n",
      "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
      "            \"pad_token_id\": self.tokenizer.pad_token_id\n",
      "        }\n",
      "```\n",
      "\n",
      "### 修复方案\n",
      "\n",
      "1.  **直接增大数值**：将 512 改为 1024 或 2048。\n",
      "2.  **从 Config 读取**：最好从配置文件中读取，这样不用改代码就能调参。\n",
      "\n",
      "**建议修改代码如下：**\n",
      "\n",
      "```python\n",
      "    def _prepare_summary_generation_batch(self, batch: DataProto, raw_prompts: List[str], ground_truths: List[str]) -> DataProto:\n",
      "        # ... (前文不变) ...\n",
      "\n",
      "        # 尝试从 config 中读取，如果没有则默认 1024\n",
      "        # 假设你在 config.actor_rollout_ref.rollout 下加了一个 summary_max_new_tokens\n",
      "        # 或者直接复用 data.max_response_length\n",
      "        \n",
      "        max_tokens = self.config.data.get(\"max_response_length\", 1024)\n",
      "        \n",
      "        summary_batch.meta_info = {\n",
      "            \"do_sample\": True,\n",
      "            \"temperature\": 0.1, \n",
      "            \"max_new_tokens\": max_tokens, # <--- 修改这里\n",
      "            \"eos_token_id\": self.tokenizer.eos_token_id,\n",
      "            \"pad_token_id\": self.tokenizer.pad_token_id\n",
      "        }\n",
      "        \n",
      "        return summary_batch\n",
      "```\n",
      "\n",
      "这样，Summary 的长度限制就会跟随你启动脚本中的 `data.max_response_length=1024`，从而解决截断问题。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我发现运行过程中summary出现截断的现象，如何增大summary的长度限制\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2a13dc9-9c14-4960-ad89-203661611f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据你提供的启动脚本和代码逻辑，每次 Rollout（生成并进行蒸馏）的数据量是 **256 条**。\n",
      "\n",
      "具体的计算逻辑如下：\n",
      "\n",
      "### 1. 核心参数\n",
      "在你的启动脚本中：\n",
      "```bash\n",
      "data.train_batch_size=256\n",
      "```\n",
      "这是控制每次 PPO/蒸馏迭代（Iteration/Step）处理的总 Prompt 数量的核心参数。\n",
      "\n",
      "### 2. 代码逻辑解析\n",
      "在 `RayPPOTrainer`（你的 `TeacherStudentReflectiveTrainer` 的父类）的 `_create_dataloader` 方法中：\n",
      "\n",
      "```python\n",
      "self.train_dataloader = StatefulDataLoader(\n",
      "    dataset=self.train_dataset,\n",
      "    # 如果没设置 gen_batch_size，默认使用 train_batch_size\n",
      "    batch_size=self.config.data.get(\"gen_batch_size\", self.config.data.train_batch_size),\n",
      "    # ...\n",
      ")\n",
      "```\n",
      "\n",
      "在你的 `fit` 循环中：\n",
      "```python\n",
      "# 每次从 Dataloader 取出一个 batch\n",
      "for batch_dict in self.train_dataloader:\n",
      "    batch = DataProto.from_single_dict(batch_dict)\n",
      "    \n",
      "    # 获取 N (每个 Prompt 生成几个回答，默认为 1)\n",
      "    N = self.config.actor_rollout_ref.rollout.n \n",
      "    \n",
      "    # ...\n",
      "    \n",
      "    # Student 生成 (Rollout)\n",
      "    gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)\n",
      "```\n",
      "\n",
      "### 3. 最终计算\n",
      "*   **Prompts 数量**：256 (来自 `data.train_batch_size`)\n",
      "*   **每条 Prompt 生成数 (N)**：通常默认为 1 (除非你在 config 文件里改了 `actor_rollout_ref.rollout.n`)。\n",
      "*   **单次蒸馏总数据量** = $256 \\times 1 = \\mathbf{256}$ 条序列。\n",
      "\n",
      "### 4. 其它 Batch Size 的含义\n",
      "为了避免混淆，这里解释一下脚本里其他的 Batch Size：\n",
      "\n",
      "*   **`actor.ppo_mini_batch_size=64`**：\n",
      "    *   这是**更新参数（Update）**时的 Batch Size。\n",
      "    *   这意味着那 256 条 Rollout 出来的数据，会被切分成 $256 / 64 = 4$ 个 Mini-Batches。\n",
      "    *   模型会在这 4 个 Mini-Batch 上轮流进行反向传播更新。\n",
      "\n",
      "*   **`ppo_micro_batch_size_per_gpu=4`**：\n",
      "    *   这是为了**显存优化**（Gradient Accumulation）。\n",
      "    *   在处理那 64 条 Mini-Batch 时，单张 GPU 一次只能吃得消 4 条。\n",
      "    *   所以它会累积 $64 / 4 / \\text{World\\_Size}$ 次梯度，最后做一次更新。\n",
      "\n",
      "### 总结\n",
      "**你的 Teacher 和 Student 每一步（Step）会“过招” 256 个问题。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在的代码实现是每次rollout几条数据进行蒸馏呢？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83e42225-bedc-43cc-a252-e5de47bc77b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常有用的功能，能让你直接在滚动日志中看到 Student 是否做对了，而不仅仅是看 LogProb。\n",
      "\n",
      "我们需要做两件事：\n",
      "1.  **实现一个简单的答案提取函数**（针对 GSM8K 的 `####` 格式）。\n",
      "2.  **在 Debug 打印模块中调用它**，对比 Student 回答和 Ground Truth。\n",
      "3.  **(可选但推荐)** 计算当前 Batch 的训练集准确率（Training Accuracy）并记录到 Metric 中。\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py` 的 `fit` 函数。\n",
      "\n",
      "### 1. 添加提取函数 (放在 `TeacherStudentReflectiveTrainer` 类内部或外部)\n",
      "\n",
      "```python\n",
      "def extract_answer(text: str) -> str:\n",
      "    \"\"\"从文本中提取 #### 后的答案，用于简单的正确性匹配\"\"\"\n",
      "    if \"####\" in text:\n",
      "        return text.split(\"####\")[-1].strip()\n",
      "    return \"[No Answer]\"\n",
      "```\n",
      "\n",
      "### 2. 修改 `fit` 函数中的 Debug 部分\n",
      "\n",
      "找到 `=== Log Part 1: Generation Content ===` 区域，插入以下代码：\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.5 Student Computes LogProb 之后) ...\n",
      "\n",
      "                    # === Log Part 1: Generation Content ===\n",
      "                    if True:\n",
      "                        print(f\"\\n{'='*20} Teacher-Student Reflection Debug (Step {self.global_steps}) {'='*20}\")\n",
      "                        try:\n",
      "                            idx = 0 \n",
      "                            \n",
      "                            # ... (获取 Summary, Teacher Input 等代码保持不变) ...\n",
      "                            \n",
      "                            # === FIX: 获取 Student Response 和 Ground Truth 进行对比 ===\n",
      "                            # 1. 获取 GT\n",
      "                            gt_raw = current_ground_truths[idx]\n",
      "                            gt_ans = extract_answer(gt_raw)\n",
      "                            # 如果 GT 本身就是纯数字(有些数据集是这样)，直接用\n",
      "                            if gt_ans == \"[No Answer]\" and len(gt_raw) < 20: \n",
      "                                gt_ans = gt_raw.strip()\n",
      "\n",
      "                            # 2. 获取 Student Response\n",
      "                            s_ids = batch.batch['responses'][idx]\n",
      "                            s_ids = s_ids[s_ids != self.tokenizer.pad_token_id]\n",
      "                            s_resp_text = self.tokenizer.decode(s_ids, skip_special_tokens=True)\n",
      "                            s_ans = extract_answer(s_resp_text)\n",
      "                            \n",
      "                            # 3. 判定\n",
      "                            # 简单的字符串匹配 (对于数学题通常足够)\n",
      "                            # 移除逗号以防 \"1,000\" vs \"1000\"\n",
      "                            is_correct = (s_ans.replace(',', '') == gt_ans.replace(',', ''))\n",
      "                            \n",
      "                            status_icon = \"✅\" if is_correct else \"❌\"\n",
      "                            \n",
      "                            print(f\"--- [Answer Check] ---\")\n",
      "                            print(f\"Ground Truth Raw: {gt_raw[-50:].strip()}...\") # 只打印最后一点\n",
      "                            print(f\"Student Answer:   {s_ans}\")\n",
      "                            print(f\"Target Answer:    {gt_ans}\")\n",
      "                            print(f\"Result:           {status_icon} (Match: {is_correct})\")\n",
      "                            print(f\"----------------------\\n\")\n",
      "                            # ==========================================================\n",
      "\n",
      "                            # ... (后续的 Teacher Greedy Probe 代码保持不变) ...\n",
      "```\n",
      "\n",
      "### 3. (强烈推荐) 在 Metric 中加入 Training Accuracy\n",
      "\n",
      "除了看单条日志，最好统计一下这 256 条数据的**实时准确率**。这比 Validation Accuracy 更灵敏，能告诉你 Student 是否正在“背诵”或者“学会”当前的 Batch。\n",
      "\n",
      "在 `fit` 函数中，计算 Reward 之前或之后，添加以下循环：\n",
      "\n",
      "```python\n",
      "                    # ... (在计算 kl_diff 附近) ...\n",
      "\n",
      "                    # =================================================================\n",
      "                    # === FIX: 计算当前 Training Batch 的准确率 ===\n",
      "                    # =================================================================\n",
      "                    n_correct = 0\n",
      "                    total = len(current_ground_truths)\n",
      "                    \n",
      "                    # 遍历整个 Batch\n",
      "                    for i in range(total):\n",
      "                        gt_str = extract_answer(current_ground_truths[i])\n",
      "                        # 处理 GT 格式\n",
      "                        if gt_str == \"[No Answer]\" and len(current_ground_truths[i]) < 20:\n",
      "                            gt_str = current_ground_truths[i].strip()\n",
      "                            \n",
      "                        # 解码 Student\n",
      "                        # 注意：为了速度，这里不应该再次 decode 整个 batch，\n",
      "                        # 但因为我们没有保留 decode 后的 list，这里只能简易处理。\n",
      "                        # 或者利用前面 summary 生成时的 raw_prompts/responses 逻辑。\n",
      "                        # 为了不拖慢速度，我们可以只采样前 10 个算个大概，或者全算。\n",
      "                        # 既然是 Training Loop，全算稍微慢点也行。\n",
      "                        \n",
      "                        # 优化：只对 batch['responses'][i] 解码\n",
      "                        r_ids = batch.batch['responses'][i]\n",
      "                        r_ids = r_ids[r_ids != self.tokenizer.pad_token_id]\n",
      "                        r_text = self.tokenizer.decode(r_ids, skip_special_tokens=True)\n",
      "                        \n",
      "                        pred_str = extract_answer(r_text)\n",
      "                        \n",
      "                        if pred_str != \"[No Answer]\" and (pred_str.replace(',', '') == gt_str.replace(',', '')):\n",
      "                            n_correct += 1\n",
      "                            \n",
      "                    train_acc = n_correct / total\n",
      "                    metrics['train/batch_acc'] = train_acc\n",
      "                    # =================================================================\n",
      "```\n",
      "\n",
      "这样你就可以在 SwanLab/WandB 上看到一条 `train/batch_acc` 的曲线。\n",
      "*   如果曲线在上升，说明 On-Policy Distillation 有效。\n",
      "*   如果 `train/batch_acc` 很高但 `val/acc` 很低，说明过拟合。\n",
      "*   如果 `train/batch_acc` 很低且不涨，说明 Teacher 没教好。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我想在日志中每个step输出的调试信息中，加入答案是否正确的标记\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d54b776a-0deb-4614-b964-d6c23ecee677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的观察非常敏锐，而且你的直觉是完全正确的。\n",
      "\n",
      "**目前的 4:4 分配是非常不合理的，导致了巨大的资源浪费。**\n",
      "\n",
      "### 1. 为什么 4:4 不合理？\n",
      "\n",
      "让我们对比一下 **Student** 和 **Teacher** 在一个训练 Step 中的工作量：\n",
      "\n",
      "*   **Student (Actor) 的工作：**\n",
      "    1.  **Generate (Rollout)**: 生成 256 条回答（这是 Token-by-Token 的，非常慢）。\n",
      "    2.  **Generate (Summary)**: 生成 256 条摘要（同样是 Token-by-Token）。\n",
      "    3.  **Forward (Student LogProb)**: 计算一次概率。\n",
      "    4.  **Backward (PPO Update)**: **最重的一步**。需要计算梯度并更新参数，计算量通常是 Forward 的 3 倍。\n",
      "*   **Teacher (RefPolicy) 的工作：**\n",
      "    1.  **Forward (Teacher LogProb)**: 仅仅计算一次概率（Parallel Forward）。不生成，不反向传播。\n",
      "\n",
      "**结论**：Student 的计算负载是 Teacher 的 **10 倍以上**。\n",
      "目前的状况是：Teacher 的 4 张卡瞬间算完，然后“围观”Student 的 4 张卡在那儿哼哧哼哧地生成和训练。\n",
      "\n",
      "### 2. 推荐的分配方案\n",
      "\n",
      "既然你的配置中 `tensor_model_parallel_size=1`（即使用 Data Parallelism，单卡跑模型），我们可以随意调整 GPU 数量，不受模型 Head 数能否整除的限制。\n",
      "\n",
      "**强烈建议改为：Student 7卡 : Teacher 1卡**\n",
      "或者保守一点：**Student 6卡 : Teacher 2卡**\n",
      "\n",
      "对于 4B 这种小模型，1 张卡做 Teacher (Inference Only) 处理 256 的 Batch Size 绰绰有余（显存不够可以调小 Teacher 的 `micro_batch_size`，反正它算得快，多切几份也比 Student 快）。\n",
      "\n",
      "### 3. 代码修改方案\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py` 中的 `init_workers` 函数，支持自定义分配比例。\n",
      "\n",
      "```python\n",
      "    def init_workers(self):\n",
      "        \"\"\"\n",
      "        Manually split resources and initialize two independent vLLM-enabled workers.\n",
      "        \"\"\"\n",
      "        # 1. Calculate Resource Split\n",
      "        n_gpus_per_node = self.config.trainer.n_gpus_per_node\n",
      "        n_nodes = self.config.trainer.nnodes\n",
      "        \n",
      "        # === FIX: 调整分配比例 ===\n",
      "        # 之前的逻辑: student_gpus = n_gpus_per_node // 2 (4:4)\n",
      "        # 新的逻辑: 尽可能多给 Student，Teacher 保留 1-2 张即可\n",
      "        \n",
      "        if n_gpus_per_node >= 8:\n",
      "            teacher_gpus = 1  # 8卡机器：7 Student : 1 Teacher\n",
      "        elif n_gpus_per_node >= 4:\n",
      "            teacher_gpus = 1  # 4卡机器：3 Student : 1 Teacher\n",
      "        else:\n",
      "            teacher_gpus = 1  # 至少给1张\n",
      "            \n",
      "        student_gpus = n_gpus_per_node - teacher_gpus\n",
      "        \n",
      "        if student_gpus < 1:\n",
      "            raise ValueError(f\"Not enough GPUs! Need at least 2 GPUs per node.\")\n",
      "            \n",
      "        print(f\">>> Resource Splitting: Student={student_gpus} GPUs, Teacher={teacher_gpus} GPUs (per node)\")\n",
      "        # =========================\n",
      "\n",
      "        # 2. Create Independent Resource Pools\n",
      "        student_pool = RayResourcePool(\n",
      "            process_on_nodes=[student_gpus] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"student_pool\"\n",
      "        )\n",
      "        \n",
      "        teacher_pool = RayResourcePool(\n",
      "            process_on_nodes=[teacher_gpus] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"teacher_pool\"\n",
      "        )\n",
      "        \n",
      "        self.resource_pool_to_cls = {student_pool: {}, teacher_pool: {}}\n",
      "\n",
      "        # 3. Initialize Student (Actor) Config\n",
      "        student_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        \n",
      "        # === FIX: 显式更新 DP/TP 设置 ===\n",
      "        # 你的脚本里 tensor_model_parallel_size=1，所以我们是在增加 DP worker 数量\n",
      "        # RayWorkerGroup 会根据资源池大小自动启动对应数量的 Worker\n",
      "        # 这里不需要改 config，因为 config 传进去的是单卡配置\n",
      "        \n",
      "        student_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.ActorRollout],\n",
      "            config=student_config,\n",
      "            role=\"actor_rollout\",\n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[student_pool][\"actor_rollout\"] = student_cls\n",
      "\n",
      "        # 4. Initialize Teacher (Ref) Config\n",
      "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        \n",
      "        # 针对 Teacher (1张卡) 优化显存\n",
      "        # 如果 1 张卡吃不下 256 batch，调小它的 micro_batch\n",
      "        # 注意：这里的 log_prob_micro_batch_size_per_gpu 是配置里的，\n",
      "        # 如果你原来设的是 4，现在 Teacher 只有 1 张卡，它一次处理 4 条，完全没问题。\n",
      "        # 如果 OOM，可以在这里强制改小：\n",
      "        # teacher_config.ref.log_prob_micro_batch_size_per_gpu = 2 \n",
      "        \n",
      "        teacher_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.RefPolicy],\n",
      "            config=teacher_config,\n",
      "            role=\"actor_rollout\", \n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[teacher_pool][\"ref\"] = teacher_cls\n",
      "\n",
      "        # ... (后续代码保持不变) ...\n",
      "```\n",
      "\n",
      "### 4. 预期效果\n",
      "\n",
      "改为 **7:1** 后：\n",
      "1.  **训练速度大幅提升**：Student 现在有 7 张卡并行做 Rollout 和 Backward，理论速度接近原来的 $7/4 = 1.75$ 倍（受限于最慢环节）。\n",
      "2.  **GPU 利用率均衡**：\n",
      "    *   Student 的 7 张卡会跑得很满。\n",
      "    *   Teacher 的 1 张卡虽然还是会比 Student 快（因为它只做 Forward），但它现在要处理 256 条数据（原来是 4 张卡分 256，现在是 1 张卡分 256，负载增加了 4 倍），所以它的空闲时间会大幅减少。\n",
      "3.  **显存注意**：Teacher 那张卡现在要独自承担所有 Teacher 模型的 KV Cache 和 Forward 计算。对于 4B 模型问题不大，如果是 70B 模型可能需要 2 张卡做 Teacher。\n",
      "\n",
      "**总结：请立刻改为 7:1 或 6:2，你的训练效率会起飞。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在是4卡做student，4卡做teacher，均衡分配的，但我看到有4张卡经常空置，感觉是不是其中某一项的GPU任务会更重一些？这样均衡分配不合理？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8758e325-7db5-4c5d-a302-407cb02be09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del chat_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d28f01d6-c5da-4026-aace-6a7fbb2782db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9656054162575665)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.exp(-0.035)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94837090-7a19-4fa0-afed-0fdafc9a5921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是最关键的一步。之前的修改其实已经让 `DP_Actor` 正确计算了概率（因为 Left Padding 配合了切片逻辑），但**返回的结果仍然是 Left Padded 的**。\n",
      "\n",
      "而你的 Student 数据（以及后续的 PPO 逻辑、日志打印）全部假设数据是 **Right Padded**（有效数据在左，Padding 在右）。\n",
      "\n",
      "这就导致了：\n",
      "1.  **日志误导**：你打印第 0 个位置，Student 是 \"Let\"，Teacher 是 \"Pad\"。你看到的是 \"Pad\" 的概率（-14.82），却以为是 \"Let\" 的概率。\n",
      "2.  **Reward 错误**：直接相减 `T - S`，等于用 `LogP(Pad)` 减去 `LogP(Let)`，完全算错。\n",
      "\n",
      "### 解决方案：在 `fit` 中对齐 Tensor\n",
      "\n",
      "我们需要在拿到 `teacher_full_log_probs` 后，立刻将其**从 Left Padding 转为 Right Padding**，使其与 Student 完美对齐。\n",
      "\n",
      "请修改 `teacher_student_reflective_trainer.py` 的 `fit` 函数。\n",
      "\n",
      "#### 1. 添加一个辅助函数 (放在类外或类内均可)\n",
      "\n",
      "```python\n",
      "def left_to_right_padding(tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
      "    \"\"\"\n",
      "    将 Left Padded 的 Tensor 转换为 Right Padded。\n",
      "    tensor: [B, L]\n",
      "    mask: [B, L] (1 for valid, 0 for pad)\n",
      "    \"\"\"\n",
      "    B, L = tensor.shape\n",
      "    device = tensor.device\n",
      "    \n",
      "    # 计算每个样本的有效长度\n",
      "    lens = mask.sum(dim=-1).long() # [B]\n",
      "    \n",
      "    # 创建新的 Right Padded Tensor\n",
      "    new_tensor = torch.zeros_like(tensor)\n",
      "    \n",
      "    for i in range(B):\n",
      "        l = lens[i]\n",
      "        if l > 0:\n",
      "            # 取出有效部分 (Left Padded 的有效部分在末尾)\n",
      "            valid_part = tensor[i, -l:]\n",
      "            # 放到开头 (Right Padded)\n",
      "            new_tensor[i, :l] = valid_part\n",
      "            \n",
      "    return new_tensor\n",
      "```\n",
      "\n",
      "#### 2. 修改 `fit` 函数\n",
      "\n",
      "在 Step 3.4 和 3.5 之间，对 Teacher 的 LogProb 进行转换。\n",
      "\n",
      "```python\n",
      "                    # ... (Step 3.4 Teacher Computes LogProb) ...\n",
      "                    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries)\n",
      "                    teacher_log_prob_output = self.ref_policy_wg.compute_log_prob(teacher_batch)\n",
      "                    teacher_full_log_probs = teacher_log_prob_output.batch['old_log_probs']\n",
      "                    \n",
      "                    # === FIX: 将 Teacher 的 LogProbs 从 Left Pad 转为 Right Pad ===\n",
      "                    # 这里的 teacher_batch['response_mask'] 是 Left Padded 的，正好用来提取有效 LogProbs\n",
      "                    t_resp_mask = teacher_batch.batch['response_mask']\n",
      "                    \n",
      "                    # 转换!\n",
      "                    teacher_full_log_probs_aligned = left_to_right_padding(teacher_full_log_probs, t_resp_mask)\n",
      "                    \n",
      "                    # 现在的 teacher_full_log_probs_aligned 是 [Let, 's, ..., Pad, Pad]\n",
      "                    # 与 Student 的结构一致了\n",
      "                    # ==========================================================\n",
      "\n",
      "                    # ... (Step 3.5 Student Computes LogProb) ...\n",
      "                    batch.meta_info['temperature'] = 1.0 \n",
      "                    student_log_prob_output = self.actor_rollout_wg.compute_log_prob(batch)\n",
      "                    student_full_log_probs = student_log_prob_output.batch['old_log_probs']\n",
      "                    \n",
      "                    # ... (Reward Calculation) ...\n",
      "                    # 注意：下面计算 Reward 时，要用 teacher_full_log_probs_aligned\n",
      "                    \n",
      "                    token_level_rewards = torch.zeros_like(student_full_log_probs)\n",
      "                    s_probs = student_full_log_probs\n",
      "                    t_probs = teacher_full_log_probs_aligned # <--- 使用对齐后的 Tensor\n",
      "                    \n",
      "                    min_len = min(s_probs.shape[1], t_probs.shape[1])\n",
      "                    s_part = s_probs[:, :min_len]\n",
      "                    t_part = t_probs[:, :min_len]\n",
      "                    \n",
      "                    kl_diff = t_part - s_part\n",
      "```\n",
      "\n",
      "#### 3. 同时也修正 Log 打印部分\n",
      "\n",
      "在 Debug Log 的 `Token-wise KL Breakdown` 部分，确保使用的是 `t_part` (已经是对齐过的)。\n",
      "\n",
      "```python\n",
      "                            # ... inside Log loop ...\n",
      "                            s_vals = s_part[idx].tolist()\n",
      "                            t_vals = t_part[idx].tolist() # 这里引用的 t_part 已经是 Right Padded 的了\n",
      "                            diff_vals = kl_diff[idx].tolist()\n",
      "                            \n",
      "                            # 现在的 t_vals[0] 对应的是 \"Let\" 的 LogProb，而不是 Pad 的\n",
      "```\n",
      "\n",
      "### 总结\n",
      "1.  **Left Padding (Input & Labels)**：保证了 `DP_Actor` 能算对每个 Token 的 LogProb。\n",
      "2.  **Left-to-Right Shift (Post-processing)**：保证了计算出的 LogProb 能与 Student 对齐，从而正确计算 KL Divergence。\n",
      "\n",
      "这次修改后，`T_LogP` 在日志中应该会显示为正常数值（比如 -0.5），而不是 -14.82。这才是真正的 On-Policy Distillation。\n"
     ]
    }
   ],
   "source": [
    "print(chat_history[-3].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e1b69-fb0c-4db1-9e0a-433d010d2213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2a3cf888-9e4d-4876-96e5-4fff7498e333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个配置参数的数学逻辑冲突。\n",
      "\n",
      "### 错误原因分析\n",
      "\n",
      "```text\n",
      "ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len\n",
      "```\n",
      "\n",
      "1.  **最大上下文长度 (`max_model_len`)**：\n",
      "    Verl 会根据你的数据配置计算模型需要的最大序列长度：\n",
      "    $$ \\text{Max Model Len} = \\text{Max Prompt} + \\text{Max Response} $$\n",
      "    $$ 4096 + 38912 = \\mathbf{43008} $$\n",
      "\n",
      "2.  **当前批处理上限 (`max_num_batched_tokens`)**：\n",
      "    你设置了：`actor_rollout_ref.rollout.max_num_batched_tokens=40000`\n",
      "\n",
      "3.  **冲突**：\n",
      "    $$ 40000 < 43008 $$\n",
      "    vLLM（在 Verl 的封装下）要求 `max_num_batched_tokens` 必须至少能容纳一个完整的最大长度序列，否则无法保证能处理最坏情况下的长文本。\n",
      "\n",
      "### 解决方案\n",
      "\n",
      "请在启动脚本中，将 `max_num_batched_tokens` 增加到大于 **43008** 的数值。\n",
      "\n",
      "**建议设置为 45000 或 48000。**\n",
      "\n",
      "#### 修改启动脚本\n",
      "\n",
      "找到 `actor_rollout_ref.rollout.max_num_batched_tokens` 这一行，修改如下：\n",
      "\n",
      "```bash\n",
      "# 原来是 40000，改为 45000\n",
      "actor_rollout_ref.rollout.max_num_batched_tokens=45000\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 额外建议：关于 Summary 的长度\n",
      "\n",
      "我注意到你还设置了：\n",
      "`+data.summary_max_new_tokens=38912`\n",
      "\n",
      "**这可能会导致 Teacher 节点 OOM。**\n",
      "*   Teacher 负责生成 Summary（对 Student 的回答进行点评）。\n",
      "*   Teacher 只有 1 张显卡（在 7:1 配置下）。\n",
      "*   Summary 通常只需要几百到几千个 Token（比如 \"Step 1 is correct, but Step 2...\"）。\n",
      "*   **不需要** 让 Teacher 生成 3万字的 Summary。\n",
      "*   如果 Teacher 生成时预分配了 38k 的显存，单卡很容易爆。\n",
      "\n",
      "**建议**：将 Summary 的生成长度限制在合理范围（例如 2048 或 4096），这不影响 Student 的长思维链（Student 的 38k 依然有效）。\n",
      "\n",
      "```bash\n",
      "# 建议改小 Teacher 的生成长度，节省显存\n",
      "+data.summary_max_new_tokens=4096\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "修改之后报错\n",
    "\"\"\"\n",
    "        please increase max_num_batched_tokens or disable chunked prefill\n",
    "Error executing job with overrides: ['data.train_files=/data/kcl/lpy/data/OpenThoughts/train_verl.parquet', 'data.val_files=[/data/kcl/lpy/data/AIME_2024/test_verl.parquet,/data/kcl/lpy/data/AIME_2025/test_verl.parquet,/data/kcl/lpy/data/HMMT_FEB_2025/test_verl.parquet]', 'data.train_batch_size=56', 'data.max_prompt_length=4096', 'data.max_response_length=38912', '+data.summary_max_new_tokens=38912', 'actor_rollout_ref.model.path=/data/kcl/lpy/models/qwen3_4b_instruct', '+actor_rollout_ref.model.torch_dtype=bfloat16', 'actor_rollout_ref.actor.optim.lr=5e-7', 'actor_rollout_ref.actor.ppo_mini_batch_size=28', 'actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2', 'actor_rollout_ref.actor.policy_loss=distillation', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.rollout.max_num_batched_tokens=40000', 'actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=2', 'critic.optim.lr=1e-5', 'critic.model.path=/data/kcl/lpy/models/qwen3_4b_instruct', '+critic.model.torch_dtype=bfloat16', 'critic.ppo_micro_batch_size_per_gpu=4', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.logger=console', 'trainer.val_before_train=False', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=30', 'trainer.test_freq=30', \"trainer.logger=['swanlab', 'console']\", 'trainer.project_name=self_reflect', 'trainer.experiment_name=sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_alleval', 'trainer.default_local_dir=/data/kcl/lpy/verl/verl_checkpoints/sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_alleval', 'trainer.total_epochs=15']\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 41, in main\n",
    "    run_ppo(config)\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 75, in run_ppo\n",
    "    ray.get(runner.run.remote(config))\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
    "    return fn(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 2967, in get\n",
    "    values, debugger_breakpoint = worker.get_objects(\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 1015, in get_objects\n",
    "    raise value.as_instanceof_cause()\n",
    "ray.exceptions.RayTaskError(ValueError): ray::TaskRunner.run() (pid=3248513, ip=192.168.131.187, actor_id=1fdf7901d5079229fd44cfae01000000, repr=<main_self_reflective.TaskRunner object at 0x7fefa300fd70>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 243, in run\n",
    "    trainer.init_workers()\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/ppo/teacher_student_reflective_trainer.py\", line 198, in init_workers\n",
    "    self.actor_rollout_wg.init_model()\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 50, in __call__\n",
    "    output = ray.get(output)\n",
    "             ^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "ray.exceptions.RayTaskError(ValueError): ray::WorkerDict.actor_rollout_init_model() (pid=3250675, ip=192.168.131.187, actor_id=8ab9ac0bb6d5e3d71852dae201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ef0a4bea720>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "    return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "    self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "                                                  ^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "    rollout = vllm_rollout_cls(\n",
    "              ^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "    raise ValueError(\n",
    "ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "\n",
    "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
    "(TaskRunner pid=3248513) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3249599, ip=192.168.131.187, actor_id=773eb9ddd45817c5b9df55bc01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ee9074ae780>)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3248513)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3248513)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3248513)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3248513)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3248513)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3248513)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3248513)     raise ValueError(\n",
    "(TaskRunner pid=3248513) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(TaskRunner pid=3248513) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3250680, ip=192.168.131.187, actor_id=87c0b9404342b2232e78ee2101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ee83a9edb80>)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3248513)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3248513)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3248513)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3248513)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3248513)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3248513)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3248513)     raise ValueError(\n",
    "(TaskRunner pid=3248513) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(TaskRunner pid=3248513) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3250679, ip=192.168.131.187, actor_id=a20cf69b4a4a413f4e54919b01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fc61bfbad20>)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3248513)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3248513)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3248513)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3248513)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3248513)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3248513)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3248513)     raise ValueError(\n",
    "(TaskRunner pid=3248513) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(TaskRunner pid=3248513) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3250677, ip=192.168.131.187, actor_id=36a7c2af2fce7e97e3606c7301000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f54f784ee70>)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3248513)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3248513)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3248513)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3248513)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3248513)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3248513)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3248513)     raise ValueError(\n",
    "(TaskRunner pid=3248513) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(TaskRunner pid=3248513) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3250676, ip=192.168.131.187, actor_id=340592e3d978c4a7d945e6d101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ef007e6e9f0>)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3248513)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3248513)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3248513)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3248513)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3248513)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3248513)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3248513)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3248513)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3248513)     raise ValueError(\n",
    "(TaskRunner pid=3248513) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "\"\"\"\n",
    "怎么解决\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cbce12be-76b0-4d47-9ee6-f17e805262ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将 `max_num_batched_tokens` 设置得很大（如 45000）会导致运行变慢，主要原因在于 **vLLM 的内存管理机制** 和 **GPU 计算效率** 的变化。\n",
      "\n",
      "### 1. KV Cache 碎片化与调度开销\n",
      "*   **大内存池**：当你设置 45000 时，vLLM 会尝试在显存中预留能容纳 45000 个 Token 的 KV Cache 空间（Block Table）。\n",
      "*   **碎片化**：在生成过程中，不同的 Sequence 长度不一（有的 100 Token，有的 30k Token）。vLLM 需要频繁地分配和回收这些 Block。\n",
      "*   **调度压力**：由于总 Token 数上限很高，vLLM 可能会尝试在一个 Batch 中塞入更多的小请求，或者不得不为了一个超长请求而驱逐其他请求的 Cache。这种复杂的调度逻辑会消耗 CPU 时间。\n",
      "\n",
      "### 2. Attention 计算的二次方复杂度 (虽有 FlashAttn)\n",
      "*   虽然 FlashAttention 优化了显存，但计算量依然与序列长度相关。\n",
      "*   如果你的 Batch 中混入了一个 30k 长度的请求，整个 Batch 的计算时间会被这个长请求拖慢（木桶效应）。\n",
      "*   在 `max_num_batched_tokens` 很大时，vLLM 倾向于“能塞就塞”，导致单次 Forward 的计算延迟显著增加。\n",
      "\n",
      "### 3. Chunked Prefill 的副作用\n",
      "*   你开启了 Chunked Prefill（因为上下文太长）。\n",
      "*   当 Token 数上限很高时，vLLM 可能会选择更大的 Chunk Size 进行 Prefill。\n",
      "*   这虽然避免了 OOM，但会导致 GPU 计算流（Stream）被长时间占用，阻塞了 Decode 阶段的小 Token 生成，导致整体吞吐量（Tokens/sec）下降，虽然利用率看起来很高。\n",
      "\n",
      "### 4. 显存带宽瓶颈 (Memory Bound)\n",
      "*   4B 模型本身计算量不大，但在处理 30k+ 上下文时，瓶颈从计算转移到了显存带宽。\n",
      "*   加载庞大的 KV Cache 需要大量带宽。\n",
      "*   如果 `max_num_batched_tokens` 设置过大，导致 Cache Miss 或者频繁的显存-HBM 交换，速度会断崖式下跌。\n",
      "\n",
      "### 优化建议\n",
      "\n",
      "1.  **接受现实**：处理 32k+ 的长文本本身就是很慢的。这是物理规律。\n",
      "2.  **适当减小**：如果你的数据大部分其实没那么长，只有极个别是 38k，可以尝试把 `max_num_batched_tokens` 设得稍微紧凑一点（比如 40000 够用就行，不要设 60000）。\n",
      "3.  **减少并发**：减少 Student 的并发数（Rollout N），让 GPU 专心处理当前的几个长序列，减少上下文切换。\n",
      "4.  **检查数据**：确认是否真的需要 38k 的 Response？如果是 OpenThoughts，有些思维链确实很长，但如果大部分在 8k 以内，可以考虑截断极长样本，或者分桶训练。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "为啥把max_num_batched_tokens设置成45000之后运行的非常慢\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d3857ad0-fe37-49dc-b552-1b15c4dede2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常好的想法！将 Ground Truth (Standard Answer) 显式地包含在 Teacher 的 Forward 上下文中，能进一步增强 Teacher 的“上帝视角”能力，使其 LogProb 对正确推理路径更敏感。\n",
      "\n",
      "我们需要修改 `teacher_student_reflective_trainer.py` 中的 `_prepare_teacher_forward_batch` 函数。\n",
      "\n",
      "### 修改逻辑\n",
      "\n",
      "1.  **原逻辑**：\n",
      "    *   System Prompt: Hint (Summary) + Instruction\n",
      "    *   User: Question\n",
      "    *   Assistant: Student Response (Target)\n",
      "\n",
      "2.  **新逻辑**：\n",
      "    *   System Prompt: **Hint (Summary) + Ground Truth** + Instruction\n",
      "    *   User: Question\n",
      "    *   Assistant: Student Response (Target)\n",
      "\n",
      "### 代码修改\n",
      "\n",
      "请替换 `_prepare_teacher_forward_batch` 函数中的 System Prompt 构造部分。\n",
      "\n",
      "```python\n",
      "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor) -> DataProto:\n",
      "        # ... (前文不变: 获取 prompts, responses, cleaning ...) ...\n",
      "\n",
      "        # 注意：你需要在这里获取 ground_truths\n",
      "        # fit 函数调用时并没有传 ground_truths 进来，我们需要修改 fit 函数传参，\n",
      "        # 或者在这里重新从 batch.non_tensor_batch 里提取。\n",
      "        \n",
      "        # 方案 A: 在函数内部提取 GT (更简单，不用改 fit 签名)\n",
      "        current_ground_truths = []\n",
      "        if \"reward_model\" in batch.non_tensor_batch:\n",
      "            rm_data = batch.non_tensor_batch[\"reward_model\"]\n",
      "            current_ground_truths = [item.get('ground_truth', '') if isinstance(item, dict) else '' for item in rm_data]\n",
      "        elif \"solution\" in batch.non_tensor_batch:\n",
      "            current_ground_truths = batch.non_tensor_batch[\"solution\"].tolist()\n",
      "        elif \"ground_truth\" in batch.non_tensor_batch:\n",
      "            current_ground_truths = batch.non_tensor_batch[\"ground_truth\"].tolist()\n",
      "        else:\n",
      "            current_ground_truths = [\"\"] * len(prompts)\n",
      "\n",
      "        for i in range(len(prompts)):\n",
      "            # ... (解码 Prompt, Cleaning ...) ...\n",
      "            \n",
      "            # ... (获取 Summary, Cleaning ...) ...\n",
      "            \n",
      "            # === FIX: 获取当前样本的 Ground Truth ===\n",
      "            gt_text = current_ground_truths[i]\n",
      "            \n",
      "            # 4. System Prompt 注入 Hint + GT\n",
      "            system_content = (\n",
      "                \"You are a helpful math assistant.\\n\"\n",
      "                \"I will provide a reference analysis (Hint) and the standard solution below. \"\n",
      "                \"Please use this information to help you solve the user's problem step-by-step.\\n\\n\"\n",
      "                \"Reference Analysis/Hint:\\n\"\n",
      "                f\"{s_text}\\n\\n\"\n",
      "                \"Standard Solution (Ground Truth):\\n\" # <--- 新增部分\n",
      "                f\"{gt_text}\\n\\n\"                      # <--- 新增部分\n",
      "                \"Instruction: Solve the problem step-by-step. Do not just state the answer.\"\n",
      "            )\n",
      "\n",
      "            # ... (后续代码保持不变: chat template, left padding ...) ...\n",
      "```\n",
      "\n",
      "### 检查点\n",
      "\n",
      "由于 `_prepare_teacher_forward_batch` 原本的签名是：\n",
      "`def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor) -> DataProto:`\n",
      "\n",
      "它并没有接收 `ground_truths` 列表。\n",
      "虽然我在上面的代码片段里加了内部提取逻辑，但这依赖于 `batch` 对象里是否还保留着 `non_tensor_batch` 的数据。\n",
      "在 `fit` 函数中：\n",
      "```python\n",
      "# fit 函数中\n",
      "gen_batch = batch.pop(...) # 把 tensor 移走了\n",
      "# ...\n",
      "batch = batch.union(gen_batch_output) # 合并了 output\n",
      "# batch.non_tensor_batch 应该还在\n",
      "```\n",
      "是的，`non_tensor_batch` 通常会保留。所以上面的内部提取逻辑是可行的。\n",
      "\n",
      "**如果想更稳健一点，建议修改 `fit` 函数的调用处：**\n",
      "\n",
      "1.  修改 `fit` 调用：\n",
      "    ```python\n",
      "    # fit 函数中\n",
      "    teacher_batch = self._prepare_teacher_forward_batch(batch, summaries, current_ground_truths) # <--- 传入 GT\n",
      "    ```\n",
      "\n",
      "2.  修改函数签名：\n",
      "    ```python\n",
      "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor, ground_truths: List[str]) -> DataProto:\n",
      "        # ...\n",
      "        # 直接使用传入的 ground_truths[i]\n",
      "    ```\n",
      "\n",
      "**推荐采用第 2 种（改签名）的方式，因为 `fit` 函数里已经提取过一次 `current_ground_truths` 了，直接传进去效率更高。**\n",
      "\n",
      "### 完整代码 (修改签名版)\n",
      "\n",
      "```python\n",
      "    # 1. 修改函数定义\n",
      "    def _prepare_teacher_forward_batch(self, batch: DataProto, summaries: torch.Tensor, ground_truths: List[str]) -> DataProto:\n",
      "        # ...\n",
      "        \n",
      "        for i in range(len(prompts)):\n",
      "            # ... (解码 Prompt) ...\n",
      "            # ... (解码 Summary) ...\n",
      "            \n",
      "            # 获取 GT\n",
      "            gt_text = ground_truths[i]\n",
      "            \n",
      "            # 构造 System Prompt\n",
      "            system_content = (\n",
      "                \"You are a helpful math assistant.\\n\"\n",
      "                \"I will provide a reference analysis (Hint) and the standard solution below. \"\n",
      "                \"Please use this information to help you solve the user's problem step-by-step.\\n\\n\"\n",
      "                \"Reference Analysis/Hint:\\n\"\n",
      "                f\"{s_text}\\n\\n\"\n",
      "                \"Standard Solution (Ground Truth):\\n\" \n",
      "                f\"{gt_text}\\n\\n\"\n",
      "                \"Instruction: Solve the problem step-by-step. Do not just state the answer.\"\n",
      "            )\n",
      "            \n",
      "            # ... (后续不变) ...\n",
      "\n",
      "    # 2. 修改 fit 中的调用\n",
      "    def fit(self):\n",
      "        # ...\n",
      "        # 在 Step 3.4\n",
      "        teacher_batch = self._prepare_teacher_forward_batch(batch, summaries, current_ground_truths)\n",
      "        # ...\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我想把teacher最后forward的input再增加拼接上groundtruth到summary之后，你需要修改代码\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e368fb-4b45-4c14-be42-33f22a6dea27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4615ac30-4746-4eba-9f06-eb83e513c0b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个关于 **vLLM Chunked Prefill** 的配置错误。\n",
      "\n",
      "### 错误分析\n",
      "```text\n",
      "ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len, please increase max_num_batched_tokens or disable chunked prefill\n",
      "```\n",
      "\n",
      "1.  **背景**：你将 `max_prompt_length` 和 `max_response_length` 增加到了 **38912**（非常巨大的上下文）。\n",
      "2.  **vLLM 机制**：\n",
      "    *   `max_model_len`：模型支持的最大上下文长度（现在是 38912）。\n",
      "    *   `max_num_batched_tokens`：vLLM 每次迭代处理的最大 Token 数。\n",
      "    *   **Chunked Prefill**：一种优化技术，将长 Prompt 切分成小块处理，防止单次计算量过大。\n",
      "3.  **冲突**：\n",
      "    *   vLLM 默认开启了 Chunked Prefill。\n",
      "    *   但是默认的 `max_num_batched_tokens`（通常是 8192 或更小）远小于你的 `max_model_len` (38912)。\n",
      "    *   vLLM 报错提示：如果你想跑这么长的模型，你要么增大 `max_num_batched_tokens`，要么关掉 Chunked Prefill（但这在超长文本下会导致 OOM 或极慢）。\n",
      "\n",
      "### 解决方案\n",
      "\n",
      "你需要显式增大 vLLM 的 `max_num_batched_tokens` 参数，使其至少等于或大于你的 `max_model_len` (38912)。\n",
      "\n",
      "在启动脚本中添加以下参数：\n",
      "\n",
      "```bash\n",
      "# 增加 vLLM 的批处理 Token 上限\n",
      "actor_rollout_ref.rollout.max_num_batched_tokens=40000 \\\n",
      "```\n",
      "\n",
      "或者，如果你想更安全一点（防止显存爆炸），可以尝试禁用 Chunked Prefill（虽然不推荐）：\n",
      "```bash\n",
      "actor_rollout_ref.rollout.enable_chunked_prefill=False\n",
      "```\n",
      "\n",
      "**推荐方案：**\n",
      "修改启动脚本，追加 `max_num_batched_tokens`：\n",
      "\n",
      "```bash\n",
      "# ... 其他参数 ...\n",
      "data.max_prompt_length=38912 \\\n",
      "data.max_response_length=38912 \\\n",
      "# 确保这个值 > 38912\n",
      "actor_rollout_ref.rollout.max_num_batched_tokens=40000 \\\n",
      "# ...\n",
      "```\n",
      "\n",
      "**注意**：\n",
      "38k 的上下文长度对于 4B 模型来说，即使在 A100 上，显存压力也非常巨大。\n",
      "*   如果再次遇到 OOM，你可能需要开启 **Tensor Parallelism (TP)**。\n",
      "*   即：`actor_rollout_ref.rollout.tensor_model_parallel_size=2` 或 `4`。\n",
      "*   这意味着你需要减少 Student 的并发数量（例如 8 卡只能跑 4 个 Student 实例）。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我想新增了2个验证集，并且增加了summary_max_new_tokens和max_response_length到38912，我改了之后发现目前代码报错\n",
    "\"\"\"\n",
    "Traceback (most recent call last):\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 41, in main\n",
    "    run_ppo(config)\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 75, in run_ppo\n",
    "    ray.get(runner.run.remote(config))\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
    "    return fn(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 2967, in get\n",
    "    values, debugger_breakpoint = worker.get_objects(\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/ray/_private/worker.py\", line 1015, in get_objects\n",
    "    raise value.as_instanceof_cause()\n",
    "ray.exceptions.RayTaskError(ValueError): ray::TaskRunner.run() (pid=3234117, ip=192.168.131.187, actor_id=083cf27693e5fd9d1767207801000000, repr=<main_self_reflective.TaskRunner object at 0x7ee6a65474d0>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/main_self_reflective.py\", line 243, in run\n",
    "    trainer.init_workers()\n",
    "  File \"/data/kcl/lpy/verl/verl/trainer/ppo/teacher_student_reflective_trainer.py\", line 198, in init_workers\n",
    "    self.actor_rollout_wg.init_model()\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 50, in __call__\n",
    "    output = ray.get(output)\n",
    "             ^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "                                  ^^^^^^^^^^^^^^^^^^^\n",
    "ray.exceptions.RayTaskError(ValueError): ray::WorkerDict.actor_rollout_init_model() (pid=3236312, ip=192.168.131.187, actor_id=fc7121a0cbd9b3bea4762c1201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7ef96eab6a80>)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "    return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "    return func(*args, **kwargs)\n",
    "           ^^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "    self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "                                                  ^^^^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "    rollout = vllm_rollout_cls(\n",
    "              ^^^^^^^^^^^^^^^^^\n",
    "  File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "    raise ValueError(\n",
    "ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "\n",
    "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
    "(TaskRunner pid=3234117) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3236313, ip=192.168.131.187, actor_id=57fb632e65ff58a596c5585301000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7edc200f9fd0>)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3234117)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3234117)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3234117)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3234117)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3234117)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3234117)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3234117)     raise ValueError(\n",
    "(TaskRunner pid=3234117) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(TaskRunner pid=3234117) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3236311, ip=192.168.131.187, actor_id=cb5fb665ebdfbdea5724723301000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f655318e8d0>)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3234117)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3234117)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3234117)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3234117)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3234117)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3234117)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3234117)     raise ValueError(\n",
    "(TaskRunner pid=3234117) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(TaskRunner pid=3234117) Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model() (pid=3235091, ip=192.168.131.187, actor_id=1234f131690693b9473070a201000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f4d8d16e8a0>)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/single_controller/ray/base.py\", line 705, in func\n",
    "(TaskRunner pid=3234117)     return getattr(self.worker_dict[key], name)(*args, **kwargs)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/single_controller/base/decorator.py\", line 514, in inner\n",
    "(TaskRunner pid=3234117)     return func(*args, **kwargs)\n",
    "(TaskRunner pid=3234117)            ^^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 626, in init_model\n",
    "(TaskRunner pid=3234117)     self.rollout, self.rollout_sharding_manager = self._build_rollout(\n",
    "(TaskRunner pid=3234117)                                                   ^^^^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/fsdp_workers.py\", line 495, in _build_rollout\n",
    "(TaskRunner pid=3234117)     rollout = vllm_rollout_cls(\n",
    "(TaskRunner pid=3234117)               ^^^^^^^^^^^^^^^^^\n",
    "(TaskRunner pid=3234117)   File \"/data/kcl/lpy/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py\", line 141, in __init__\n",
    "(TaskRunner pid=3234117)     raise ValueError(\n",
    "(TaskRunner pid=3234117) ValueError: Enable chunked prefill, max_num_batched_tokens is smaller than max_model_len,                              please increase max_num_batched_tokens or disable chunked prefill\n",
    "(raylet) [2026-02-01 05:57:09,387 E 3227950 3227974] (raylet) file_system_monitor.cc:116: /data/kcl/lpy/tmp/ray/session_2026-02-01_05-56-05_659797_3227052 is over 95% full, available space: 568.388 GB; capacity: 40960 GB. Object creation will fail if spilling is required.\n",
    "(WorkerDict pid=3236313) /data/kcl/lpy/anaconda3/envs/verl_new/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you. [repeated 6x across cluster]\n",
    "\"\"\"\n",
    "怎么解决?\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabdb4a6-1242-4bd2-a868-2b1d500699df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04b07a-1f82-45f2-abee-113c8aa7a453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "01df5f13-d86c-4abb-a594-904331025ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是因为 Verl 的评测系统是**数据驱动（Data-Driven）**的。指标名称和计算逻辑会自动根据你加载的数据集类型进行调整。\n",
      "\n",
      "### 1. 为什么指标名称变了？\n",
      "\n",
      "**原因**：你的验证数据集 Parquet 文件中的 `data_source` 列发生了变化。\n",
      "*   之前 GSM8K 数据集里，`data_source` 列的值是 `\"openai/gsm8k\"`。\n",
      "*   现在 AIME 数据集里，`data_source` 列的值应该是 `\"math_dapo\"`（或者类似的值）。\n",
      "\n",
      "**代码逻辑**：\n",
      "在 `ray_trainer.py` 的 `_validate` 函数最后：\n",
      "```python\n",
      "# 提取数据源名称\n",
      "data_source_lst.append(test_batch.non_tensor_batch.get(\"data_source\", ...))\n",
      "# ...\n",
      "# 拼接 Metric 名称\n",
      "pfx = f\"{metric_sec}/{data_source}/{var_name}/{metric_name}\"\n",
      "```\n",
      "所以 `val-core/math_dapo/...` 中的 `math_dapo` 就是直接从你的数据文件里读出来的。\n",
      "\n",
      "### 2. 指标含义解析\n",
      "\n",
      "*   **`val-core/math_dapo/acc/mean@32`**\n",
      "    *   **含义**：**Pass@32 准确率**（或者更准确地说是 N=32 时的平均准确率）。\n",
      "    *   **来源**：这意味着在验证时，针对每道 AIME 题目，模型生成了 **32 个回答**（`n=32`）。\n",
      "    *   **计算**：如果这 32 个回答中有任何一个做对了，或者计算这 32 个回答的平均正确率（取决于具体实现，通常 `acc` 指的是 Accuracy，`mean` 指的是平均分）。但在这里，结合 `@32`，通常是指 Pass@K 的某种变体或者是 Majority Vote 的基础数据。\n",
      "    *   **注意**：AIME 这种难题，通常需要多次采样（Pass@K）才能评估模型能力，因为单次采样的方差太大。\n",
      "\n",
      "*   **`val-aux/math_dapo/reward/worst@2/mean`**\n",
      "    *   **含义**：最差的 2 个回答的平均 Reward。\n",
      "    *   **用途**：用于监控模型的下限。\n",
      "\n",
      "### 3. 代码是怎么自动切换采样数 (N) 的？\n",
      "\n",
      "你可能会问：“我没改 N 啊，为什么变成 @32 了？”\n",
      "\n",
      "这通常是由 **Dataset Config** 或者 **Data Load 逻辑** 决定的。\n",
      "在 `main_self_reflective.py` 中：\n",
      "```python\n",
      "val_dataset = create_rl_dataset(config.data.val_files, ...)\n",
      "```\n",
      "如果你的 AIME Parquet 文件中，每条数据实际上包含了 **32 个重复的 Prompt**（或者你在 Config 里设置了 `val_kwargs.n=32`），那么验证循环就会针对每道题跑 32 次。\n",
      "\n",
      "**检查你的 Config**：\n",
      "看一下 `actor_rollout_ref.rollout.val_kwargs.n` 的值。如果它是 32，那就是这里控制的。\n",
      "或者，你的 AIME 数据集本身就是为了 Pass@K 评测准备的，数据加载器自动识别了这一点。\n",
      "\n",
      "### 总结\n",
      "*   **名称变化**：源于数据文件中的 `data_source` 字段。\n",
      "*   **@32**：源于验证时的采样次数（`n=32`），适合 AIME 这种高难度数学题。\n",
      "*   **计算逻辑**：依然是 `Reward Function`（正则匹配答案）的结果聚合。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "修改验证数据为AIME-2024之后，validate的metric自动变了，变成了\n",
    "val-core/math_dapo/acc/mean@32\n",
    "val-aux/math_dapo/reward/worst@2/mean\n",
    "val-core/math_dapo/acc/mean@32 \n",
    "这些指标是怎么算的？ 具体是什么含义？ 代码是怎么自动切换的？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45823bde-2a6f-4b85-a6ac-082418649b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：你的指标偏低大概率是正常的，因为你评测的是 `AIME 2024`，而不是旧版 `AIME`。**\n",
      "\n",
      "你看到的“70多分”的论文（例如 Qwen-2.5-Math, DeepSeek-Math 等），通常汇报的是 **AIME 2024 Pass@1 (Greedy)** 或者 **Pass@1 (Maj@k)** 的分数，但请注意以下几个巨大的陷阱：\n",
      "\n",
      "### 1. 数据集难度差异 (AIME 2024 vs AIME ALL)\n",
      "*   **AIME 2024**：这是最新的、难度极高的竞赛题。对于 7B/4B 这种小模型来说，Pass@1 通常只有 **个位数** 或者 **10%~20%**。\n",
      "    *   例如：DeepSeek-Math-7B-RL 在 AIME 2024 上的 Pass@1 只有 **15% - 20%** 左右。\n",
      "*   **AIME (Historical/Average)**：很多论文汇报的是 1983-2023 年所有 AIME 题目的平均分。早期的 AIME 题目相对简单。\n",
      "*   **你的分数**：\n",
      "    *   `acc/mean@32` (Pass@1 平均值) = **0.3 (30%)**。\n",
      "    *   `acc/maj@32` (Majority Vote @ 32) = **0.25 (25%)**。\n",
      "    *   **分析**：对于一个 4B 的模型（Qwen-3-4B），在 AIME 2024 上能跑到 25%-30%，这其实已经**非常非常高**了，甚至有点高得离谱（除非是训练集里有类似的题）。\n",
      "\n",
      "### 2. 评价指标的混淆 (Pass@K vs Mean@K vs Maj@K)\n",
      "\n",
      "*   **论文里的 \"Average@16\"**：这通常指的是 **Pass@16**（即采样 16 次，只要有一次对就算对）。\n",
      "    *   Pass@K 的公式是：$1 - (1 - acc)^K$。如果单次 Acc 是 10%，Pass@16 能达到 $1 - 0.9^{16} \\approx 81\\%$。\n",
      "    *   所以 Pass@K 的数值通常会非常高。\n",
      "*   **你的指标**：\n",
      "    *   `mean@32`：这是 **32 次采样的平均准确率**（即 Pass@1 的无偏估计）。它等于单次生成的准确率。\n",
      "    *   `best@32`：这是 **Pass@32**（32 次里只要有一个对就是 1.0，然后求平均）。**0.4 (40%)**。\n",
      "    *   `maj@32`：这是 **投票准确率**（32 次里出现最多的答案如果是对的，得 1.0）。**0.25**。\n",
      "\n",
      "**对比结论**：\n",
      "*   你的 `best@32` (Pass@32) 是 40%。\n",
      "*   如果你看到论文里是 70%，那确实有差距。但你需要确认论文用的是 **4B 模型** 还是 **70B 模型**。\n",
      "*   Qwen-2.5-Math-7B 在 AIME 2024 上的 Pass@1 大概是 50% 左右。4B 模型能有 30% (mean) 已经很不错了。\n",
      "\n",
      "### 3. 潜在的问题：数据泄露或过拟合？\n",
      "\n",
      "你说 `mean@32` 是 0.3，而 `maj@32` 只有 0.25。\n",
      "通常 `Maj@K` 应该比 `Mean` 高（群体的智慧）。\n",
      "如果 `Maj < Mean`，说明：\n",
      "*   模型生成了很多**错误的答案**，而且这些错误的答案**高度一致**（即模型非常确信一个错误的解法）。\n",
      "*   正确的答案反而比较分散。\n",
      "*   这通常意味着模型**并没有真正学会推理**，而是记住了一些错误的模式。\n",
      "\n",
      "### 4. 建议\n",
      "\n",
      "1.  **确认基准线**：去查阅 Qwen-2.5-Math-7B 或者类似的 **4B/7B 模型** 在 **AIME 2024** 单项上的分数，不要看 AIME Average。\n",
      "2.  **切换回 GSM8K 验证**：GSM8K 是更稳定的基准。如果你的模型在 GSM8K 上能达到 70-80%，说明训练没问题。AIME 2024 对 4B 模型来说太难了，方差极大，不适合作为唯一的验证集。\n",
      "3.  **关注 Pass@1 (Mean)**：在 RL 训练中，我们主要关注 Pass@1 (即你的 `mean@32`) 的提升。只要它在涨，训练就是有效的。\n",
      "\n",
      "**简而言之：你的分数对于 4B 模型在 AIME 2024 上是合理的，甚至偏高。不要和 70B 模型或者旧版 AIME 的分数直接比。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "问题在于我现在AIME-2024的这几个metric的分数如下：\n",
    "val-core/math_dapo/acc/mean@32  0.3左右\n",
    "val-core/math_dapo/acc/best@32/mean  0.4左右\n",
    "val-core/math_dapo/acc/maj@32/mean   0.25左右\n",
    "可是别的论文里汇报的指标average@16 是70多分，这和我这评价标准相同吗？ 还是哪里出了问题？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f570cc-4817-49c3-af9b-9dbe84f12b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat('''\n",
    "我看到官方qwen3 technical report里写qwen3_4b_instruct(也就是我用的model)，AIME 2024\n",
    " For each question, we sample 64 times and take the average accuracy as the final\n",
    "score.\n",
    " 38,912 TEXT LENGTH\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ea77d02-b619-48d8-bd79-270894658c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**是的，在目前的配置下，每张 GPU 上都存储了一个完整的、独立的模型副本。**\n",
      "\n",
      "这是由你的配置 `tensor_model_parallel_size=1` 决定的。\n",
      "\n",
      "以下是详细的资源分配分析：\n",
      "\n",
      "### 1. 为什么是“独立副本”？\n",
      "\n",
      "*   **Student (7 张卡)**：\n",
      "    *   你使用的是 **Data Parallelism (数据并行)** 模式。\n",
      "    *   **Rollout 阶段 (vLLM)**：因为 `tensor_model_parallel_size=1`，每张卡上的 vLLM 引擎都会加载一份完整的 Qwen-4B 权重（约 8GB 显存）。这 7 张卡之间**不共享**权重，也不进行张量通讯，它们各自处理不同的数据（Batch）。\n",
      "    *   **Training 阶段 (PyTorch FSDP)**：虽然 FSDP 会在训练时对参数进行分片（Sharding）以节省显存，但在初始化阶段和前向传播计算时，每张卡在逻辑上都维护着模型的结构。\n",
      "\n",
      "*   **Teacher (1 张卡)**：\n",
      "    *   它独立运行，显然必须加载一份完整的模型权重。\n",
      "\n",
      "### 2. 这种方式（4B模型）合理吗？\n",
      "\n",
      "**对于 4B 这样的小模型，这是最高效的配置。**\n",
      "\n",
      "*   **优势（吞吐量最大化）**：\n",
      "    *   如果把 4B 模型切分到 8 张卡上（Tensor Parallelism），计算量太小，GPU 大部分时间都在等待网络通讯（All-Reduce），速度反而会变慢。\n",
      "    *   让每张卡独立跑一个完整模型，7 张卡就能同时并行处理 $7 \\times \\text{Batch}$ 的数据，吞吐量是单卡的 7 倍。\n",
      "\n",
      "*   **代价（显存占用）**：\n",
      "    *   代价是显存的“重复占用”。\n",
      "    *   8 张卡总共存了 8 份模型权重（$8 \\times 8\\text{GB} = 64\\text{GB}$）。\n",
      "    *   但在 A100/H800 (80GB) 甚至 24GB/40GB 的卡上，4B 模型只占很小一部分，显存的大头其实是 **KV Cache (8k Context)** 和 **Logits/Gradients**。\n",
      "\n",
      "### 3. 如果是 70B 模型该怎么办？\n",
      "\n",
      "如果是 72B 模型（权重约 144GB），单张 GPU 根本放不下。那时候你就不能让“每张卡存一个模型”了。\n",
      "\n",
      "你需要修改配置为 **Tensor Parallelism (模型并行)**：\n",
      "*   `tensor_model_parallel_size=4`（4 张卡拼成一个模型）。\n",
      "*   那么 8 张卡只能跑 2 个 Student 实例（每个实例跨 4 张卡）。\n",
      "\n",
      "### 4. 针对你当前 OOM 的建议\n",
      "\n",
      "既然每张卡都有独立模型，**显存瓶颈**主要来自：\n",
      "1.  **模型权重**：8GB (bf16)。\n",
      "2.  **vLLM KV Cache**：这是大头！8192 长度的 Context，如果不限制，vLLM 会试图占满剩余显存。\n",
      "3.  **PyTorch Activation/Logits**：计算 LogProb 时的中间变量。\n",
      "\n",
      "**这也是为什么之前让你：**\n",
      "1.  开启 `bfloat16`（权重从 16GB 降到 8GB）。\n",
      "2.  调整 `gpu_memory_utilization`（限制 vLLM 不要把显存吃光，给 PyTorch 留活路）。\n",
      "3.  减小 `micro_batch_size`（减少 PyTorch 的瞬间峰值显存）。\n",
      "\n",
      "**结论：目前的架构是合理的，不需要改动架构，只需要微调显存参数即可。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "现在init_workers中每个GPU上是会独立存储一个模型吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17b2d5de-410d-40e6-b602-505bfd6bb283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常重要的发现。你引用的这两篇 Paper 中汇报的“分数”定义，与 Verl 默认输出的指标定义确实存在巨大的差异。\n",
      "\n",
      "要对齐他们的指标，我们需要仔细拆解他们的术语。\n",
      "\n",
      "### 1. Qwen3 Technical Report 的指标拆解\n",
      "\n",
      "> \"For each question, we sample 64 times and take the average accuracy as the final score.\"\n",
      "\n",
      "这句话其实有点歧义，但结合 Qwen3 在 AIME 2024 上的惊人表现（73.8分），只有一种解释：\n",
      "\n",
      "*   **他们指的不是 Pass@1 (Mean Accuracy)**：如果是 Mean Accuracy，意味着模型每次做对的概率是 73.8%。对于 4B 模型在 AIME 2024 上，这几乎是不可能的（除非模型本身就在这个数据集上训练过，或者使用了极强的推理模式）。\n",
      "*   **他们指的大概率是 Pass@64 (Best of 64)**：即采样 64 次，只要有一次对了就算对。\n",
      "    *   **证据**：如果是 Average Accuracy，通常不需要采样 64 次这么多次。采样 64 次通常是为了做 **Pass@K** 或者 **Majority Vote**。\n",
      "    *   **另一种可能**：他们使用的是 **Majority Vote (Consensus)**。即 64 次里出现次数最多的答案作为最终答案。这通常是刷榜分数的标准做法。\n",
      "\n",
      "**然而**，如果是 73.8 分，这对于 4B 模型来说依然高得离谱。\n",
      "**唯一的解释是**：Qwen3-Math（专门的数学模型）确实很强，或者他们使用了 **CoT + System 2 (长思维链)** 模式。\n",
      "\n",
      "### 2. Arxiv Paper 的指标拆解\n",
      "\n",
      "> \"We report average@16 using suggested sampling parameters... temperature of 1.2\"\n",
      "\n",
      "这里的 `average@16` 术语非常不标准。在 RL 和 LLM 评测中：\n",
      "*   **Pass@K**：$1 - (1-p)^K$。\n",
      "*   **Maj@K**：投票。\n",
      "*   **Mean (Average)**：单次准确率。\n",
      "\n",
      "结合 Temperature 1.2（高温度，鼓励探索），这通常是为了 **Pass@K** 设计的。\n",
      "如果真的是 Average（Mean），高温度通常会降低准确率（因为会引入错误推理）。\n",
      "**所以，这里的 `average@16` 极大概率指的是 Pass@16。**\n",
      "\n",
      "### 3. 你的指标 vs 他们的指标\n",
      "\n",
      "*   **你的 `val-core/math_dapo/acc/mean@32`**：这是 **Pass@1 (Mean Accuracy)**。即单次生成的期望准确率。你的是 30%。\n",
      "*   **你的 `val-core/math_dapo/acc/best@32/mean`**：这是 **Pass@32**。你的是 40%。\n",
      "*   **你的 `val-core/math_dapo/acc/maj@32/mean`**：这是 **Majority Vote @ 32**。你的是 25%。\n",
      "\n",
      "**差距来源**：\n",
      "1.  **采样次数**：他们用了 64 次或 16 次。你用了 32 次。这影响 Pass@K。\n",
      "2.  **指标定义**：他们汇报的大概率是 **Pass@K** (Best) 或 **Maj@K**。而你主要关注的是 Mean。\n",
      "3.  **最关键的：Temperature**。\n",
      "    *   Verl 默认验证时的 Temperature 可能是 0.0 (Greedy) 或者很低。\n",
      "    *   Paper 里明确说了 `temperature of 1.2`。\n",
      "    *   **高温度对于 Pass@K 至关重要**。如果温度低，生成的 32 个回答都一样，Pass@32 就等于 Pass@1。只有温度高，多样性强，Pass@K 才能大幅超过 Pass@1。\n",
      "\n",
      "### 4. 如何对齐指标？\n",
      "\n",
      "要在你的 Log 中复现他们的指标，你需要做两件事：\n",
      "\n",
      "#### 步骤 1：修改验证参数 (Temperature & N)\n",
      "\n",
      "修改你的启动脚本或 Config，在验证阶段使用高温度和多采样。\n",
      "\n",
      "```bash\n",
      "# 修改启动脚本\n",
      "# 采样次数 N=64 (对齐 Qwen3 Report)\n",
      "actor_rollout_ref.rollout.val_kwargs.n=64 \n",
      "# 温度 T=1.2 (对齐 Arxiv Paper，激发多样性)\n",
      "actor_rollout_ref.rollout.val_kwargs.temperature=1.2 \n",
      "# 开启采样\n",
      "actor_rollout_ref.rollout.val_kwargs.do_sample=True\n",
      "```\n",
      "\n",
      "#### 步骤 2：关注正确的 Metric\n",
      "\n",
      "修改配置后，看你的日志中的以下两个指标：\n",
      "\n",
      "1.  **`val-core/.../acc/best@64/mean`**\n",
      "    *   这就是 **Pass@64**。\n",
      "    *   如果 Qwen3 真的很强，这个值应该能接近 70%。\n",
      "    *   这是最容易“刷”高的指标。\n",
      "\n",
      "2.  **`val-core/.../acc/maj@64/mean`**\n",
      "    *   这是 **Majority Vote**。\n",
      "    *   如果你的模型推理稳健，这个值也应该很高。\n",
      "\n",
      "### 总结\n",
      "你现在的 30% 是 **单次采样准确率 (Pass@1)**。\n",
      "Paper 里的 70%+ 是 **多次采样最佳准确率 (Pass@64)**。\n",
      "两者没有可比性。\n",
      "**请把验证温度调高到 1.0 或 1.2，并关注 `best@...` 指标，你就能看到接近的分数了。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我现在看到两种paper，汇报的在AIME 2024数据集上的分数，\n",
    "\n",
    "1. 官方qwen3 technical report里写qwen3_4b_instruct(也就是我用的model)，它的评价设置是\n",
    "\"\"\"\n",
    "For each question, we sample 64 times and take the average accuracy as the final score.\n",
    "\"\"\"\n",
    "得分是73.8分。\n",
    "\n",
    "2. 另一篇arxiv上的paper的设置如下\n",
    "\"\"\"\n",
    "We report average@16 using suggested sampling parameters from the Qwen3 blog with temperature of 1.2 and generation length of 38k\n",
    "\"\"\"\n",
    "得分是75.2分\n",
    "\n",
    "与我这的得分差异都很大，评价指标应该也不是同一个？他们的评价指标应该是相同的？我想对齐和他们的评价指标。\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "10ff7044-103c-4395-8221-e53ec472ec20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**这就完全解释通了！你找到了问题的核心。**\n",
      "\n",
      "如果是 `Qwen2.5-Math` (或者所谓的 \"Thinking\" / \"Reasoning\" 版本) 与普通的 `Qwen2.5-Instruct` 相比，在 AIME 这种竞赛数学题上的表现差异是**巨大**的。\n",
      "\n",
      "### 1. 模型能力的巨大鸿沟\n",
      "\n",
      "*   **Qwen-Instruct (通用指令模型)**：\n",
      "    *   **训练数据**：通用文本、对话、代码、少量数学。\n",
      "    *   **能力**：能做 GSM8K（小学/初中数学），但在 AIME（高中竞赛数学）上非常吃力。\n",
      "    *   **基准分数**：对于 4B/7B 的通用模型，AIME 2024 的 Pass@1 通常就在 **10% ~ 30%** 之间。\n",
      "    *   **你的表现**：你跑出了 **30% (Mean)** 和 **40% (Best)**，这对于一个 4B 的通用模型来说，**其实已经非常出色了**。\n",
      "\n",
      "*   **Qwen-Math / Thinking (数学/推理专用模型)**：\n",
      "    *   **训练数据**：海量的合成数学题、思维链（CoT）、OpenMathInstruct 等。\n",
      "    *   **能力**：专门针对 AIME 这种难题进行了强化，学会了 System 2 的深度思考模式。\n",
      "    *   **基准分数**：7B 版本在 AIME 2024 上确实能达到 **50% ~ 70%** (Pass@1 或 Maj@K)。\n",
      "\n",
      "### 2. 你的实验还有意义吗？\n",
      "\n",
      "**非常有意义。** 你现在的实验本质上是在**复现“从 Instruct 到 Thinking”的进化过程**。\n",
      "\n",
      "*   那些 70+ 分的模型不是天上掉下来的，它们正是通过你现在正在做的这种 **On-Policy Distillation (RLHF)** 方法，从通用模型一步步训练出来的。\n",
      "*   **你的目标**：不应该是“起步就达到 70%”，而应该是“通过训练，让这个 30% 的模型慢慢爬升到 40%、50%”。\n",
      "*   如果你直接加载一个已经 75 分的模型，你的蒸馏可能反而没效果了（因为 Teacher 和 Student 都已经到了天花板，或者 Teacher 也就是这个水平）。\n",
      "\n",
      "### 3. 如何对齐评价指标（为了写 Paper 或做对比）\n",
      "\n",
      "既然确定了模型底子不同，你就不能直接和 Technical Report 里的 73.8 分比绝对值。你应该关注**相对提升**。\n",
      "\n",
      "但为了确保你的验证过程是标准的，建议你依然按照 Paper 的设置来调整验证参数（即使分数低，也要用标准的姿势去测）：\n",
      "\n",
      "1.  **Temperature**: 设置为 **0.6 ~ 1.0**（通用模型不建议设太高，1.2 可能太高了会导致胡言乱语，数学模型能承受 1.2 是因为它们被 SFT 过）。\n",
      "2.  **Sampling N**: 设置为 **16 或 32**。\n",
      "3.  **关注指标**:\n",
      "    *   **`val-core/.../acc/maj@32` (Majority Vote)**：这是最鲁棒的指标。\n",
      "    *   **`val-core/.../acc/mean@32` (Pass@1)**：这是反映模型基础能力的指标。\n",
      "\n",
      "### 4. 总结与建议\n",
      "\n",
      "1.  **不要焦虑分数**：你的 4B Instruct 模型跑出 30% 是正常的，甚至是优秀的。不要拿它和经过专项数学微调的模型比。\n",
      "2.  **验证训练效果**：\n",
      "    *   你的核心关注点应该是：**随着 Step 增加，Validation Metric 是否在上涨？**\n",
      "    *   如果从 30% 涨到了 35%，这就是巨大的成功。\n",
      "3.  **Teacher 的角色**：\n",
      "    *   你现在 Teacher 也是 4B Instruct。\n",
      "    *   在 Self-Reflection 模式下（Teacher + Hint），Teacher 的能力上限 = **模型理解 Hint 的能力**。\n",
      "    *   只要 Teacher 能看懂 Hint 并生成正确的步骤，它就能教 Student。\n",
      "    *   **进阶玩法**：如果你想训练出 70+ 的模型，你可能需要一个**更强的 Teacher**（比如 Qwen-72B-Instruct 或者 DeepSeek-V3），让 4B Student 去蒸馏 72B Teacher 的 Logits。这叫 **Supervised Fine-Tuning (SFT) Distillation** 或 **Model Distillation**。\n",
      "\n",
      "**一句话：你的代码没问题，分数低是因为模型底子是通用的，继续跑，看涨幅！**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "他们用的应该是qwen3_4b_thinking，我用的qwen3_4b_instruct\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8f1a7c1-3315-4664-a2ef-d34f8d47bbef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要完全对齐 Qwen3 Technical Report 中的评价指标：\n",
      "> \"For each question, we sample 64 times and take the average accuracy as the final score.\"\n",
      "\n",
      "你需要明白，Verl 现在的 `val-core/.../acc/mean@32` **在数学定义上就是** 这个指标，只是采样次数（N）不同。\n",
      "\n",
      "*   **Average Accuracy** = 所有采样结果中正确的比例 = `mean`。\n",
      "*   **Sample 64 times** = `n=64`。\n",
      "\n",
      "所以，你只需要在启动脚本中修改 **验证参数**，将采样数改为 64，并确保开启采样（非 Greedy）。\n",
      "\n",
      "### 修改启动脚本\n",
      "\n",
      "请在你的启动命令中追加或修改以下参数：\n",
      "\n",
      "```bash\n",
      "# 1. 将采样次数从默认的 N 改为 64 (对齐 Report)\n",
      "actor_rollout_ref.rollout.val_kwargs.n=64 \\\n",
      "\n",
      "# 2. 开启采样模式 (Report 说是 \"sample\"，暗示不是 Greedy Search)\n",
      "actor_rollout_ref.rollout.val_kwargs.do_sample=True \\\n",
      "\n",
      "# 3. 设置温度 (Report 没细说，但通常计算 Average Acc 时，0.6-0.8 是比较稳健的，1.2 适合刷 Pass@K)\n",
      "# 建议先设为 0.7\n",
      "actor_rollout_ref.rollout.val_kwargs.temperature=0.7 \\\n",
      "```\n",
      "\n",
      "### 预期结果\n",
      "\n",
      "修改并运行后，你的日志中会自动出现以下指标（Verl 会根据 N 自动重命名）：\n",
      "\n",
      "*   **`val-core/math_dapo/acc/mean@64`**\n",
      "    *   **这就是你要对齐的指标。**\n",
      "    *   它计算的是：针对一道题生成的 64 个回答中，做对的比例（例如 64 个里对了 20 个，就是 20/64 = 0.3125）。\n",
      "    *   然后对所有题目取平均。\n",
      "\n",
      "### 为什么说它就是同一个指标？\n",
      "\n",
      "我们可以看 `verl/trainer/ppo/metric_utils.py` 的源码：\n",
      "\n",
      "```python\n",
      "def compute_score_metrics(scores, n=1):\n",
      "    # scores 是一个扁平的列表 [0, 1, 0, 0, 1, ...]，包含了所有题目 * 64 次采样的结果\n",
      "    \n",
      "    # mean = sum(scores) / len(scores)\n",
      "    # 这正是 \"Average Accuracy\" 的定义\n",
      "    mean = np.mean(scores) \n",
      "    \n",
      "    return {\n",
      "        f\"mean@{n}\": mean, \n",
      "        # ...\n",
      "    }\n",
      "```\n",
      "\n",
      "### 总结\n",
      "\n",
      "你不需要改任何 Python 代码。只需要在启动脚本里把 `val_kwargs.n` 改成 **64**，然后看日志里的 **`mean@64`**，那就是你要的“官方同款”指标。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我现在还需要做的是对齐和他们的评价指标，因为现在的评价指标是verl自己生成的，我想对齐官方qwen3 technical report里那种指标\n",
    "\"\"\"\n",
    "For each question, we sample 64 times and take the average accuracy as the final score.\n",
    "\"\"\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b177d3b0-e1b0-45da-bb06-e432e58c106c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常关键的细节！如果数据集本身已经做了重复（Data Augmentation），而你又在代码里设置了 `n=64`，就会发生**乘法效应**。\n",
      "\n",
      "### 情况分析\n",
      "\n",
      "假设你的 AIME Parquet 文件里，每道题已经重复了 32 行（Row 1: Q1, Row 2: Q1, ..., Row 32: Q1）。\n",
      "\n",
      "1.  **Verl 的验证逻辑**：\n",
      "    Verl 的 `RayPPOTrainer` 会遍历 Dataloader 里的每一条数据。\n",
      "    *   如果 Dataloader 里有 32 条 Q1。\n",
      "    *   且你设置了 `val_kwargs.n = 64`。\n",
      "    *   那么系统会对 **每一行** Q1 生成 64 个回答。\n",
      "    *   总共对 Q1 生成了 $32 \\times 64 = 2048$ 个回答。\n",
      "\n",
      "2.  **Metric 计算逻辑**：\n",
      "    Verl 的 `process_validation_metrics` 并没有智能到把这 2048 个回答聚合回 Q1。\n",
      "    *   它会把这 32 条数据当成 **32 道不同的题**。\n",
      "    *   它会计算 32 个 `mean@64`，然后求平均。\n",
      "    *   **数学上：** $\\text{Mean}(\\text{Mean}(Q1_1), \\text{Mean}(Q1_2), ...) = \\text{Mean}(Q1_{\\text{all}})$。\n",
      "    *   **结论：** 只要你关注的是 **Mean Accuracy**，结果是**正确且无偏的**。重复数据不会导致 Mean 指标虚高或虚低，只会增加验证时间。\n",
      "\n",
      "3.  **但是！对于 Pass@K (Best) 和 Maj@K (Majority) 有影响**：\n",
      "    *   如果你想算 Q1 的 Pass@64。\n",
      "    *   现在的逻辑算出的是：32 个“Pass@64”的平均值。\n",
      "    *   这**不等于** Pass@(32*64)。\n",
      "    *   不过，既然你要对齐的是 **Average Accuracy (Mean)**，这不受影响。\n",
      "\n",
      "### 潜在的坑：验证时间爆炸\n",
      "\n",
      "AIME 数据集通常很小（几十道题）。如果它被扩充到了几千行（32倍），而你又让每行生成 64 次：\n",
      "*   生成总量 = 题目数 $\\times 32 \\times 64$。\n",
      "*   这会极大地拖慢训练速度（每次 Validate 都要跑很久）。\n",
      "\n",
      "### 建议解决方案\n",
      "\n",
      "为了对齐指标且节省时间，你应该**确保验证集是“去重”的**（每道题只出现一次），然后完全由 `val_kwargs.n` 来控制采样次数。\n",
      "\n",
      "**操作步骤：**\n",
      "\n",
      "1.  **检查数据集**：\n",
      "    用 Pandas 读一下 `test_verl.parquet`，看看有多少行，是否有重复的 `prompt`。\n",
      "    ```python\n",
      "    import pandas as pd\n",
      "    df = pd.read_parquet(\"...\")\n",
      "    print(len(df))\n",
      "    print(df['prompt'].nunique())\n",
      "    ```\n",
      "    如果 `len > nunique`，说明有重复。\n",
      "\n",
      "2.  **如果是重复的**：\n",
      "    *   **方案 A (修改数据)**：自己写个脚本去重，保存为 `test_verl_dedup.parquet`。\n",
      "        ```python\n",
      "        df.drop_duplicates(subset=['prompt'], inplace=True)\n",
      "        df.to_parquet(\"test_verl_dedup.parquet\")\n",
      "        ```\n",
      "        然后在启动脚本里用这个新文件，并设置 `val_kwargs.n=64`。**这是最标准、最推荐的做法。**\n",
      "\n",
      "    *   **方案 B (修改参数)**：如果你不想改数据，且确认重复了 32 次。\n",
      "        *   设置 `val_kwargs.n=2`。\n",
      "        *   这样每道题总共采样 $32 \\times 2 = 64$ 次。\n",
      "        *   最后得到的 `mean@2` 其实就是全量的平均准确率（数学期望一致）。\n",
      "        *   **缺点**：日志里显示的是 `mean@2`，虽然数值是对的，但名字不好听，且容易混淆。\n",
      "\n",
      "**强烈建议采用方案 A：清洗验证集，确保每题一行，然后在 Config 里设 N=64。** 这样一切都清清楚楚。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "可是我用的aime的数据集好像自己就每条数据重复了32次，会不会有影响\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0cb5e16a-6750-4274-bde6-2f13cf7224e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**结论：是的，你的理解完全正确。数值上会非常接近，几乎没有区别。**\n",
      "\n",
      "从统计学的角度来看：\n",
      "\n",
      "1.  **本质都是求期望（Expectation）**：\n",
      "    *   **Average Accuracy** 的定义就是模型在特定题目上的**答对概率** $P(\\text{correct})$。\n",
      "    *   无论是采样 32 次还是 64 次，本质上都是在用**样本均值**来估计这个**真实概率**。\n",
      "    *   根据大数定律，只要采样次数足够多（32次其实已经不少了），算出来的平均值就会收敛到真实的概率。\n",
      "\n",
      "2.  **差异仅在于“方差”**：\n",
      "    *   采样 64 次比 32 次更“稳”一点，波动会小一点点。\n",
      "    *   但是，**30% 和 70% 的巨大差距，绝对不是因为采样次数（32 vs 64）造成的**。\n",
      "    *   如果是 32 次，你可能测出来是 29.5% 或 30.5%；如果是 64 次，可能是 29.8% 或 30.2%。它们会在同一个水平线上。\n",
      "\n",
      "### 既然数值差不多，还需要改吗？\n",
      "\n",
      "虽然数值没问题，但我还是建议你**关注一下效率问题**：\n",
      "\n",
      "**如果你不清洗数据（保留 32 条重复），直接在 Config 设置 `n=64`：**\n",
      "*   **计算量爆炸**：Verl 会对这 32 条重复数据，每一条都生成 64 次。\n",
      "*   **总生成量**：$32 \\times 64 = 2048$ 次/题。\n",
      "*   **后果**：你的验证过程会变得**极慢**（是正常速度的 32 倍），严重拖慢训练进度，浪费 GPU 算力。\n",
      "\n",
      "### 最佳实践建议\n",
      "\n",
      "1.  **为了省时间**：\n",
      "    *   既然数据集已经重复了 32 次，你可以在 Config 里设置 **`n=2`**。\n",
      "    *   这样总共采样 $32 \\times 2 = 64$ 次。\n",
      "    *   日志里显示的虽然是 `mean@2`，但你心里清楚，这就是 **全量采样 64 次的 Average Accuracy**。这和官方的指标在数学上是**完全等价**的。\n",
      "\n",
      "2.  **为了指标名字好听（对齐官方）**：\n",
      "    *   去重数据集（只留 1 条）。\n",
      "    *   Config 设置 `n=64`。\n",
      "    *   这样日志里显示 `mean@64`，看着舒服，计算量也正常。\n",
      "\n",
      "**总结：不用纠结了，目前的指标（mean@32）已经足够代表模型的真实水平了。那个 73.8 分是模型本身能力的差距，不是评测方法的锅。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "是不是即使不改，也和官方report里的指标数值应该会差不多？ 因为32次平均和64次应该都是为了求一个期望？ 虽然理论上是越大越接近？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06211208-4a60-4e91-90a5-a5a685e1234c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4dfbd417-c647-463b-81c6-f54c0ef4eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del chat_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3cdf8-adc5-40a1-8e6d-d21f208e791f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dd314e8-86e7-4b04-9819-b58d7cbefb95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的 `init_workers` 代码逻辑在设计时已经考虑到了 `n_nodes`，因此**核心分配逻辑（7:1 切分）在多机多卡下是通用的，不需要重写**。\n",
      "\n",
      "但是，从单机扩展到多机，**Batch Size 的整除约束**会发生变化，如果不提前检查，启动后会直接报错（和之前一样的 `ActorDiedError`）。\n",
      "\n",
      "建议对 `init_workers` 做如下**增强性修改**，加入自动检查和拓扑打印功能，确保多机启动一次成功。\n",
      "\n",
      "### 1. 修改 `init_workers` (增强版)\n",
      "\n",
      "请用以下代码替换 `teacher_student_reflective_trainer.py` 中的 `init_workers`。\n",
      "\n",
      "**主要改动点**：\n",
      "1.  自动计算 Student 的总 World Size（节点数 × 单节点卡数）。\n",
      "2.  **提前检查 Batch Size 整除性**，如果配置不对，直接抛出带有修改建议的 Error，避免等 Ray 启动半天后才报错。\n",
      "3.  打印清晰的多机拓扑结构。\n",
      "\n",
      "```python\n",
      "    def init_workers(self):\n",
      "        \"\"\"\n",
      "        初始化 Worker，支持多机多卡 (Multi-Node) 自动分配与检查。\n",
      "        \"\"\"\n",
      "        # 1. 获取资源配置\n",
      "        n_gpus_per_node = self.config.trainer.n_gpus_per_node\n",
      "        n_nodes = self.config.trainer.nnodes\n",
      "        \n",
      "        # === 资源切分策略 (7:1) ===\n",
      "        if n_gpus_per_node >= 8:\n",
      "            teacher_gpus_per_node = 1\n",
      "        elif n_gpus_per_node >= 4:\n",
      "            teacher_gpus_per_node = 1\n",
      "        else:\n",
      "            teacher_gpus_per_node = 1 # 至少给1张\n",
      "            \n",
      "        student_gpus_per_node = n_gpus_per_node - teacher_gpus_per_node\n",
      "        \n",
      "        if student_gpus_per_node < 1:\n",
      "            raise ValueError(f\"Not enough GPUs per node! Got {n_gpus_per_node}, need at least 2.\")\n",
      "\n",
      "        # 计算总的 World Size\n",
      "        student_world_size = student_gpus_per_node * n_nodes\n",
      "        teacher_world_size = teacher_gpus_per_node * n_nodes\n",
      "        \n",
      "        print(f\"\\n{'='*20} Multi-Node Topology Setup {'='*20}\")\n",
      "        print(f\">>> Nodes: {n_nodes} | GPUs per Node: {n_gpus_per_node}\")\n",
      "        print(f\">>> Student (Actor): {student_gpus_per_node} GPUs/node * {n_nodes} nodes = {student_world_size} Total GPUs\")\n",
      "        print(f\">>> Teacher (Ref):   {teacher_gpus_per_node} GPUs/node * {n_nodes} nodes = {teacher_world_size} Total GPUs\")\n",
      "\n",
      "        # === 关键检查: Batch Size 整除性 (防止启动后报错) ===\n",
      "        # 规则: ppo_mini_batch_size 必须能被 (Student_World_Size * Micro_Batch) 整除\n",
      "        # 或者至少: (ppo_mini_batch_size / Student_World_Size) 必须是整数，且能被 Micro_Batch 整除\n",
      "        \n",
      "        mini_batch = self.config.actor_rollout_ref.actor.ppo_mini_batch_size\n",
      "        micro_batch = self.config.actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu\n",
      "        \n",
      "        # 1. 检查每张卡分到的 Batch 是否为整数\n",
      "        if mini_batch % student_world_size != 0:\n",
      "            suggested = (mini_batch // student_world_size + 1) * student_world_size\n",
      "            raise ValueError(\n",
      "                f\"\\n[Config Error] ppo_mini_batch_size ({mini_batch}) cannot be divided evenly by Student World Size ({student_world_size}).\\n\"\n",
      "                f\"Student GPUs = {student_world_size} (Nodes: {n_nodes} * GPUs: {student_gpus_per_node})\\n\"\n",
      "                f\"Suggested ppo_mini_batch_size: {suggested} or {suggested + student_world_size}\"\n",
      "            )\n",
      "            \n",
      "        local_batch = mini_batch // student_world_size\n",
      "        \n",
      "        # 2. 检查每张卡的 Local Batch 是否能被 Micro Batch 整除\n",
      "        if local_batch % micro_batch != 0:\n",
      "            raise ValueError(\n",
      "                f\"\\n[Config Error] Local Batch ({local_batch}) cannot be divided by Micro Batch ({micro_batch}).\\n\"\n",
      "                f\"Global Mini Batch: {mini_batch}, Student World Size: {student_world_size}\\n\"\n",
      "                f\"Please adjust ppo_mini_batch_size or ppo_micro_batch_size_per_gpu.\"\n",
      "            )\n",
      "            \n",
      "        print(f\">>> Batch Check Passed: Global={mini_batch} -> PerGPU={local_batch} -> Micro={micro_batch} (Accum={local_batch//micro_batch})\")\n",
      "        print(f\"{'='*60}\\n\")\n",
      "\n",
      "        # 2. 创建资源池 (RayResourcePool 会自动处理跨节点调度)\n",
      "        # process_on_nodes=[N, N] 表示 Node 0 需要 N 张卡，Node 1 需要 N 张卡...\n",
      "        student_pool = RayResourcePool(\n",
      "            process_on_nodes=[student_gpus_per_node] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"student_pool\"\n",
      "        )\n",
      "        \n",
      "        teacher_pool = RayResourcePool(\n",
      "            process_on_nodes=[teacher_gpus_per_node] * n_nodes,\n",
      "            use_gpu=True,\n",
      "            max_colocate_count=1,\n",
      "            name_prefix=\"teacher_pool\"\n",
      "        )\n",
      "        \n",
      "        self.resource_pool_to_cls = {student_pool: {}, teacher_pool: {}}\n",
      "\n",
      "        # 3. 初始化 Config\n",
      "        student_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        student_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.ActorRollout],\n",
      "            config=student_config,\n",
      "            role=\"actor_rollout\",\n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[student_pool][\"actor_rollout\"] = student_cls\n",
      "\n",
      "        teacher_config = deepcopy(self.config.actor_rollout_ref)\n",
      "        # 显存优化: Teacher 显存占用降低\n",
      "        teacher_config.rollout.gpu_memory_utilization = 0.2\n",
      "        \n",
      "        teacher_cls = RayClassWithInitArgs(\n",
      "            cls=self.role_worker_mapping[Role.RefPolicy],\n",
      "            config=teacher_config,\n",
      "            role=\"actor_rollout\",\n",
      "            profile_option=self.config.trainer.npu_profile.options,\n",
      "        )\n",
      "        self.resource_pool_to_cls[teacher_pool][\"ref\"] = teacher_cls\n",
      "\n",
      "        # 4. 启动 Workers\n",
      "        all_wg = {}\n",
      "        wg_kwargs = {\n",
      "            \"ray_wait_register_center_timeout\": self.config.trainer.ray_wait_register_center_timeout,\n",
      "            \"device_name\": self.device_name\n",
      "        }\n",
      "        \n",
      "        for resource_pool, class_dict in self.resource_pool_to_cls.items():\n",
      "            worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)\n",
      "            wg_dict = self.ray_worker_group_cls(\n",
      "                resource_pool=resource_pool,\n",
      "                ray_cls_with_init=worker_dict_cls,\n",
      "                **wg_kwargs,\n",
      "            )\n",
      "            spawn_wg = wg_dict.spawn(prefix_set=class_dict.keys())\n",
      "            all_wg.update(spawn_wg)\n",
      "\n",
      "        self.actor_rollout_wg = all_wg[\"actor_rollout\"]\n",
      "        self.actor_rollout_wg.init_model()\n",
      "        \n",
      "        self.ref_policy_wg = all_wg[\"ref\"]\n",
      "        self.ref_policy_wg.init_model()\n",
      "            \n",
      "        self.async_rollout_mode = False\n",
      "        if self.config.actor_rollout_ref.rollout.mode == 'async':\n",
      "            from verl.experimental.agent_loop import AgentLoopManager\n",
      "            self.async_rollout_mode = True\n",
      "            self.async_rollout_manager = AgentLoopManager(\n",
      "                config=self.config,\n",
      "                worker_group=self.actor_rollout_wg\n",
      "            )\n",
      "```\n",
      "\n",
      "### 2. 启动脚本修改 (Param Changes)\n",
      "\n",
      "假设你有 **2 台机器** (N_NODES=2)，每台 **8 张卡**。\n",
      "那么：\n",
      "*   Student 总卡数 = $7 \\times 2 = 14$ 张。\n",
      "*   Teacher 总卡数 = $1 \\times 2 = 2$ 张。\n",
      "\n",
      "你需要调整 `ppo_mini_batch_size` 使得它能被 14 整除，且商能被 4 (Micro Batch) 整除。即被 56 整除。\n",
      "\n",
      "**推荐配置 (2 机 16 卡)**：\n",
      "\n",
      "```bash\n",
      "export N_NODES=2\n",
      "export n_gpus_per_node=8\n",
      "\n",
      "# Batch Size 计算:\n",
      "# Student Total GPUs = 14\n",
      "# Micro Batch = 4\n",
      "# 最小 Mini Batch = 14 * 4 = 56\n",
      "# 建议 Mini Batch = 112 (每卡分 8 条，累积 2 次)\n",
      "\n",
      "# 启动参数覆盖:\n",
      "trainer.nnodes=$N_NODES \\\n",
      "trainer.n_gpus_per_node=$n_gpus_per_node \\\n",
      "data.train_batch_size=224 \\  # 224 / 112 = 2 个 Mini-Batch\n",
      "actor_rollout_ref.actor.ppo_mini_batch_size=112 \\\n",
      "actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4\n",
      "```\n",
      "\n",
      "### 3. 多机启动命令 (Ray Cluster)\n",
      "\n",
      "多机训练不是直接跑 python 脚本，需要先搭建 Ray 集群。\n",
      "\n",
      "**Step 1: 在主节点 (Head Node) 启动 Ray**\n",
      "```bash\n",
      "# 假设主节点 IP 是 192.168.1.100\n",
      "ray start --head --port=6379 --num-gpus=8 --block\n",
      "```\n",
      "\n",
      "**Step 2: 在从节点 (Worker Node) 启动 Ray**\n",
      "```bash\n",
      "# 连接到主节点\n",
      "ray start --address='192.168.1.100:6379' --num-gpus=8 --block\n",
      "```\n",
      "\n",
      "**Step 3: 在主节点运行 Python 训练脚本**\n",
      "```bash\n",
      "# 你的 python3 -m verl.trainer.main_self_reflective ... 命令\n",
      "# 确保加上 trainer.nnodes=2\n",
      "```\n",
      "\n",
      "**注意**：\n",
      "*   确保两台机器的代码、路径、Python 环境完全一致。\n",
      "*   确保 `export VLLM_USE_V1=1` 等环境变量在两台机器上都生效。\n",
      "*   Verl 会自动检测 Ray 集群并分发任务。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我现在要改成多机多卡运行，有什么地方要修改吗？ 比如init_workers?\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a84c5313-f06c-4686-a2fd-d92f93430d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常反直觉但实际上很常见的现象。代码逻辑虽然没变，但底层的**资源调度**和**数据处理流程**因为代码的“通用化改造”而发生了微妙的变化。\n",
      "\n",
      "主要原因可能集中在 `init_workers` 的改动、Padding 逻辑的引入，以及 Ray 的调度开销上。\n",
      "\n",
      "### 1. `_prepare_teacher_forward_batch` 中的 Padding 开销\n",
      "**这是最大的嫌疑犯。**\n",
      "\n",
      "*   **改动前**：你使用的是 `pad_sequence`（C++ 实现，极快）。\n",
      "*   **改动后**：为了解决 DP_Actor 对齐问题，你引入了 **Python 循环** 来进行手动 Left Padding。\n",
      "    ```python\n",
      "    for i in range(len(new_input_ids)):\n",
      "        # ... torch.cat ...\n",
      "        # ... torch.cat ...\n",
      "    ```\n",
      "*   **影响**：\n",
      "    *   在单机 8 卡、Batch Size 224 的情况下，这个循环要在 CPU 上跑 224 次。\n",
      "    *   每次循环涉及多次 `torch.cat` 和内存分配。\n",
      "    *   虽然单次很快，但累积起来，加上 Python 的 GIL 锁（如果是在多线程环境下），会显著增加 CPU 预处理时间，导致 GPU 等待数据（空转）。\n",
      "\n",
      "**优化建议**：\n",
      "尝试将手动 Padding 的逻辑向量化，或者使用 `torch.nn.functional.pad`。\n",
      "但最简单的方法是：**把这个函数放到 GPU 上跑**（如果你现在的 tensor 是在 CPU 上），或者**用多进程预处理**。不过考虑到代码复杂度，目前的 Python 循环可能是瓶颈。\n",
      "\n",
      "### 2. Ray 资源调度的开销\n",
      "在 `init_workers` 中，你引入了更复杂的 `RayResourcePool` 和多机检测逻辑。\n",
      "*   即使是单机运行，Ray 现在也会走一遍“跨节点资源检查”的流程。\n",
      "*   虽然这只影响启动时间，但如果你的 `fit` 循环中有频繁的 Worker 通讯（`generate_sequences`, `compute_log_prob`），Ray 的 Object Store 开销在单机上可能比纯 PyTorch DDP 要大。\n",
      "*   **特别是 `actor_rollout_wg` 和 `ref_policy_wg` 的频繁切换**。\n",
      "\n",
      "### 3. vLLM 的配置变化\n",
      "你为了适配多机/长文本，修改了 vLLM 的配置：\n",
      "*   `gpu_memory_utilization` 改为了 0.5（之前是 0.4 或 0.2）。\n",
      "*   `max_num_batched_tokens` 改为了 45000。\n",
      "*   `torch_dtype` 改为了 `bfloat16`。\n",
      "\n",
      "**影响**：\n",
      "*   **45000 的 Token 上限**：如上一条回答所述，这会显著拖慢 vLLM 的调度和计算速度，即使是在单机上。vLLM 需要管理更大的 KV Cache 池，导致 Cache 命中率下降或调度算法变慢。\n",
      "*   **bfloat16**：虽然省显存，但在某些旧硬件（非 Ampere 架构）上可能比 fp16 慢，或者导致更多的类型转换开销（如果输入数据是 fp32）。\n",
      "\n",
      "### 4. 数据传输瓶颈 (Gather/Scatter)\n",
      "*   **Student (7卡)** vs **Teacher (1卡)**。\n",
      "*   每次 Step，7 张 Student 卡生成的数据（224 条长文本）需要全部 **Gather** 到 Driver 进程（CPU），进行 Padding 处理，然后再 **Scatter** 给那 1 张 Teacher 卡。\n",
      "*   Teacher 算完后，Logits 又要传回 Driver，再分发给 Student。\n",
      "*   **数据量**：224 * 4096 * 2 Bytes $\\approx$ 1.8 MB (Input IDs) + Logits (巨大)。\n",
      "*   **Logits 传输**：Teacher 返回的 Logits 是 `[224, 4096, 152000]` 吗？不对，Verl 通常只返回 Target Logits。但如果传输了完整的 Logits，那带宽就爆了。\n",
      "*   **关键点**：`_prepare_teacher_forward_batch` 是在 Driver (CPU) 上跑的。224 条 4k 长度的 Tensor 在 GPU -> CPU -> GPU 之间搬运，带宽压力很大。\n",
      "\n",
      "### 排查建议\n",
      "\n",
      "1.  **检查 GPU 利用率 (`nvidia-smi`)**：\n",
      "    *   如果 GPU 经常跳到 0%，说明瓶颈在 CPU（Padding 循环）或 IO/网络传输。\n",
      "    *   如果 GPU 一直 100% 但进度条走得慢，说明是 `max_num_batched_tokens=45000` 导致的计算量增加。\n",
      "\n",
      "2.  **回滚 vLLM 配置测试**：\n",
      "    *   尝试把 `max_num_batched_tokens` 改回默认（或较小值），看看速度是否恢复。这是最可能的“变慢”原因。\n",
      "\n",
      "3.  **优化 Padding 代码**：\n",
      "    *   如果是 CPU 瓶颈，尝试用 `torch.nn.utils.rnn.pad_sequence` 的变体，或者接受 Right Padding 的结果，然后在 GPU 上用 `torch.roll` 或 `index_select` 来实现 Left Shift，而不是在 CPU 上写循环。\n",
      "\n",
      "**总结：大概率是 vLLM 配置（45000）和 CPU 上的手动 Padding 循环共同导致的。**\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "改成多机多卡运行之后，同样的代码重新运行单机多卡的脚本运行变得比没改之前慢很多 不知道为啥？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c553e8e-6a58-4cd7-bc41-b0566ba86fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这一现象的核心原因在于 **Teacher（1张卡）成为了整个训练流程的超级瓶颈**。\n",
      "\n",
      "### 1. 瓶颈计算分析\n",
      "\n",
      "在 **7:1** 的架构下，处理 `train_batch_size=112` 的数据：\n",
      "\n",
      "*   **Student (7张卡)**：\n",
      "    *   总任务量：112 条数据。\n",
      "    *   每张卡分摊：$112 / 7 = 16$ 条。\n",
      "    *   `micro_batch = 4`。\n",
      "    *   **计算步数**：$16 / 4 = \\mathbf{4}$ **个串行步**。\n",
      "    *   Student 只需要做 4 次前向/反向传播就完事了。\n",
      "\n",
      "*   **Teacher (1张卡)**：\n",
      "    *   总任务量：112 条数据（Teacher 必须处理所有数据以计算 Reward）。\n",
      "    *   每张卡分摊：**112 条**。\n",
      "    *   你的脚本设置：`ref.log_prob_micro_batch_size_per_gpu=2`。\n",
      "    *   **计算步数**：$112 / 2 = \\mathbf{56}$ **个串行步**！\n",
      "\n",
      "**结论**：\n",
      "Student 的 7 张显卡在做完 4 步计算后，必须**停下来等待** Teacher 跑完它的 56 步计算。\n",
      "**Teacher 的速度比 Student 慢了 14 倍**。这导致 7 张 Student 卡有 90% 的时间在空转（Idle）。\n",
      "\n",
      "### 2. 为什么之前（单机4:4）没这么慢？\n",
      "*   之前 Teacher 有 4 张卡。\n",
      "*   每张卡分摊 $112 / 4 = 28$ 条。\n",
      "*   步数 $28 / 2 = 14$ 步。\n",
      "*   虽然还是比 Student 慢，但差距没那么大。\n",
      "\n",
      "### 3. 解决方案\n",
      "\n",
      "要解决这个问题，必须**大幅提升 Teacher 的吞吐量**。\n",
      "\n",
      "#### 方法 A：增大 Teacher 的 Micro Batch (最推荐，改动最小)\n",
      "对于 4B 模型，即使是 8k 长度，单张 A100/H800 也能承受较大的 Batch Size（特别是开启 Chunking 后）。\n",
      "将 Teacher 的 Micro Batch 从 2 提升到 **8** 甚至 **16**。\n",
      "\n",
      "*   如果设为 8：步数降为 $112/8 = 14$ 步。速度提升 4 倍。\n",
      "*   如果设为 16：步数降为 $112/16 = 7$ 步。速度提升 8 倍，接近 Student。\n",
      "\n",
      "#### 方法 B：开启 Logits Chunking (防止 OOM)\n",
      "为了支持更大的 Micro Batch，必须开启分块计算。\n",
      "\n",
      "#### 方法 C：调整架构为 6:2 (如果方法 A 依然 OOM)\n",
      "如果 1 张卡实在跑不动大 Batch，就只能牺牲一张 Student 卡给 Teacher，改为 6:2。但这需要修改 `init_workers` 和 Batch Size 整除逻辑。\n",
      "\n",
      "---\n",
      "\n",
      "### 修改后的启动脚本建议\n",
      "\n",
      "请在你的脚本中做以下修改，重点是**压榨 Teacher 的单卡性能**：\n",
      "\n",
      "```bash\n",
      "# 1. 增大 Teacher 的计算 Batch (关键！)\n",
      "# 从 2 改为 8 或 16 (视显存而定，4B模型通常16没问题)\n",
      "actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 \\\n",
      "\n",
      "# 2. 开启分块计算以节省显存 (关键！)\n",
      "actor_rollout_ref.actor.entropy_from_logits_with_chunking=True \\\n",
      "\n",
      "# 3. 保持 Student 配置\n",
      "actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \\\n",
      "```\n",
      "\n",
      "**修改后的完整脚本片段：**\n",
      "\n",
      "```bash\n",
      "# ... 前面的变量 ...\n",
      "\n",
      "PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_self_reflective \\\n",
      "    data.train_files=$DATA_DIR/OpenThoughts/train_verl.parquet \\\n",
      "    data.val_files=\"[$DATA_DIR/AIME_2024/test_verl.parquet,$DATA_DIR/AIME_2025/test_verl.parquet,$DATA_DIR/HMMT_FEB_2025/test_verl.parquet]\" \\\n",
      "    data.train_batch_size=112 \\\n",
      "    data.max_prompt_length=4096 \\\n",
      "    data.max_response_length=4096 \\\n",
      "    +data.summary_max_new_tokens=4096 \\\n",
      "    actor_rollout_ref.model.path=$BASE_MODEL \\\n",
      "    +actor_rollout_ref.model.torch_dtype=bfloat16 \\\n",
      "    actor_rollout_ref.actor.optim.lr=5e-7 \\\n",
      "    actor_rollout_ref.actor.ppo_mini_batch_size=56 \\\n",
      "    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n",
      "    actor_rollout_ref.actor.policy_loss=distillation \\\n",
      "    actor_rollout_ref.actor.entropy_from_logits_with_chunking=True \\\n",
      "    actor_rollout_ref.rollout.name=vllm \\\n",
      "    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \\\n",
      "    actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
      "    actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
      "    actor_rollout_ref.rollout.max_num_batched_tokens=45000 \\\n",
      "    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=8 \\\n",
      "    # ... 后续参数 ...\n",
      "```\n",
      "\n",
      "**核心变动**：\n",
      "1.  `ref.log_prob_micro_batch_size_per_gpu`：**2 -> 8** (Teacher 提速 4 倍)。\n",
      "2.  `actor.entropy_from_logits_with_chunking=True` (防止 Teacher OOM)。\n",
      "\n",
      "试一下这个配置，速度应该会显著回升。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我的单机多卡脚本如下\n",
    "\"\"\"\n",
    "export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
    "export DATA_DIR='/data/kcl/lpy/data'\n",
    "WAND_PROJECT=\"self_reflect\"\n",
    "export SWANLAB_API_KEY=\"DDMUFxYQU6Om1ZuHWYoxm\"           # 设置在线跟踪模式API\n",
    "export BASE_MODEL='/data/kcl/lpy/models/qwen3_4b_instruct'\n",
    "export EXPERIMENT_NAME=sr_dl_teacher_student_openthoughts_qwen3_4b_instruct_alleval\n",
    "N_NODES=1\n",
    "n_gpus_per_node=8\n",
    "export BASE_DIR=$(pwd)\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "export VLLM_USE_V1=1\n",
    "PYTHONUNBUFFERED=1 python3 -m verl.trainer.main_self_reflective \\\n",
    " data.train_files=$DATA_DIR/OpenThoughts/train_verl.parquet \\\n",
    " data.val_files=[$DATA_DIR/AIME_2024/test_verl.parquet,$DATA_DIR/AIME_2025/test_verl.parquet,$DATA_DIR/HMMT_FEB_2025/test_verl.parquet] \\\n",
    " data.train_batch_size=112 \\\n",
    " data.max_prompt_length=4096 \\\n",
    " data.max_response_length=4096 \\\n",
    " +data.summary_max_new_tokens=4096 \\\n",
    " actor_rollout_ref.model.path=$BASE_MODEL \\\n",
    " +actor_rollout_ref.model.torch_dtype=bfloat16 \\\n",
    " actor_rollout_ref.actor.optim.lr=5e-7 \\\n",
    " actor_rollout_ref.actor.ppo_mini_batch_size=56 \\\n",
    " actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n",
    " actor_rollout_ref.actor.policy_loss=distillation \\\n",
    " actor_rollout_ref.rollout.name=vllm \\\n",
    " actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=4 \\\n",
    " actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n",
    " actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n",
    " actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=2 \\\n",
    " critic.optim.lr=1e-5 \\\n",
    " critic.model.path=$BASE_MODEL \\\n",
    " +critic.model.torch_dtype=bfloat16 \\\n",
    " critic.ppo_micro_batch_size_per_gpu=4 \\\n",
    " algorithm.kl_ctrl.kl_coef=0.001 \\\n",
    " trainer.logger=console \\\n",
    " trainer.val_before_train=False \\\n",
    " trainer.n_gpus_per_node=$n_gpus_per_node \\\n",
    " trainer.nnodes=$N_NODES \\\n",
    " trainer.save_freq=30 \\\n",
    " trainer.test_freq=30 \\\n",
    " trainer.logger=\"['swanlab', 'console']\" \\\n",
    " trainer.project_name=$WAND_PROJECT \\\n",
    " trainer.experiment_name=$EXPERIMENT_NAME \\\n",
    " trainer.default_local_dir=${BASE_DIR}/verl_checkpoints/$EXPERIMENT_NAME \\\n",
    " trainer.total_epochs=15 2>&1 | tee verl_log/$EXPERIMENT_NAME.log\n",
    "\"\"\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9e1b80e2-518d-452b-bf20-4f791f6d581c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 机 8 卡是一个非常庞大的集群（64 张 GPU）。这意味着你的**算力极强**，但**通信和 Batch Size 的约束**也变得非常严格。\n",
      "\n",
      "### 1. 架构计算 (Topology)\n",
      "\n",
      "*   **节点数 (Nodes)**: 8\n",
      "*   **单节点卡数 (GPUs/Node)**: 8\n",
      "*   **分配策略 (7:1)**:\n",
      "    *   **Student**: 每节点 7 卡 $\\times$ 8 节点 = **56 张卡 (Global World Size)**。\n",
      "    *   **Teacher**: 每节点 1 卡 $\\times$ 8 节点 = **8 张卡**。\n",
      "\n",
      "### 2. Batch Size 计算 (关键约束)\n",
      "\n",
      "你需要满足以下整除关系：\n",
      "`ppo_mini_batch_size` % `Student_World_Size` == 0\n",
      "`Local_Batch` % `Micro_Batch` == 0\n",
      "\n",
      "*   **Student World Size**: 56\n",
      "*   **Micro Batch**: 建议设为 1 或 2 (因为 8k 长文本显存压力大)。\n",
      "\n",
      "**推荐配置：**\n",
      "\n",
      "1.  **`ppo_mini_batch_size`**:\n",
      "    *   必须是 56 的倍数。\n",
      "    *   最小可行值：**56** (每卡分到 1 条数据)。\n",
      "    *   推荐值：**112** (每卡分到 2 条)。\n",
      "\n",
      "2.  **`train_batch_size`**:\n",
      "    *   必须是 `ppo_mini_batch_size` 的倍数。\n",
      "    *   如果 mini=56，train 可以是 56, 112, 224...\n",
      "    *   如果 mini=112，train 可以是 112, 224...\n",
      "    *   **建议设大一点**，充分利用 64 卡的并行能力。比如 **448** 或 **896**。\n",
      "\n",
      "### 3. 启动参数推荐\n",
      "\n",
      "请使用以下参数覆盖启动脚本：\n",
      "\n",
      "```bash\n",
      "# === 基础环境 ===\n",
      "export N_NODES=8\n",
      "export n_gpus_per_node=8\n",
      "\n",
      "# === Batch Size 配置 (核心) ===\n",
      "# 56 张 Student 卡并行\n",
      "# 每张卡处理 2 条数据 (Local Batch=2) -> Global Mini Batch = 112\n",
      "# 每次 Update 跑 4 个 Mini Batch -> Train Batch = 448\n",
      "data.train_batch_size=448 \\\n",
      "actor_rollout_ref.actor.ppo_mini_batch_size=112 \\\n",
      "actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2 \\\n",
      "\n",
      "# === 显存与性能优化 ===\n",
      "# 8k 长文本必备\n",
      "actor_rollout_ref.model.torch_dtype=bfloat16 \\\n",
      "actor_rollout_ref.actor.entropy_from_logits_with_chunking=True \\\n",
      "\n",
      "# Teacher 提速 (现在有 8 张 Teacher 卡了，压力分散了，Micro Batch 可以小点)\n",
      "actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n",
      "\n",
      "# vLLM 配置 (防止 OOM)\n",
      "actor_rollout_ref.rollout.gpu_memory_utilization=0.5 \\\n",
      "actor_rollout_ref.rollout.max_num_batched_tokens=45000 \\\n",
      "```\n",
      "\n",
      "### 4. 为什么这么配？\n",
      "\n",
      "*   **并行度爆炸**：56 张 Student 卡同时 Rollout。一次生成 448 条数据，每张卡只需要生成 $448/56 = 8$ 条。速度会飞快。\n",
      "*   **Teacher 也不弱**：8 张 Teacher 卡同时计算 Reward。每张卡处理 $448/8 = 56$ 条数据。压力适中。\n",
      "*   **显存安全**：Micro Batch = 2，配合 Chunking，即使是 8k 长度也能稳稳跑在 4B 模型上。\n",
      "\n",
      "**总结**：8 机 8 卡下，瓶颈主要在**数据传输**和**同步**。只要 Batch Size 对齐了（56 的倍数），训练速度将是单机的 7-8 倍。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我要8机8卡运行，运行参数怎么设置\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d550bce5-252d-4416-8d4f-3327815f8bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个非常好的问题。这三个参数（Train, Mini, Micro）就像是**俄罗斯套娃**，一层套一层，共同决定了数据如何在庞大的 GPU 集群中流动。\n",
      "\n",
      "在 **8机8卡（共56张 Student 卡）** 的环境下，理解它们至关重要，否则就会出现“卡住”、“报错”或者“显存爆炸”。\n",
      "\n",
      "我用一个**“吃自助餐”**的比喻来解释这三个层级：\n",
      "\n",
      "---\n",
      "\n",
      "### 第一层：`data.train_batch_size` (总盘子)\n",
      "**含义**：**一次 PPO 迭代（Step）中，一共要“生产”并“消费”多少条数据。**\n",
      "\n",
      "*   **比喻**：这是厨师（Rollout Workers）一次性端上桌的所有菜的总量。\n",
      "*   **在你的配置中 (448)**：\n",
      "    *   意味着 56 张 Student 卡，齐心协力，一共要生成 **448** 个问题+回答。\n",
      "    *   平均每张卡负责生成：$448 / 56 = 8$ 条。\n",
      "*   **作用**：决定了训练的**采样多样性**。这个数越大，模型见过的题越丰富，训练越稳定，但生成时间越长。\n",
      "\n",
      "---\n",
      "\n",
      "### 第二层：`actor.ppo_mini_batch_size` (一口的大小)\n",
      "**含义**：**进行一次参数更新（Gradient Update / Optimizer Step）时，一共使用多少条数据。**\n",
      "\n",
      "*   **比喻**：虽然桌上有 448 道菜，但你嘴巴没那么大，不能一口吞下。你决定分几次吃。这一口下去的总量，就是 Mini Batch。\n",
      "*   **在你的配置中 (112)**：\n",
      "    *   桌上有 448 条数据。\n",
      "    *   你每次只取 **112** 条来算梯度、更新模型。\n",
      "    *   所以，吃完这顿饭（完成一个 Step），你需要更新：$448 / 112 = 4$ 次。\n",
      "*   **分布式约束 (为什么报错就在这里)**：\n",
      "    *   这是一个**全局参数**。Verl 会把这 112 条数据**均匀切分**给所有的 Student 卡。\n",
      "    *   **算术题**：你有 56 张卡。要把 112 条数据平分。\n",
      "    *   每张卡分到：$112 / 56 = \\mathbf{2}$ 条。\n",
      "    *   **报错原因**：如果你设置成 64。$64 / 56 = 1.14$ 条。GPU 没法处理 1.14 条数据，所以之前会报错。**它必须能被卡数整除。**\n",
      "\n",
      "---\n",
      "\n",
      "### 第三层：`ppo_micro_batch_size_per_gpu` (细嚼慢咽)\n",
      "**含义**：**单张 GPU 在显存允许的情况下，一次前向/反向传播实际能处理的数据量。**\n",
      "\n",
      "*   **比喻**：每张卡分到了 2 条数据（Local Batch）。但是，这 2 条数据可能太长了（8k长度），如果同时塞进显存计算，显存会爆（OOM）。所以要一条一条嚼。\n",
      "*   **在你的配置中 (2)**：\n",
      "    *   每张卡手头有 2 条数据。\n",
      "    *   你设置 Micro Batch = 2。\n",
      "    *   GPU 尝试一次性把这 2 条都塞进显存计算。\n",
      "    *   **梯度累积 (Gradient Accumulation)**：如果 Micro Batch 设为 1，GPU 就会先算第 1 条，把梯度存起来；再算第 2 条，把梯度加进去；最后再通讯更新。\n",
      "*   **约束**：\n",
      "    *   每张卡分到的数据量（2条），必须能被 Micro Batch（2）整除。\n",
      "    *   如果你设置 Micro Batch = 4，但手里只有 2 条数据，GPU 就会报错：“你让我一次算 4 条，但我手里只有 2 条啊！”\n",
      "\n",
      "---\n",
      "\n",
      "### 总结：8机8卡 (56 Student) 的数据流\n",
      "\n",
      "假设配置如下：\n",
      "*   `train_batch_size = 448`\n",
      "*   `ppo_mini_batch_size = 112`\n",
      "*   `micro_batch_size = 2`\n",
      "\n",
      "**流程如下：**\n",
      "\n",
      "1.  **生成阶段 (Rollout)**：\n",
      "    *   56 张卡同时开工，每张卡生成 8 条数据。\n",
      "    *   总共生成 448 条。\n",
      "\n",
      "2.  **训练阶段 (PPO Update)**：\n",
      "    *   数据被切分为 4 个 Mini-Batch（每个 112 条）。\n",
      "    *   **开始第 1 次更新**：\n",
      "        *   取前 112 条数据。\n",
      "        *   分发给 56 张卡 $\\rightarrow$ **每张卡拿到 2 条**。\n",
      "        *   **GPU 计算**：\n",
      "            *   Micro Batch = 2。\n",
      "            *   每张卡一次性把这 2 条数据送入计算（Forward/Backward）。\n",
      "            *   显存占用 = 2 条 8k 数据的量。\n",
      "        *   **同步**：56 张卡互相通信，平均梯度。\n",
      "        *   **更新**：修改模型参数。\n",
      "    *   **重复上述步骤**，直到 4 个 Mini-Batch 全部跑完。\n",
      "\n",
      "### 为什么要这么设置？\n",
      "\n",
      "1.  **为了不报错**：`mini_batch` 必须是 `56` 的倍数（因为你有 56 张卡）。\n",
      "2.  **为了不 OOM**：`micro_batch` 必须足够小（比如 1 或 2），因为 8k 长度太吃显存了。\n",
      "3.  **为了训练快**：`train_batch` 设大一点（448），可以让庞大的集群“跑一次，学久一点”，减少生成（最慢的环节）的频率。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我其实不理解这几个参数的具体含义，你帮我解释一下具体在干啥？为啥要这样设置？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6b14184e-7e00-4425-b186-fd4cd590e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verl 的 Metric 系统是基于 `DataProto` 中的 `data_source` 字段来分组的。如果你的 Metric 混在一起了，说明这些新加的数据集在加载时，**没有正确设置 `data_source` 字段**，或者它们都被默认设为了同一个名字（如 `unknown`）。\n",
      "\n",
      "你需要修改数据加载逻辑，确保每个 Parquet 文件的数据被赋予**唯一且有意义的 `data_source` 标签**。\n",
      "\n",
      "### 1. 确认数据源 (Parquet 文件)\n",
      "Verl 默认读取 Parquet 文件中的 `data_source` 列。\n",
      "*   如果你的 `train_verl.parquet` 里有 `data_source` 列，且值为 `openai/gsm8k`，那么 Metric 就是 `val-core/openai/gsm8k/...`。\n",
      "*   如果你的 OpenThoughts 数据集里没有这一列，或者所有数据的这一列都是 `default`，那么 Metric 就会混在一起。\n",
      "\n",
      "### 2. 解决方案：在加载时注入 Data Source\n",
      "\n",
      "我们不需要修改 Parquet 文件，而是修改 `verl/utils/dataset/rl_dataset.py`，让它根据**文件名**或者**配置**来强制覆盖 `data_source`。\n",
      "\n",
      "请找到 `RLHFDataset` 类（或你使用的 Dataset 类），修改 `_read_files_and_tokenize` 方法。\n",
      "\n",
      "#### 修改 `verl/utils/dataset/rl_dataset.py`\n",
      "\n",
      "```python\n",
      "    def _read_files_and_tokenize(self):\n",
      "        data_source = self.config.get(\"data_source\", None)\n",
      "        # ...\n",
      "        \n",
      "        for file_path in self.data_files:\n",
      "            # === FIX: 根据文件名自动推断 data_source ===\n",
      "            # 例如: /data/.../AIME_2024/test_verl.parquet -> \"AIME_2024\"\n",
      "            # 或者从 config 中传入一个列表对应每个文件\n",
      "            \n",
      "            import os\n",
      "            # 简单的启发式规则：取文件名的上一级目录名\n",
      "            auto_source_name = os.path.basename(os.path.dirname(file_path))\n",
      "            \n",
      "            # 或者更粗暴：直接用文件名 (test_verl.parquet 可能重复，不推荐)\n",
      "            # auto_source_name = os.path.basename(file_path).replace(\".parquet\", \"\")\n",
      "            \n",
      "            print(f\"Loading {file_path} with data_source='{auto_source_name}'\")\n",
      "            \n",
      "            # 读取 Parquet\n",
      "            df = pd.read_parquet(file_path)\n",
      "            \n",
      "            # === FIX: 强制覆盖/添加 data_source 列 ===\n",
      "            # 如果配置里没有指定全局 data_source，就用文件特定的\n",
      "            if data_source is None:\n",
      "                df['data_source'] = auto_source_name\n",
      "            else:\n",
      "                df['data_source'] = data_source\n",
      "                \n",
      "            # ... (后续的 tokenize 逻辑) ...\n",
      "```\n",
      "\n",
      "### 3. 更优雅的方案：修改启动脚本 (Hydra List)\n",
      "\n",
      "如果你不想改代码，且 Verl 支持为每个数据集指定 Config，你可以在启动脚本里把 `data.val_files` 写成带 Key 的字典（但这取决于 Hydra/Verl 的具体实现支持）。\n",
      "\n",
      "**最稳妥的办法**还是修改 `rl_dataset.py`，强行把 `data_source` 列刷成你想要的名字。\n",
      "\n",
      "**修改后的效果**：\n",
      "假设你加载了：\n",
      "1.  `.../AIME_2024/test.parquet` -> `data_source=\"AIME_2024\"`\n",
      "2.  `.../GSM8K/test.parquet` -> `data_source=\"GSM8K\"`\n",
      "\n",
      "你的日志里就会自动出现：\n",
      "*   `val-core/AIME_2024/acc/mean`\n",
      "*   `val-core/GSM8K/acc/mean`\n",
      "\n",
      "它们会自动分开计算和显示。\n"
     ]
    }
   ],
   "source": [
    "response = chat('''\n",
    "我新加了多个数据集，我希望每个数据集独立一个metric，现在好像混合起来一起计算了，怎么修改？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e582e-873d-4bb7-b849-091330e2dc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
